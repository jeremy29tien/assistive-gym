demo lengths 200
demos: (120, 200, 4)
demo_rewards: (120,)
[-191.10004521 -189.05007017 -187.50220085 -180.24009179 -171.96137924
 -169.38170168 -167.87631268 -164.27567046 -163.76742237 -156.07868959
 -155.22804552 -145.29002069 -142.8665728  -142.75441848 -136.98962801
 -132.4813577  -130.98347157 -118.24362226 -117.38411415 -116.97382686
 -115.06704102 -112.47832646 -103.24187681  -99.7422753   -98.18526662
  -88.94674758  -85.88346862  -79.38307722  -79.25112655  -73.50697856
  -72.69803192  -71.37326168  -68.27187815  -65.42298163  -65.35816021
  -49.31867416  -17.32544162   -4.37451128   -0.4258659     9.10814675
   17.73263018   40.64550157   41.11774839   48.64917674   52.5026004
   52.54502853   59.52827492   63.45606317   66.31570759   67.13050508
   72.16624272   79.16074532   84.19837606   89.30707154   90.26988507
   90.28646232   90.32667007   91.18021498   92.38187518   92.67153902
   93.69474939   93.8611286    95.24073533   95.36301018   96.09581961
   97.90553231   98.07644231   98.35872828   98.5733471    98.77895391
   99.10163704   99.47183808  101.36707045  102.70820969  103.61971442
  104.00039884  104.94014609  105.28448326  105.60678193  106.95774051
  107.03525723  107.05410486  107.36477417  107.72369032  107.78006758
  107.99397436  108.08910889  108.09776529  108.24033507  109.15712102
  109.71287331  110.25063891  110.45880856  110.48639228  111.92877272
  112.2946742   112.73910566  112.98274531  113.8461648   114.8461224
  115.08707834  115.41025719  115.56466246  115.87882503  116.51927592
  116.69939038  117.18809855  117.29357602  117.43241924  119.84073496
  122.74776107  124.10172315  124.42382576  124.4817957   125.25908938
  125.30457668  125.43458549  129.13346806  129.67718524  131.40806577]
maximum traj length 200
num training_obs 1593
num training_labels 1593
num val_obs 177
num val_labels 177
cpu
end of epoch 0: val_loss 0.022653922871009095, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fc1.weight', tensor([[ 0.0226, -0.2121,  0.0803, -0.0542]]))])
end of epoch 1: val_loss 0.02686964477295849, val_acc 1.0
trigger times: 1
end of epoch 2: val_loss 0.03606794009242514, val_acc 0.9943502824858758
trigger times: 2
end of epoch 3: val_loss 0.03187409829205842, val_acc 0.9943502824858758
trigger times: 3
end of epoch 4: val_loss 0.02007366079512526, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fc1.weight', tensor([[ 0.0043, -0.2013,  0.1544, -0.0132]]))])
end of epoch 5: val_loss 0.031659344982327774, val_acc 0.9943502824858758
trigger times: 1
end of epoch 6: val_loss 0.023268993535017986, val_acc 0.9943502824858758
trigger times: 2
end of epoch 7: val_loss 0.03622261470044276, val_acc 0.9943502824858758
trigger times: 3
end of epoch 8: val_loss 0.03175427914057957, val_acc 1.0
trigger times: 4
end of epoch 9: val_loss 0.015764020352720804, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fc1.weight', tensor([[ 0.0047, -0.2423,  0.1768, -0.0007]]))])
end of epoch 10: val_loss 0.0624810698074471, val_acc 0.9774011299435028
trigger times: 1
end of epoch 11: val_loss 0.025545433453042242, val_acc 0.9943502824858758
trigger times: 2
end of epoch 12: val_loss 0.028275021906677446, val_acc 0.9943502824858758
trigger times: 3
end of epoch 13: val_loss 0.020805896561682136, val_acc 1.0
trigger times: 4
end of epoch 14: val_loss 0.018398264486180545, val_acc 1.0
trigger times: 5
end of epoch 15: val_loss 0.018593035001336064, val_acc 1.0
trigger times: 6
end of epoch 16: val_loss 0.027108222629366064, val_acc 0.9943502824858758
trigger times: 7
end of epoch 17: val_loss 0.018578105407794997, val_acc 0.9943502824858758
trigger times: 8
end of epoch 18: val_loss 0.014088573449124073, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fc1.weight', tensor([[-0.0150, -0.3436,  0.0870, -0.0806]]))])
end of epoch 19: val_loss 0.0153574591056801, val_acc 1.0
trigger times: 1
end of epoch 20: val_loss 0.021396040328479755, val_acc 0.9943502824858758
trigger times: 2
end of epoch 21: val_loss 0.01801059804835358, val_acc 1.0
trigger times: 3
end of epoch 22: val_loss 0.024089008840743657, val_acc 1.0
trigger times: 4
end of epoch 23: val_loss 0.029006859682802907, val_acc 0.9943502824858758
trigger times: 5
end of epoch 24: val_loss 0.09943873376305336, val_acc 0.96045197740113
trigger times: 6
end of epoch 25: val_loss 0.028582072321869077, val_acc 0.9943502824858758
trigger times: 7
end of epoch 26: val_loss 0.015219926953424796, val_acc 1.0
trigger times: 8
end of epoch 27: val_loss 0.01535470555712434, val_acc 1.0
trigger times: 9
end of epoch 28: val_loss 0.029601603676543988, val_acc 0.9887005649717514
trigger times: 10
Early stopping.
0 -36.36391091346741 -191.10004520678868
1 -34.001073375344276 -189.05007017278314
2 -33.86583921313286 -187.50220085194994
3 -32.51333761960268 -180.24009179391223
4 -29.497900165617466 -171.9613792356186
5 -27.913046143949032 -169.3817016847894
6 -28.288079410791397 -167.87631268052303
7 -34.543511904776096 -164.27567046290412
8 -32.06511548161507 -163.76742236654098
9 -24.02370496839285 -156.0786895876365
10 -27.415949821472168 -155.22804551507866
11 -27.69331753998995 -145.29002069283715
12 -28.572922080755234 -142.86657280197298
13 -19.229911286383867 -142.75441848091938
14 -21.8508223220706 -136.98962801176134
15 -26.211410135030746 -132.4813576990395
16 -18.694118198007345 -130.98347157271724
17 -25.37181719392538 -118.2436222554584
18 -16.1830387506634 -117.38411415155902
19 -25.45280122756958 -116.97382686219106
20 -18.713280878961086 -115.06704102317899
21 -12.022704584524035 -112.47832645551227
22 -21.188610062003136 -103.24187681049156
23 -12.151217769831419 -99.7422752992134
24 -7.396195366047323 -98.18526661663928
25 -12.138431195169687 -88.94674758091466
26 -8.35736230108887 -85.88346861832474
27 -4.866479161195457 -79.38307722218848
28 -7.2913797330111265 -79.25112655387144
29 -14.043467168696225 -73.50697855948762
30 -13.221924216486514 -72.69803191552006
31 -7.369742409326136 -71.37326168319514
32 -10.530613467097282 -68.27187815304366
33 -9.09201069176197 -65.42298162776466
34 -7.662684263661504 -65.35816021006309
35 -6.3939452692866325 -49.3186741604369
36 -18.297842787578702 -17.325441617065966
37 -7.48916479293257 -4.374511275254108
38 -7.754273656755686 -0.4258658969524013
39 -12.072488312609494 9.108146753357724
40 -10.687491578049958 17.732630180500013
41 -3.66528198774904 40.64550156746333
42 -4.359056377783418 41.11774838972275
43 -8.896821914240718 48.64917674484657
44 -5.639927634969354 52.50260039838799
45 -5.685839598067105 52.54502852695255
46 -4.850179377011955 59.52827491841281
47 -4.3282490381971 63.45606316769021
48 -3.6706470008939505 66.31570758515764
49 -11.407664339989424 67.13050507695384
50 -1.421123088337481 72.16624272108386
51 -12.46625086478889 79.16074531670823
52 -9.352566204965115 84.19837606184075
53 -9.85760703124106 89.30707154325768
54 -9.59198772162199 90.2698850730257
55 -9.163092637434602 90.28646231697512
56 -4.260451749898493 90.32667007477052
57 -2.1428660731762648 91.18021498086692
58 -3.4113227780908346 92.38187517969644
59 -9.31647483818233 92.67153902025292
60 -9.454467548057437 93.69474938898811
61 -3.4673691261559725 93.86112859581364
62 -8.889928978867829 95.24073532592021
63 -8.576792167499661 95.3630101766082
64 -8.468672384507954 96.09581961281374
65 -7.317966043949127 97.90553231413178
66 -8.03756051324308 98.07644230926206
67 -7.641933497041464 98.35872828331676
68 -8.359467244707048 98.57334710000995
69 -7.919739379547536 98.77895390936838
70 -8.490004350431263 99.10163703819775
71 -7.451391076669097 99.47183807834362
72 -7.64794565923512 101.36707045382526
73 -6.931541935540736 102.70820969163083
74 -7.379603213630617 103.61971441966246
75 -6.666309871710837 104.00039883786899
76 -6.2855473626405 104.94014608739957
77 -7.019636821933091 105.28448326377394
78 -5.818449726328254 105.60678192821229
79 -5.282452303916216 106.95774051225432
80 -6.771372441202402 107.03525723063939
81 -5.7316573625430465 107.05410485829283
82 -5.186726937070489 107.36477417456501
83 -5.378081382252276 107.72369032284489
84 -6.6632311306893826 107.78006757763964
85 -6.434665372595191 107.99397435655325
86 -5.747368300333619 108.08910889221833
87 -6.400704151950777 108.09776529440465
88 -4.916871973313391 108.24033506535473
89 -5.24055131431669 109.15712102315346
90 -5.084416082128882 109.71287331230647
91 -5.741943775676191 110.25063890779474
92 -5.031444558873773 110.45880856467717
93 -4.822055606171489 110.48639228144049
94 -4.93571497593075 111.92877271779909
95 -5.312412931583822 112.29467419620224
96 -5.250746303237975 112.73910566375015
97 -4.4919858472421765 112.98274531176438
98 -4.305560336448252 113.84616479719523
99 -3.0403470750898123 114.84612239587612
100 -4.746606703847647 115.08707834021696
101 -3.0138911735266447 115.41025719446112
102 -2.8714747838675976 115.5646624637931
103 -3.015266207046807 115.87882502857741
104 -4.304589898325503 116.51927592390606
105 -4.045052193105221 116.69939038222442
106 -3.4179510651156306 117.18809854794453
107 -3.3135057985782623 117.29357601720739
108 -2.789880624972284 117.43241923864416
109 -3.341800726018846 119.84073495876005
110 -2.1541408225893974 122.74776106687673
111 -2.032335006631911 124.10172315382671
112 -1.4546091547235847 124.42382576156477
113 -1.674189849756658 124.48179570258212
114 -1.8306013885885477 125.2590893824027
115 -1.6211436307057738 125.30457667780085
116 -1.7895271750167012 125.43458549438884
117 -0.2033994747325778 129.13346806288703
118 -0.3936150725930929 129.67718524437547
119 0.669962384738028 131.40806576766226
train accuracy: 0.9918392969240427
validation accuracy: 0.9887005649717514
