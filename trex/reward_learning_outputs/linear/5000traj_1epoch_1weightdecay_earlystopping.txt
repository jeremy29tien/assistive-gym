demo lengths 200
demos: (120, 200, 3)
demo_rewards: (120,)
[-230.92063257228807, -198.07457053036467, -190.33844567336826, -183.83211948283233, -180.07575982895574, -174.1095776560732, -173.6456142947471, -170.3281827424187, -158.78947398447332, -150.72426840032438, -148.86166075134076, -145.1413013885656, -137.5257512442629, -136.00799200710824, -132.80791318398488, -127.64744800282678, -124.57328811104271, -124.28423464695437, -118.56318065197449, -110.13353259347102, -104.80253420738944, -88.34965038791576, -88.01433039065604, -87.31501509448967, -84.4694140744479, -78.6216654066915, -78.11329924210023, -74.95892325732618, -74.0990748682081, -73.38619706857197, -71.20066823857063, -69.05358961543712, -57.52731256355119, -47.88982491135192, -22.914022639980214, -15.302099928494407, -11.47170432705338, -3.352481339560615, -3.0359431345042682, 10.383701524102266, 12.31791507638294, 23.147286761456833, 27.715718490857288, 38.42931748621134, 40.64550156746333, 43.470660445188784, 44.421830425181334, 48.47092517552915, 49.85024225668501, 50.83500721288534, 57.37099189910515, 59.14344480600431, 59.825423254209596, 71.60086573372405, 72.16624272108386, 72.20138189336342, 74.50346197053241, 79.16074531670823, 79.85576582524605, 85.85971538543265, 87.89901675087238, 90.28646231697512, 92.55040797533242, 93.76180290847655, 94.35330714471846, 94.36406762681786, 94.89653779256442, 95.17626492214514, 95.24073532592021, 95.51459906657614, 96.09581961281374, 97.5381863476066, 97.90553231413178, 98.35872828331676, 99.1993712946773, 100.4092917873045, 100.43143775194794, 101.31546799267625, 103.40571184116031, 104.00039883786899, 104.36840899433004, 104.80806380756601, 104.9622505983549, 105.40097201910157, 105.50993889701094, 105.60678192821229, 105.9665627652905, 106.67659588616937, 106.78996870447979, 107.05410485829283, 107.53698243130093, 107.72369032284489, 108.0211460573847, 108.68922641170808, 108.83371963419775, 108.97558979126147, 109.15712102315346, 109.71287331230647, 110.48639228144049, 110.7457766138548, 111.65690204215026, 111.68676464047674, 112.27871574576209, 112.34596265826099, 112.35165134828503, 112.53853634873785, 112.65771320844377, 113.60024227377659, 113.62361560845488, 113.63488581119138, 115.06164474376946, 115.5646624637931, 117.29357601720739, 117.48498856437418, 118.80804666999971, 118.9852908865604, 119.71460919058478, 129.13346806288703, 129.67718524437547, 131.40806576766226]
maximum traj length 200
num training_obs 4500
num training_labels 4500
num val_obs 500
num val_labels 500
cuda:0
epoch 0:99 loss 35.32357176499363, val_loss 0.18312695519318822, val_acc 0.98
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0519,  0.2006, -0.3093]], device='cuda:0')), ('fc1.bias', tensor([0.0009], device='cuda:0'))])
epoch 0:199 loss 15.420080263123324, val_loss 0.16046526972276962, val_acc 0.962
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0899,  0.2769, -0.1905]], device='cuda:0')), ('fc1.bias', tensor([1.1113e-05], device='cuda:0'))])
epoch 0:299 loss 19.875975225878847, val_loss 0.16884884329662211, val_acc 0.942
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1077,  0.2455, -0.1787]], device='cuda:0')), ('fc1.bias', tensor([6.8194e-06], device='cuda:0'))])
epoch 0:399 loss 18.38944835002021, val_loss 0.18241758380463147, val_acc 0.928
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1420,  0.2679, -0.0990]], device='cuda:0')), ('fc1.bias', tensor([5.0222e-07], device='cuda:0'))])
epoch 0:499 loss 27.069314923252577, val_loss 0.16032790671715083, val_acc 0.954
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1018,  0.2758, -0.1864]], device='cuda:0')), ('fc1.bias', tensor([-8.0678e-07], device='cuda:0'))])
epoch 0:599 loss 19.24573825997407, val_loss 0.16548310309384395, val_acc 0.978
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0636,  0.2938, -0.1923]], device='cuda:0')), ('fc1.bias', tensor([3.9884e-06], device='cuda:0'))])
epoch 0:699 loss 15.722091106156505, val_loss 0.1767057564112314, val_acc 0.934
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1111,  0.2354, -0.1432]], device='cuda:0')), ('fc1.bias', tensor([3.2847e-06], device='cuda:0'))])
epoch 0:799 loss 18.63019041338879, val_loss 0.1737338944158564, val_acc 0.984
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0543,  0.2925, -0.1739]], device='cuda:0')), ('fc1.bias', tensor([1.7104e-06], device='cuda:0'))])
epoch 0:899 loss 14.58826964975497, val_loss 0.17975730983745178, val_acc 0.932
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1261,  0.2453, -0.1237]], device='cuda:0')), ('fc1.bias', tensor([-2.3088e-06], device='cuda:0'))])
epoch 0:999 loss 18.373768874446895, val_loss 0.20367252787135226, val_acc 0.92
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1433,  0.1966, -0.1035]], device='cuda:0')), ('fc1.bias', tensor([1.8025e-06], device='cuda:0'))])
epoch 0:1099 loss 16.113290262137, val_loss 0.16207104449120607, val_acc 0.96
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0927,  0.2762, -0.1748]], device='cuda:0')), ('fc1.bias', tensor([-1.3616e-06], device='cuda:0'))])
epoch 0:1199 loss 20.395466588847952, val_loss 0.1768526592639173, val_acc 0.996
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0365,  0.3487, -0.2044]], device='cuda:0')), ('fc1.bias', tensor([-1.2575e-07], device='cuda:0'))])
epoch 0:1299 loss 19.292066447754138, val_loss 0.16688131377544427, val_acc 0.938
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1161,  0.2929, -0.1121]], device='cuda:0')), ('fc1.bias', tensor([-2.2467e-06], device='cuda:0'))])
epoch 0:1399 loss 20.107823881685697, val_loss 0.16063942226858358, val_acc 0.962
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0880,  0.2987, -0.1498]], device='cuda:0')), ('fc1.bias', tensor([-4.1024e-06], device='cuda:0'))])
epoch 0:1499 loss 17.647517888750933, val_loss 0.19649902391324198, val_acc 0.926
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1290,  0.2040, -0.1010]], device='cuda:0')), ('fc1.bias', tensor([2.3518e-06], device='cuda:0'))])
epoch 0:1599 loss 19.994315530711333, val_loss 0.15793677322316046, val_acc 0.968
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0772,  0.3141, -0.1689]], device='cuda:0')), ('fc1.bias', tensor([7.7192e-06], device='cuda:0'))])
epoch 0:1699 loss 17.872692589069672, val_loss 0.1863770023487348, val_acc 0.93
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1222,  0.2107, -0.1394]], device='cuda:0')), ('fc1.bias', tensor([-5.5141e-06], device='cuda:0'))])
epoch 0:1799 loss 17.865033379504702, val_loss 0.15286125601402337, val_acc 0.98
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0692,  0.3438, -0.1958]], device='cuda:0')), ('fc1.bias', tensor([-2.9122e-06], device='cuda:0'))])
epoch 0:1899 loss 15.960421487698795, val_loss 0.16797492446754328, val_acc 0.954
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0947,  0.2603, -0.1562]], device='cuda:0')), ('fc1.bias', tensor([-5.8705e-06], device='cuda:0'))])
epoch 0:1999 loss 18.37382812921193, val_loss 0.17073204624720104, val_acc 0.948
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0979,  0.2467, -0.1600]], device='cuda:0')), ('fc1.bias', tensor([1.1463e-06], device='cuda:0'))])
epoch 0:2099 loss 19.302212593039073, val_loss 0.16391873378970923, val_acc 0.98
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0639,  0.3297, -0.1382]], device='cuda:0')), ('fc1.bias', tensor([1.5900e-06], device='cuda:0'))])
epoch 0:2199 loss 17.601847321085664, val_loss 0.16917267868768032, val_acc 0.964
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0723,  0.2889, -0.1327]], device='cuda:0')), ('fc1.bias', tensor([8.5597e-06], device='cuda:0'))])
epoch 0:2299 loss 17.948937675546908, val_loss 0.17010926500436122, val_acc 0.938
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1141,  0.2595, -0.1476]], device='cuda:0')), ('fc1.bias', tensor([2.3488e-05], device='cuda:0'))])
epoch 0:2399 loss 21.979031209744726, val_loss 0.254064446657896, val_acc 0.992
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0026,  0.3258, -0.1988]], device='cuda:0')), ('fc1.bias', tensor([-2.5148e-06], device='cuda:0'))])
epoch 0:2499 loss 18.022879812968988, val_loss 0.16882524472818927, val_acc 0.944
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1039,  0.2649, -0.1400]], device='cuda:0')), ('fc1.bias', tensor([3.2801e-06], device='cuda:0'))])
epoch 0:2599 loss 19.780882149903704, val_loss 0.18130878563638692, val_acc 0.94
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0941,  0.2410, -0.0967]], device='cuda:0')), ('fc1.bias', tensor([6.0613e-06], device='cuda:0'))])
epoch 0:2699 loss 14.773258906864925, val_loss 0.1843515482089169, val_acc 0.93
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1321,  0.2578, -0.0824]], device='cuda:0')), ('fc1.bias', tensor([-8.8590e-06], device='cuda:0'))])
epoch 0:2799 loss 19.948372363685564, val_loss 0.18267901384556945, val_acc 0.934
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1114,  0.2379, -0.0986]], device='cuda:0')), ('fc1.bias', tensor([1.6527e-06], device='cuda:0'))])
epoch 0:2899 loss 15.709596314403335, val_loss 0.17510219158295387, val_acc 0.938
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1061,  0.2542, -0.1154]], device='cuda:0')), ('fc1.bias', tensor([1.8226e-06], device='cuda:0'))])
epoch 0:2999 loss 17.75302412343764, val_loss 0.17986159090416706, val_acc 0.934
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1103,  0.2162, -0.1577]], device='cuda:0')), ('fc1.bias', tensor([-1.3275e-05], device='cuda:0'))])
epoch 0:3099 loss 20.884572940593216, val_loss 0.17469784434688357, val_acc 0.936
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1099,  0.2475, -0.1332]], device='cuda:0')), ('fc1.bias', tensor([-1.5903e-05], device='cuda:0'))])
epoch 0:3199 loss 22.036443869412295, val_loss 0.1682699959867325, val_acc 0.95
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1015,  0.2601, -0.1527]], device='cuda:0')), ('fc1.bias', tensor([8.4358e-06], device='cuda:0'))])
epoch 0:3299 loss 21.128124239340963, val_loss 0.1643866800974921, val_acc 0.962
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0882,  0.2709, -0.1707]], device='cuda:0')), ('fc1.bias', tensor([-7.5650e-06], device='cuda:0'))])
epoch 0:3399 loss 20.60373030037246, val_loss 0.1694977848542864, val_acc 0.934
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1313,  0.2736, -0.1531]], device='cuda:0')), ('fc1.bias', tensor([1.2268e-05], device='cuda:0'))])
epoch 0:3499 loss 19.490598704042327, val_loss 0.18110366393069072, val_acc 0.932
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1184,  0.2435, -0.1064]], device='cuda:0')), ('fc1.bias', tensor([-6.7371e-06], device='cuda:0'))])
epoch 0:3599 loss 23.36065382829065, val_loss 0.15836364355170066, val_acc 0.976
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0728,  0.2940, -0.2195]], device='cuda:0')), ('fc1.bias', tensor([4.5961e-06], device='cuda:0'))])
epoch 0:3699 loss 23.834982702129757, val_loss 0.16750872352204807, val_acc 0.98
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0628,  0.3071, -0.1522]], device='cuda:0')), ('fc1.bias', tensor([9.3680e-06], device='cuda:0'))])
epoch 0:3799 loss 23.11780743598581, val_loss 0.16619663947293117, val_acc 0.958
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0911,  0.2644, -0.1651]], device='cuda:0')), ('fc1.bias', tensor([3.7499e-06], device='cuda:0'))])
epoch 0:3899 loss 16.271473161864208, val_loss 0.17088968897617485, val_acc 0.952
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0950,  0.2681, -0.1191]], device='cuda:0')), ('fc1.bias', tensor([2.2533e-06], device='cuda:0'))])
epoch 0:3999 loss 16.764288812657576, val_loss 0.17022864870604185, val_acc 0.962
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0794,  0.2673, -0.1454]], device='cuda:0')), ('fc1.bias', tensor([1.4132e-05], device='cuda:0'))])
epoch 0:4099 loss 20.900362524857705, val_loss 0.16084350542156267, val_acc 0.962
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0828,  0.3152, -0.1268]], device='cuda:0')), ('fc1.bias', tensor([-9.1467e-06], device='cuda:0'))])
epoch 0:4199 loss 19.058551492146364, val_loss 0.16377790773488163, val_acc 0.95
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1038,  0.2804, -0.1497]], device='cuda:0')), ('fc1.bias', tensor([-1.0392e-05], device='cuda:0'))])
epoch 0:4299 loss 16.381958379688733, val_loss 0.17001242265378938, val_acc 0.954
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.0947,  0.2614, -0.1384]], device='cuda:0')), ('fc1.bias', tensor([2.8001e-05], device='cuda:0'))])
epoch 0:4399 loss 19.114340219952275, val_loss 0.1608743705353125, val_acc 0.944
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1136,  0.3019, -0.1390]], device='cuda:0')), ('fc1.bias', tensor([-9.5219e-06], device='cuda:0'))])
epoch 0:4499 loss 18.055126551144376, val_loss 0.16924279127902114, val_acc 0.938
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1090,  0.2645, -0.1401]], device='cuda:0')), ('fc1.bias', tensor([4.8657e-06], device='cuda:0'))])
finished training
0 -21.31459630280733 -230.92063257228807
1 -19.465360783040524 -198.07457053036467
2 -16.89414417371154 -190.33844567336826
3 -16.30206812545657 -183.83211948283233
4 -16.32139716297388 -180.07575982895574
5 -16.443950582295656 -174.1095776560732
6 -14.817579977214336 -173.6456142947471
7 -16.041570760309696 -170.3281827424187
8 -12.842874992638826 -158.78947398447332
9 -13.030647601932287 -150.72426840032438
10 -12.80631399154663 -148.86166075134076
11 -11.722266521304846 -145.1413013885656
12 -11.78527769818902 -137.5257512442629
13 -10.310971308499575 -136.00799200710824
14 -12.44488075748086 -132.80791318398488
15 -12.259987957775593 -127.64744800282678
16 -7.958163746166974 -124.57328811104271
17 -8.518317207694054 -124.28423464695437
18 -8.518777130171657 -118.56318065197449
19 -7.7694414388388395 -110.13353259347102
20 -8.695798190310597 -104.80253420738944
21 -6.80341087654233 -88.34965038791576
22 -7.911701139062643 -88.01433039065604
23 -7.521420972887427 -87.31501509448967
24 -7.209104802459478 -84.4694140744479
25 -7.767902685329318 -78.6216654066915
26 -7.105022335425019 -78.11329924210023
27 -6.526985771022737 -74.95892325732618
28 -8.633066460024565 -74.0990748682081
29 -3.181293956004083 -73.38619706857197
30 -6.868416024371982 -71.20066823857063
31 -6.249895258806646 -69.05358961543712
32 -5.170852162875235 -57.52731256355119
33 -4.031707004178315 -47.88982491135192
34 -4.450214745244011 -22.914022639980214
35 -3.298810341861099 -15.302099928494407
36 -4.251067827455699 -11.47170432705338
37 -5.310078131500632 -3.352481339560615
38 -4.7471691141836345 -3.0359431345042682
39 -3.6479863326530904 10.383701524102266
40 -4.067055901046842 12.31791507638294
41 -5.446733461460099 23.147286761456833
42 -3.7775692753493786 27.715718490857288
43 -2.5953704388812184 38.42931748621134
44 -2.383688314119354 40.64550156746333
45 -3.448740446008742 43.470660445188784
46 -3.6447768616490066 44.421830425181334
47 -4.023179259616882 48.47092517552915
48 -1.6813304610550404 49.85024225668501
49 -3.283113752724603 50.83500721288534
50 -5.656661603599787 57.37099189910515
51 -3.8931496418081224 59.14344480600431
52 -2.8021978097967803 59.825423254209596
53 -3.695594941265881 71.60086573372405
54 -1.1856297489721328 72.16624272108386
55 -1.7232706085778773 72.20138189336342
56 -2.1265408443287015 74.50346197053241
57 -4.704771943390369 79.16074531670823
58 -5.241720138583332 79.85576582524605
59 -4.660228555789217 85.85971538543265
60 -4.035911459708586 87.89901675087238
61 -3.447643146617338 90.28646231697512
62 -3.3202351715881377 92.55040797533242
63 -1.775977143086493 93.76180290847655
64 -3.2703810608945787 94.35330714471846
65 -3.751819975208491 94.36406762681786
66 -3.2096594849135727 94.89653779256442
67 -3.381649974035099 95.17626492214514
68 -3.2763286798726767 95.24073532592021
69 -3.443055233452469 95.51459906657614
70 -3.1307169245555997 96.09581961281374
71 -3.051131113898009 97.5381863476066
72 -2.6318697610404342 97.90553231413178
73 -2.8502766145393252 98.35872828331676
74 -2.937323338817805 99.1993712946773
75 -2.9046873743645847 100.4092917873045
76 -2.8565693283453584 100.43143775194794
77 -3.007004529237747 101.31546799267625
78 -2.413934874581173 103.40571184116031
79 -2.4659038165118545 104.00039883786899
80 -2.6900609177537262 104.36840899433004
81 -1.6230046276468784 104.80806380756601
82 -2.62478130008094 104.9622505983549
83 -2.483417969662696 105.40097201910157
84 -2.299087703693658 105.50993889701094
85 -2.2651758070569485 105.60678192821229
86 -2.1689549954608083 105.9665627652905
87 -2.3766022617928684 106.67659588616937
88 -1.9829265396110713 106.78996870447979
89 -2.1517914484720677 107.05410485829283
90 -2.3518119025975466 107.53698243130093
91 -1.784212477505207 107.72369032284489
92 -2.3325467449612916 108.0211460573847
93 -2.2089035254903138 108.68922641170808
94 -2.3060026080347598 108.83371963419775
95 -2.1172794830054045 108.97558979126147
96 -1.8767324797809124 109.15712102315346
97 -2.0172270466573536 109.71287331230647
98 -1.6108530194032937 110.48639228144049
99 -1.918078729417175 110.7457766138548
100 -2.0982117941603065 111.65690204215026
101 -1.888102316763252 111.68676464047674
102 -1.4673087149858475 112.27871574576209
103 -1.821611988125369 112.34596265826099
104 -1.9748295093886554 112.35165134828503
105 -1.5775102355983108 112.53853634873785
106 -1.974061484215781 112.65771320844377
107 -1.4415915650315583 113.60024227377659
108 -1.657677453244105 113.62361560845488
109 -1.8552470698487014 113.63488581119138
110 -1.674841686617583 115.06164474376946
111 -1.1667354884557426 115.5646624637931
112 -1.2128047856967896 117.29357601720739
113 -1.236535993637517 117.48498856437418
114 -1.3163600196130574 118.80804666999971
115 -1.1912820115685463 118.9852908865604
116 -0.7071684096008539 119.71460919058478
117 0.13063977076672018 129.13346806288703
118 0.08588063670322299 129.67718524437547
119 0.40907514397986233 131.40806576766226
train accuracy: 0.9402222222222222
validation accuracy: 0.938
