demo lengths 200
demos: (120, 200, 3)
demo_rewards: (120,)
[-230.92063257228807, -198.07457053036467, -190.33844567336826, -183.83211948283233, -180.07575982895574, -174.1095776560732, -173.6456142947471, -170.3281827424187, -158.78947398447332, -150.72426840032438, -148.86166075134076, -145.1413013885656, -137.5257512442629, -136.00799200710824, -132.80791318398488, -127.64744800282678, -124.57328811104271, -124.28423464695437, -118.56318065197449, -110.13353259347102, -104.80253420738944, -88.34965038791576, -88.01433039065604, -87.31501509448967, -84.4694140744479, -78.6216654066915, -78.11329924210023, -74.95892325732618, -74.0990748682081, -73.38619706857197, -71.20066823857063, -69.05358961543712, -57.52731256355119, -47.88982491135192, -22.914022639980214, -15.302099928494407, -11.47170432705338, -3.352481339560615, -3.0359431345042682, 10.383701524102266, 12.31791507638294, 23.147286761456833, 27.715718490857288, 38.42931748621134, 40.64550156746333, 43.470660445188784, 44.421830425181334, 48.47092517552915, 49.85024225668501, 50.83500721288534, 57.37099189910515, 59.14344480600431, 59.825423254209596, 71.60086573372405, 72.16624272108386, 72.20138189336342, 74.50346197053241, 79.16074531670823, 79.85576582524605, 85.85971538543265, 87.89901675087238, 90.28646231697512, 92.55040797533242, 93.76180290847655, 94.35330714471846, 94.36406762681786, 94.89653779256442, 95.17626492214514, 95.24073532592021, 95.51459906657614, 96.09581961281374, 97.5381863476066, 97.90553231413178, 98.35872828331676, 99.1993712946773, 100.4092917873045, 100.43143775194794, 101.31546799267625, 103.40571184116031, 104.00039883786899, 104.36840899433004, 104.80806380756601, 104.9622505983549, 105.40097201910157, 105.50993889701094, 105.60678192821229, 105.9665627652905, 106.67659588616937, 106.78996870447979, 107.05410485829283, 107.53698243130093, 107.72369032284489, 108.0211460573847, 108.68922641170808, 108.83371963419775, 108.97558979126147, 109.15712102315346, 109.71287331230647, 110.48639228144049, 110.7457766138548, 111.65690204215026, 111.68676464047674, 112.27871574576209, 112.34596265826099, 112.35165134828503, 112.53853634873785, 112.65771320844377, 113.60024227377659, 113.62361560845488, 113.63488581119138, 115.06164474376946, 115.5646624637931, 117.29357601720739, 117.48498856437418, 118.80804666999971, 118.9852908865604, 119.71460919058478, 129.13346806288703, 129.67718524437547, 131.40806576766226]
maximum traj length 200
num training_obs 4500
num training_labels 4500
num val_obs 500
num val_labels 500
cuda:0
epoch 0:99 loss 23.652664613437302, val_loss 0.14231803469684293, val_acc 0.962
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1030,  0.4133, -0.3932]], device='cuda:0')), ('fc1.bias', tensor([0.4658], device='cuda:0'))])
epoch 0:199 loss 10.153873957950594, val_loss 0.10740777225203155, val_acc 0.98
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1063,  0.6074, -0.5817]], device='cuda:0')), ('fc1.bias', tensor([0.7441], device='cuda:0'))])
epoch 0:299 loss 7.7288812652837535, val_loss 0.09072317808474961, val_acc 0.986
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1128,  0.7809, -0.7108]], device='cuda:0')), ('fc1.bias', tensor([0.8777], device='cuda:0'))])
epoch 0:399 loss 9.152683009191271, val_loss 0.08147885904014525, val_acc 0.986
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1287,  0.8736, -0.8236]], device='cuda:0')), ('fc1.bias', tensor([1.0806], device='cuda:0'))])
epoch 0:499 loss 7.153409705200161, val_loss 0.08786037316241459, val_acc 0.968
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.2090,  0.9470, -0.9030]], device='cuda:0')), ('fc1.bias', tensor([1.1682], device='cuda:0'))])
epoch 0:599 loss 3.8072709550461923, val_loss 0.09796019575858282, val_acc 0.964
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.2486,  1.0326, -0.9397]], device='cuda:0')), ('fc1.bias', tensor([1.3121], device='cuda:0'))])
epoch 0:699 loss 5.282833774515304, val_loss 0.09338168050846932, val_acc 0.966
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.2587,  1.1111, -1.0032]], device='cuda:0')), ('fc1.bias', tensor([1.6112], device='cuda:0'))])
epoch 0:799 loss 7.021859680924372, val_loss 0.06308285150842868, val_acc 0.982
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.2054,  1.2437, -1.1443]], device='cuda:0')), ('fc1.bias', tensor([1.6279], device='cuda:0'))])
epoch 0:899 loss 4.643782436526543, val_loss 0.061515875754236914, val_acc 0.992
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1414,  1.3853, -1.2479]], device='cuda:0')), ('fc1.bias', tensor([1.7350], device='cuda:0'))])
epoch 0:999 loss 2.7566759075075566, val_loss 0.05595043339651943, val_acc 0.992
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.1684,  1.4828, -1.2469]], device='cuda:0')), ('fc1.bias', tensor([1.8820], device='cuda:0'))])
epoch 0:1099 loss 3.9987926556608286, val_loss 0.06055320903263291, val_acc 0.978
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.2609,  1.5395, -1.2736]], device='cuda:0')), ('fc1.bias', tensor([2.1629], device='cuda:0'))])
epoch 0:1199 loss 6.831135311105335, val_loss 0.04900538549728351, val_acc 0.99
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.2215,  1.6641, -1.4392]], device='cuda:0')), ('fc1.bias', tensor([2.3736], device='cuda:0'))])
epoch 0:1299 loss 4.945503854059432, val_loss 0.04952037420793592, val_acc 0.984
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.2528,  1.7036, -1.4943]], device='cuda:0')), ('fc1.bias', tensor([2.4830], device='cuda:0'))])
epoch 0:1399 loss 5.372889782530784, val_loss 0.05259336687806268, val_acc 0.98
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.2889,  1.7575, -1.5669]], device='cuda:0')), ('fc1.bias', tensor([2.5817], device='cuda:0'))])
epoch 0:1499 loss 3.5521348986474592, val_loss 0.04603625671909759, val_acc 0.986
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.2699,  1.8417, -1.6627]], device='cuda:0')), ('fc1.bias', tensor([2.7850], device='cuda:0'))])
epoch 0:1599 loss 2.3745719738996414, val_loss 0.043133355127726716, val_acc 0.99
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.2301,  1.9323, -1.7469]], device='cuda:0')), ('fc1.bias', tensor([2.8858], device='cuda:0'))])
epoch 0:1699 loss 2.20818455741761, val_loss 0.04298363779619693, val_acc 0.986
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.2773,  1.9763, -1.7812]], device='cuda:0')), ('fc1.bias', tensor([2.9794], device='cuda:0'))])
epoch 0:1799 loss 3.4608869352682845, val_loss 0.04335563473376912, val_acc 0.986
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.3002,  2.0518, -1.8286]], device='cuda:0')), ('fc1.bias', tensor([3.3384], device='cuda:0'))])
epoch 0:1899 loss 4.551773616983191, val_loss 0.03954009147384255, val_acc 0.99
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.2617,  2.1371, -1.9443]], device='cuda:0')), ('fc1.bias', tensor([3.6105], device='cuda:0'))])
epoch 0:1999 loss 3.4988902816496363, val_loss 0.041112063801361384, val_acc 0.986
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.3156,  2.1810, -1.9793]], device='cuda:0')), ('fc1.bias', tensor([3.7252], device='cuda:0'))])
epoch 0:2099 loss 2.9861039170953703, val_loss 0.03761539400593988, val_acc 0.992
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.2662,  2.2940, -2.1198]], device='cuda:0')), ('fc1.bias', tensor([3.8896], device='cuda:0'))])
epoch 0:2199 loss 4.222987888561356, val_loss 0.03988446007550114, val_acc 0.986
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.3437,  2.3299, -2.1655]], device='cuda:0')), ('fc1.bias', tensor([4.1038], device='cuda:0'))])
epoch 0:2299 loss 2.996066519919381, val_loss 0.03593008688735428, val_acc 0.99
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.3091,  2.4296, -2.2479]], device='cuda:0')), ('fc1.bias', tensor([4.4884], device='cuda:0'))])
epoch 0:2399 loss 3.391792748982432, val_loss 0.0351707175973293, val_acc 0.992
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.2942,  2.4963, -2.3284]], device='cuda:0')), ('fc1.bias', tensor([4.7593], device='cuda:0'))])
epoch 0:2499 loss 1.9733539047253856, val_loss 0.035445099026035466, val_acc 0.99
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.3399,  2.5343, -2.3746]], device='cuda:0')), ('fc1.bias', tensor([5.0689], device='cuda:0'))])
epoch 0:2599 loss 4.91795614993314, val_loss 0.036021172759865695, val_acc 0.992
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.2539,  2.7127, -2.5630]], device='cuda:0')), ('fc1.bias', tensor([5.1650], device='cuda:0'))])
epoch 0:2699 loss 4.020648206849373, val_loss 0.033106274456358775, val_acc 0.99
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.3235,  2.7208, -2.5618]], device='cuda:0')), ('fc1.bias', tensor([5.2872], device='cuda:0'))])
epoch 0:2799 loss 2.2978472547752986, val_loss 0.034306739761354905, val_acc 0.99
check pointing
Weights: OrderedDict([('fc1.weight', tensor([[-0.3694,  2.7238, -2.5627]], device='cuda:0')), ('fc1.bias', tensor([5.3785], device='cuda:0'))])
Early stopping.
0 993.8352282047272 -230.92063257228807
1 1008.5247974395752 -198.07457053036467
2 1009.0587215423584 -190.33844567336826
3 1011.0977604389191 -183.83211948283233
4 1013.1114466190338 -180.07575982895574
5 1016.8499076366425 -174.1095776560732
6 1014.1298983097076 -173.6456142947471
7 1018.2356524467468 -170.3281827424187
8 1020.9305365085602 -158.78947398447332
9 1024.444384098053 -150.72426840032438
10 1025.2169632911682 -148.86166075134076
11 1024.7897741794586 -145.1413013885656
12 1032.8937995433807 -137.5257512442629
13 1029.650104045868 -136.00799200710824
14 1032.702464580536 -132.80791318398488
15 1035.4194719791412 -127.64744800282678
16 1037.7528805732727 -124.57328811104271
17 1035.8237869739532 -124.28423464695437
18 1035.8221971988678 -118.56318065197449
19 1040.483076095581 -110.13353259347102
20 1043.5335936546326 -104.80253420738944
21 1052.1309986114502 -88.34965038791576
22 1050.3944356441498 -88.01433039065604
23 1051.7385149002075 -87.31501509448967
24 1052.8140907287598 -84.4694140744479
25 1055.0501747131348 -78.6216654066915
26 1055.2527966499329 -78.11329924210023
27 1055.163225889206 -74.95892325732618
28 1047.3756160736084 -74.0990748682081
29 1064.6051223278046 -73.38619706857197
30 1058.1478943824768 -71.20066823857063
31 1058.197749376297 -69.05358961543712
32 1063.99409866333 -57.52731256355119
33 1061.4091367721558 -47.88982491135192
34 1061.51358127594 -22.914022639980214
35 1071.9869108200073 -15.302099928494407
36 1070.7877135276794 -11.47170432705338
37 1064.5258297920227 -3.352481339560615
38 1064.3841636180878 -3.0359431345042682
39 1074.6776452064514 10.383701524102266
40 1070.8869018554688 12.31791507638294
41 1069.7614538669586 23.147286761456833
42 1071.6165981292725 27.715718490857288
43 1075.6879396438599 38.42931748621134
44 1076.4169478416443 40.64550156746333
45 1076.9095561504364 43.470660445188784
46 1078.3146905899048 44.421830425181334
47 1074.6639955043793 48.47092517552915
48 1078.8357846736908 49.85024225668501
49 1077.2126898765564 50.83500721288534
50 1072.931735277176 57.37099189910515
51 1079.2723178863525 59.14344480600431
52 1078.8689057826996 59.825423254209596
53 1079.6854107379913 71.60086573372405
54 1084.4361696243286 72.16624272108386
55 1082.5845992565155 72.20138189336342
56 1085.356297492981 74.50346197053241
57 1080.1031818389893 79.16074531670823
58 1078.2539958953857 79.85576582524605
59 1080.2565822601318 85.85971538543265
60 1082.4066514968872 87.89901675087238
61 1084.4325771331787 90.28646231697512
62 1084.8713555335999 92.55040797533242
63 1086.2963361740112 93.76180290847655
64 1085.0430455207825 94.35330714471846
65 1083.3850321769714 94.36406762681786
66 1085.2521605491638 94.89653779256442
67 1084.659852027893 95.17626492214514
68 1085.0225610733032 95.24073532592021
69 1084.448377609253 95.51459906657614
70 1085.5240330696106 96.09581961281374
71 1085.7981152534485 97.5381863476066
72 1087.2419996261597 97.90553231413178
73 1086.4898319244385 98.35872828331676
74 1086.1900563240051 99.1993712946773
75 1086.3024516105652 100.4092917873045
76 1086.4681587219238 100.43143775194794
77 1085.9500832557678 101.31546799267625
78 1087.9925384521484 103.40571184116031
79 1087.8135662078857 104.00039883786899
80 1087.0415921211243 104.36840899433004
81 1090.716407775879 104.80806380756601
82 1087.2664136886597 104.9622505983549
83 1087.7532482147217 105.40097201910157
84 1088.3880596160889 105.50993889701094
85 1088.5048451423645 105.60678192821229
86 1088.836224079132 105.9665627652905
87 1088.1211099624634 106.67659588616937
88 1089.4768800735474 106.78996870447979
89 1088.8953304290771 107.05410485829283
90 1088.2064833641052 107.53698243130093
91 1090.1612267494202 107.72369032284489
92 1088.2728352546692 108.0211460573847
93 1088.6986417770386 108.68922641170808
94 1088.364248752594 108.83371963419775
95 1089.0141878128052 108.97558979126147
96 1089.8426055908203 109.15712102315346
97 1089.35875082016 109.71287331230647
98 1090.75825881958 110.48639228144049
99 1089.7002086639404 110.7457766138548
100 1089.0798478126526 111.65690204215026
101 1089.8034420013428 111.68676464047674
102 1091.2525992393494 112.27871574576209
103 1090.0324263572693 112.34596265826099
104 1089.5047631263733 112.35165134828503
105 1090.873083114624 112.53853634873785
106 1089.507411956787 112.65771320844377
107 1091.34117269516 113.60024227377659
108 1090.5969972610474 113.62361560845488
109 1089.9165902137756 113.63488581119138
110 1090.5378856658936 115.06164474376946
111 1092.2877402305603 115.5646624637931
112 1092.1290860176086 117.29357601720739
113 1092.047357082367 117.48498856437418
114 1091.7724537849426 118.80804666999971
115 1092.203206062317 118.9852908865604
116 1093.8704352378845 119.71460919058478
117 1096.755741596222 129.13346806288703
118 1096.6015920639038 129.67718524437547
119 1097.7146401405334 131.40806576766226
train accuracy: 0.9937777777777778
validation accuracy: 0.99
