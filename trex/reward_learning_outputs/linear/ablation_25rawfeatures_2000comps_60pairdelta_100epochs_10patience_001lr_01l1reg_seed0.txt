demo lengths 200
demos: (120, 200, 28)
demo_rewards: (120,)
[-191.10004521 -189.05007017 -187.50220085 -180.24009179 -171.96137924
 -169.38170168 -167.87631268 -164.27567046 -163.76742237 -156.07868959
 -155.22804552 -145.29002069 -142.8665728  -142.75441848 -136.98962801
 -132.4813577  -130.98347157 -118.24362226 -117.38411415 -116.97382686
 -115.06704102 -112.47832646 -103.24187681  -99.7422753   -98.18526662
  -88.94674758  -85.88346862  -79.38307722  -79.25112655  -73.50697856
  -72.69803192  -71.37326168  -68.27187815  -65.42298163  -65.35816021
  -49.31867416  -17.32544162   -4.37451128   -0.4258659     9.10814675
   17.73263018   40.64550157   41.11774839   48.64917674   52.5026004
   52.54502853   59.52827492   63.45606317   66.31570759   67.13050508
   72.16624272   79.16074532   84.19837606   89.30707154   90.26988507
   90.28646232   90.32667007   91.18021498   92.38187518   92.67153902
   93.69474939   93.8611286    95.24073533   95.36301018   96.09581961
   97.90553231   98.07644231   98.35872828   98.5733471    98.77895391
   99.10163704   99.47183808  101.36707045  102.70820969  103.61971442
  104.00039884  104.94014609  105.28448326  105.60678193  106.95774051
  107.03525723  107.05410486  107.36477417  107.72369032  107.78006758
  107.99397436  108.08910889  108.09776529  108.24033507  109.15712102
  109.71287331  110.25063891  110.45880856  110.48639228  111.92877272
  112.2946742   112.73910566  112.98274531  113.8461648   114.8461224
  115.08707834  115.41025719  115.56466246  115.87882503  116.51927592
  116.69939038  117.18809855  117.29357602  117.43241924  119.84073496
  122.74776107  124.10172315  124.42382576  124.4817957   125.25908938
  125.30457668  125.43458549  129.13346806  129.67718524  131.40806577]
maximum traj length 200
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
cuda:0
end of epoch 0: val_loss 0.6976085279293079, val_acc 0.945
trigger times: 0
saving model weights...
Weights: OrderedDict([('fc1.weight', tensor([[ 0.3222,  0.0429, -0.0151, -0.0082, -0.0152, -0.0201,  0.2317, -0.0236,
          0.2802,  0.0293, -0.0521,  0.0016, -0.0825, -0.0571,  0.0530, -0.0667,
         -0.0710,  0.2359, -0.1898, -0.0988, -0.4204, -0.2334, -0.0774, -0.0415,
          0.1829, -0.6199,  0.9164, -0.2768]], device='cuda:0'))])
end of epoch 1: val_loss 0.5262795260839891, val_acc 0.97
trigger times: 0
saving model weights...
Weights: OrderedDict([('fc1.weight', tensor([[ 3.1716e-01,  9.3327e-02,  3.0173e-05, -6.8730e-02,  4.3913e-02,
          9.9676e-02,  1.8310e-03,  2.4169e-02,  2.8861e-01, -2.3881e-03,
          1.0240e-01, -8.9378e-02,  3.0145e-02,  1.0668e-02,  1.1091e-01,
          4.2926e-02, -1.4733e-01,  1.0250e-01, -4.5455e-02,  7.5621e-02,
         -1.5854e-01, -1.0994e-01,  7.8149e-02,  1.1096e-02,  3.5989e-01,
         -1.1195e+00,  7.7969e-01,  1.2253e-03]], device='cuda:0'))])
end of epoch 2: val_loss 0.3103184122408206, val_acc 0.98
trigger times: 0
saving model weights...
Weights: OrderedDict([('fc1.weight', tensor([[ 4.0297e-01,  5.9905e-03, -5.8352e-02,  1.7231e-02,  4.1956e-02,
          2.6273e-02,  1.2068e-01,  5.6566e-02, -1.1155e-02,  1.5525e-04,
         -2.0468e-03,  9.7974e-02,  3.5077e-02, -2.1555e-01, -5.9675e-02,
          8.7616e-02,  4.0155e-02,  1.9551e-01, -4.6449e-02, -5.8313e-02,
         -2.8128e-01, -1.4526e-01,  8.0594e-02,  1.6704e-02,  3.6252e-01,
         -1.0155e+00,  3.0362e-01, -3.5636e-02]], device='cuda:0'))])
end of epoch 3: val_loss 0.20157084353028723, val_acc 0.98
trigger times: 0
saving model weights...
Weights: OrderedDict([('fc1.weight', tensor([[ 2.0050e-01, -9.9621e-05,  5.8188e-04, -9.8405e-02, -9.1524e-06,
          3.9039e-05,  2.7676e-01, -7.2167e-05,  1.8513e-01, -1.7989e-04,
          1.7110e-04, -6.8373e-06, -1.4868e-01, -5.8880e-02,  1.2717e-01,
         -9.8459e-02, -1.6201e-01,  5.0053e-02, -4.8431e-02,  4.4996e-06,
         -2.3127e-01, -2.3987e-02,  3.5404e-02,  8.4265e-06,  3.0643e-01,
         -1.3295e+00,  2.9818e-01, -1.5092e-04]], device='cuda:0'))])
end of epoch 4: val_loss 0.09065330262951388, val_acc 0.99
trigger times: 0
saving model weights...
Weights: OrderedDict([('fc1.weight', tensor([[-3.0191e-04,  9.0618e-03, -6.9922e-05, -9.0944e-05, -8.0207e-05,
          3.6051e-02,  2.1771e-05,  1.7860e-04, -8.9649e-06, -1.9288e-04,
         -3.1417e-04, -3.7804e-05, -6.7632e-06, -7.4467e-02,  3.1982e-05,
         -8.4403e-02,  2.1288e-05,  5.4965e-03,  2.0109e-06, -8.0258e-05,
         -1.1423e-01,  1.0723e-05, -3.4669e-05,  6.2670e-07,  2.8566e-01,
         -1.0302e+00,  6.3253e-05,  6.0796e-04]], device='cuda:0'))])
end of epoch 5: val_loss 0.029106410144579372, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fc1.weight', tensor([[ 0.1260, -0.0129,  0.0430, -0.0855, -0.0232, -0.0018,  0.1265, -0.1105,
          0.0957,  0.1767,  0.0036, -0.0430, -0.0974, -0.1140,  0.1710, -0.0275,
         -0.0306,  0.0574, -0.0289, -0.0441, -0.3293, -0.2741,  0.1081,  0.0253,
          0.2411, -1.5899,  0.0730, -0.0925]], device='cuda:0'))])
end of epoch 6: val_loss 0.2055706682859425, val_acc 0.98
trigger times: 1
end of epoch 7: val_loss 0.018756925050258673, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fc1.weight', tensor([[ 1.1915e-01, -1.1951e-05, -1.2194e-04, -1.3577e-04, -4.6344e-05,
         -8.1913e-05,  2.9066e-05,  6.5691e-06,  4.1971e-05,  1.5531e-04,
         -1.5532e-05, -9.5601e-05,  2.0718e-05, -6.7777e-02,  1.0841e-01,
          4.8275e-05, -1.9845e-02, -1.2953e-05, -1.7875e-02, -2.5280e-04,
         -1.6835e-01, -1.9884e-01,  1.1817e-01,  1.5837e-04,  1.7900e-01,
         -1.4094e+00, -6.9595e-06, -5.2721e-06]], device='cuda:0'))])
end of epoch 8: val_loss 0.20456223000710788, val_acc 0.985
trigger times: 1
end of epoch 9: val_loss 0.005718306111334428, val_acc 0.995
trigger times: 0
saving model weights...
Weights: OrderedDict([('fc1.weight', tensor([[ 0.2796,  0.0304,  0.0910, -0.0184,  0.0695,  0.0531,  0.0599,  0.1679,
          0.2535,  0.0226,  0.0226, -0.0171,  0.0643, -0.0556, -0.0045, -0.0055,
         -0.0151,  0.1491, -0.0144, -0.0035, -0.2265,  0.0695, -0.0378,  0.0523,
          0.2540, -1.6583,  0.2973, -0.0790]], device='cuda:0'))])
end of epoch 10: val_loss 0.5703281470895479, val_acc 0.96
trigger times: 1
end of epoch 11: val_loss 0.5268530156831922, val_acc 0.965
trigger times: 2
end of epoch 12: val_loss 0.1508878208743408, val_acc 0.98
trigger times: 3
end of epoch 13: val_loss 0.3313477581532698, val_acc 0.98
trigger times: 4
end of epoch 14: val_loss 0.0734241978440636, val_acc 0.99
trigger times: 5
end of epoch 15: val_loss 0.05673829581866798, val_acc 0.995
trigger times: 6
end of epoch 16: val_loss 0.03847018277661757, val_acc 0.995
trigger times: 7
end of epoch 17: val_loss 0.03193893053421806, val_acc 0.99
trigger times: 8
end of epoch 18: val_loss 0.15649953473271364, val_acc 0.97
trigger times: 9
end of epoch 19: val_loss 0.3373888117763566, val_acc 0.96
trigger times: 10
Early stopping.
0 -84.60317093133926 -191.10004520678868
1 -82.12039098143578 -189.05007017278314
2 -97.65682357549667 -187.50220085194994
3 -73.56125596165657 -180.24009179391223
4 -71.01451388001442 -171.9613792356186
5 -44.99995324993506 -169.3817016847894
6 -49.74701488018036 -167.87631268052303
7 -91.80118262767792 -164.27567046290412
8 -101.2729811668396 -163.76742236654098
9 -42.438956532627344 -156.0786895876365
10 -71.28475999832153 -155.22804551507866
11 -57.774685367941856 -145.29002069283715
12 -54.95850846171379 -142.86657280197298
13 -9.669462162419222 -142.75441848091938
14 -26.23652647435665 -136.98962801176134
15 -59.21287474036217 -132.4813576990395
16 -7.066447339951992 -130.98347157271724
17 -52.293663293123245 -118.2436222554584
18 -7.2049660384655 -117.38411415155902
19 -66.23801735043526 -116.97382686219106
20 14.254522621631622 -115.06704102317899
21 15.72335322573781 -112.47832645551227
22 -15.040271908044815 -103.24187681049156
23 1.4900785088539124 -99.7422752992134
24 30.194350466132164 -98.18526661663928
25 37.50177022814751 -88.94674758091466
26 64.4764725714922 -85.88346861832474
27 60.229470402002335 -79.38307722218848
28 40.700745329260826 -79.25112655387144
29 10.449141576886177 -73.50697855948762
30 28.16261914372444 -72.69803191552006
31 46.80556783080101 -71.37326168319514
32 14.160422295331955 -68.27187815304366
33 42.534729301929474 -65.42298162776466
34 53.07404413819313 -65.35816021006309
35 68.1580939590931 -49.3186741604369
36 2.041180118918419 -17.325441617065966
37 28.4785538315773 -4.374511275254108
38 41.03816297650337 -0.4258658969524013
39 10.748763099312782 9.108146753357724
40 48.64532536268234 17.732630180500013
41 87.81955847144127 40.64550156746333
42 78.02833358570933 41.11774838972275
43 51.82339205779135 48.64917674484657
44 73.87396314740181 52.50260039838799
45 60.03549548983574 52.54502852695255
46 80.97411361336708 59.52827491841281
47 63.37256035208702 63.45606316769021
48 44.68518324196339 66.31570758515764
49 31.625914737582207 67.13050507695384
50 89.80233439803123 72.16624272108386
51 30.78536507487297 79.16074531670823
52 43.429889142513275 84.19837606184075
53 51.785700261592865 89.30707154325768
54 38.1846289485693 90.2698850730257
55 41.35697332024574 90.28646231697512
56 72.79720038175583 90.32667007477052
57 63.40738809108734 91.18021498086692
58 75.27548870444298 92.38187517969644
59 49.18290412425995 92.67153902025292
60 39.188165962696075 93.69474938898811
61 85.86123138666153 93.86112859581364
62 30.875807777047157 95.24073532592021
63 69.38578322529793 95.3630101766082
64 70.79708287119865 96.09581961281374
65 46.30697502195835 97.90553231413178
66 62.83524549007416 98.07644230926206
67 46.2918641269207 98.35872828331676
68 51.965361922979355 98.57334710000995
69 61.23014286160469 98.77895390936838
70 51.16396144032478 99.10163703819775
71 63.11259979009628 99.47183807834362
72 50.43606352806091 101.36707045382526
73 59.85710600018501 102.70820969163083
74 44.53050109744072 103.61971441966246
75 66.28405126929283 104.00039883786899
76 78.53306278586388 104.94014608739957
77 58.82535484433174 105.28448326377394
78 73.88949790596962 105.60678192821229
79 57.57749870419502 106.95774051225432
80 37.81071013212204 107.03525723063939
81 59.56776338815689 107.05410485829283
82 65.69861449301243 107.36477417456501
83 44.00437940657139 107.72369032284489
84 54.799069195985794 107.78006757763964
85 75.9224044084549 107.99397435655325
86 89.9011697769165 108.08910889221833
87 76.72186243534088 108.09776529440465
88 66.04985220730305 108.24033506535473
89 68.1977097094059 109.15712102315346
90 73.29847773909569 109.71287331230647
91 85.64344692230225 110.25063890779474
92 83.92827233672142 110.45880856467717
93 53.07394623756409 110.48639228144049
94 77.84871968626976 111.92877271779909
95 48.43116581439972 112.29467419620224
96 88.02012926340103 112.73910566375015
97 77.66171792149544 112.98274531176438
98 66.65763112902641 113.84616479719523
99 56.42852255702019 114.84612239587612
100 50.48985640704632 115.08707834021696
101 84.70476357638836 115.41025719446112
102 104.02274063229561 115.5646624637931
103 81.62221857905388 115.87882502857741
104 87.35121610760689 116.51927592390606
105 76.8245250582695 116.69939038222442
106 68.26528024673462 117.18809854794453
107 94.35037541389465 117.29357601720739
108 85.36988970637321 117.43241923864416
109 74.85404515266418 119.84073495876005
110 68.42926412820816 122.74776106687673
111 82.28661167621613 124.10172315382671
112 104.45903703570366 124.42382576156477
113 100.47480255365372 124.48179570258212
114 85.7534009218216 125.2590893824027
115 91.36680805683136 125.30457667780085
116 88.39479169249535 125.43458549438884
117 106.65606534481049 129.13346806288703
118 105.59864521026611 129.67718524437547
119 122.53446885943413 131.40806576766226
train accuracy: 0.9866666666666667
validation accuracy: 0.96
