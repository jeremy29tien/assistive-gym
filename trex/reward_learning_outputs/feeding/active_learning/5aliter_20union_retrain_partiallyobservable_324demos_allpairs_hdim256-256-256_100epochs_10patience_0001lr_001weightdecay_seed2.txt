demos: (480, 200, 32)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=32, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 140288
Number of trainable paramters: 140288
device: cuda:2
end of epoch 0: val_loss 0.30801757249548073, val_acc 0.8546099290780141
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.302758742087317, val_acc 0.8643617021276596
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.2770277760028206, val_acc 0.87677304964539
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.36648357873277676, val_acc 0.8554964539007093
trigger times: 1
end of epoch 4: val_loss 0.24993565998915668, val_acc 0.8794326241134752
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.35948374280348827, val_acc 0.851063829787234
trigger times: 1
end of epoch 6: val_loss 0.4189118578083261, val_acc 0.7987588652482269
trigger times: 2
end of epoch 7: val_loss 0.2971750715062945, val_acc 0.8634751773049646
trigger times: 3
end of epoch 8: val_loss 0.33590143646686416, val_acc 0.850177304964539
trigger times: 4
end of epoch 9: val_loss 0.2664230491851162, val_acc 0.87322695035461
trigger times: 5
end of epoch 10: val_loss 0.2947920846119069, val_acc 0.8625886524822695
trigger times: 6
end of epoch 11: val_loss 0.3746478674719106, val_acc 0.8297872340425532
trigger times: 7
end of epoch 12: val_loss 0.2736840623404945, val_acc 0.8714539007092199
trigger times: 8
end of epoch 13: val_loss 0.25775732867967727, val_acc 0.8776595744680851
trigger times: 9
end of epoch 14: val_loss 0.2737506385733497, val_acc 0.8696808510638298
trigger times: 10
Early stopping.
0 7.237860171124339 -202.19173244106338
1 8.942376039922237 -189.65792573794374
2 7.535268702544272 -162.12021763693463
3 11.211091173812747 -146.95623432460457
4 13.977679140865803 -146.8781492057455
5 14.731427930295467 -130.0623604093787
6 13.17824680916965 -120.40988890935725
7 15.343926191329956 -117.56064723209383
8 14.236445274204016 -104.4810453495825
9 14.990501251071692 -97.38983155484487
10 16.711300000548363 -87.12595597627949
11 20.842376697808504 -63.52410268673754
12 19.86586445197463 -40.18888097237036
13 18.941992357373238 -25.690221770027268
14 18.041194645687938 -20.635708086354146
15 20.08069183304906 5.848376712772666
16 18.471904447302222 11.612289242987064
17 20.852148122045037 31.292646621686625
18 21.18430840037763 33.975348715294075
19 18.255388014018536 52.74553531088168
20 20.634465603157878 54.089525054137525
21 19.189188066869974 66.71193243213483
22 20.063875317573547 81.09359582871949
23 20.016459537670016 85.74426695968816
24 18.17410182952881 86.29909444812428
25 19.66864949092269 86.34494544170542
26 17.739554531872272 91.60759786486705
27 19.530442597344518 93.6321003319453
28 23.534868222628575 96.79908925678686
29 20.931038070470095 97.31123430564293
30 19.75863154232502 103.69301865868974
31 21.62364112958312 103.71935195099562
32 21.47060152515769 106.98604693228711
33 20.16554857417941 107.41417015930337
34 22.282186523079872 108.19351028466765
35 20.949108444154263 109.78275288448633
36 21.7693129805848 113.6146907736559
37 20.930160446092486 116.20758485182266
38 21.978168906643987 116.28813151931428
39 22.248372461646795 116.79577177737987
40 24.19074922800064 118.46731683423405
41 23.144434925168753 119.12944013342411
42 24.14921285584569 119.16687100033135
43 22.970745865255594 120.93165361575389
44 23.759687818586826 121.56783565336603
45 22.661307372152805 124.5606016172533
46 25.76988598331809 128.67397189759063
47 25.521079521626234 131.499457484949
train accuracy: 0.8724343538585024
validation accuracy: 0.8696808510638298
demos: (500, 200, 32)
demo_rewards: (500,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1225
num val_labels 1225
ModuleList(
  (0): Linear(in_features=32, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 140288
Number of trainable paramters: 140288
device: cuda:0
end of epoch 0: val_loss 0.3483514585476406, val_acc 0.8514285714285714
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.27676100599513054, val_acc 0.8848979591836734
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.30645400902464226, val_acc 0.8710204081632653
trigger times: 1
end of epoch 3: val_loss 0.3301090742734153, val_acc 0.8555102040816327
trigger times: 2
end of epoch 4: val_loss 0.2674452821778551, val_acc 0.8718367346938776
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.3016885133314812, val_acc 0.8726530612244898
trigger times: 1
end of epoch 6: val_loss 0.3072850911591235, val_acc 0.866938775510204
trigger times: 2
end of epoch 7: val_loss 0.27132050324432877, val_acc 0.8751020408163265
trigger times: 3
end of epoch 8: val_loss 0.2546710044131342, val_acc 0.8840816326530613
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.23817838669038965, val_acc 0.8963265306122449
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.2861860960814817, val_acc 0.8742857142857143
trigger times: 1
end of epoch 11: val_loss 0.26967146815112153, val_acc 0.8759183673469387
trigger times: 2
end of epoch 12: val_loss 0.28046333884606506, val_acc 0.893061224489796
trigger times: 3
end of epoch 13: val_loss 0.2648706704706142, val_acc 0.8865306122448979
trigger times: 4
end of epoch 14: val_loss 0.2491548518776013, val_acc 0.8889795918367347
trigger times: 5
end of epoch 15: val_loss 0.3098172132617791, val_acc 0.8865306122448979
trigger times: 6
end of epoch 16: val_loss 0.30240849732950126, val_acc 0.8587755102040816
trigger times: 7
end of epoch 17: val_loss 0.2513522207146918, val_acc 0.8857142857142857
trigger times: 8
end of epoch 18: val_loss 0.2883579731705718, val_acc 0.876734693877551
trigger times: 9
end of epoch 19: val_loss 0.3023953705678208, val_acc 0.8636734693877551
trigger times: 10
Early stopping.
0 -18.47761639393866 -180.94031503146633
1 -19.26334780175239 -178.4936800344805
2 -18.429428851231933 -171.07185874229646
3 -19.925695431418717 -157.7287316199696
4 -14.251139107509516 -153.84889359885997
5 -11.108880825689994 -146.8781492057455
6 -12.813086069014389 -139.8598026529788
7 -13.274119846755639 -136.6102332063945
8 -16.450540996156633 -132.32422579815238
9 -10.068367455583939 -116.02779530304753
10 -11.302215752191842 -107.36911392314994
11 -15.630683475057594 -106.61264278156179
12 -8.048086136463098 -88.94899174825389
13 -9.945477832341567 -77.66430866119553
14 -9.948468831629725 -67.52476265318596
15 -9.629829339988646 -67.25891012483117
16 -5.702213894128363 -64.73742614056225
17 -8.548440763624967 -52.74702703769523
18 -9.388350642984733 -41.854187767463074
19 -7.4237420819627005 36.38260726753759
20 -8.311406578119204 38.01283008215399
21 -5.258862815913744 43.13799553400644
22 -6.344964823802002 81.81757056315458
23 -7.695723971701227 82.33704164389918
24 -7.576858019296196 88.41217626317327
25 -7.0113205262459815 93.6321003319453
26 -4.682926177076297 94.1092447438114
27 -4.041635970119387 96.79908925678686
28 -5.793745988172304 98.7426598371722
29 -7.430689155939035 100.54764565324264
30 -7.0513544717105106 102.74578030549429
31 -6.559656517260009 103.71935195099562
32 -4.7046285289980005 106.10300338049267
33 -7.122511945373844 106.98604693228711
34 -5.8793650167644955 111.20504336739191
35 -4.555411945373635 111.3085591795261
36 -6.509136726337601 111.50257243127638
37 -4.701685581327183 111.83190145510993
38 -5.675911718841235 113.49209291788252
39 -5.3858522984955925 114.66425172742227
40 -5.39130902083707 116.28813151931428
41 -5.753668150340673 116.81427077665914
42 -5.014304595875728 116.96227305693691
43 -6.7978380573913455 117.4266484465981
44 -3.638827716913511 118.46731683423405
45 -4.2992970346604125 119.12944013342411
46 -4.391432449527201 122.13514936637644
47 -5.348806498423073 124.5606016172533
48 -3.3437516005769794 125.86136637734936
49 -2.7461912697181106 131.499457484949
train accuracy: 0.8933608531131751
validation accuracy: 0.8636734693877551
demos: (520, 200, 32)
demo_rewards: (520,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1326
num val_labels 1326
ModuleList(
  (0): Linear(in_features=32, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 140288
Number of trainable paramters: 140288
device: cuda:1
end of epoch 0: val_loss 0.26319647605881746, val_acc 0.8778280542986425
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.25124900760936764, val_acc 0.8665158371040724
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.3967921149368471, val_acc 0.8453996983408748
trigger times: 1
end of epoch 3: val_loss 0.307711602331472, val_acc 0.8536953242835595
trigger times: 2
end of epoch 4: val_loss 0.4094988564635826, val_acc 0.8340874811463047
trigger times: 3
end of epoch 5: val_loss 0.32516314259810125, val_acc 0.8634992458521871
trigger times: 4
end of epoch 6: val_loss 0.3798747778121382, val_acc 0.8401206636500754
trigger times: 5
end of epoch 7: val_loss 0.2679182888327344, val_acc 0.8687782805429864
trigger times: 6
end of epoch 8: val_loss 0.3453605503146294, val_acc 0.8484162895927602
trigger times: 7
end of epoch 9: val_loss 0.3020536729339965, val_acc 0.8567119155354449
trigger times: 8
end of epoch 10: val_loss 0.3047378297158944, val_acc 0.8567119155354449
trigger times: 9
end of epoch 11: val_loss 0.37239067278750165, val_acc 0.8491704374057315
trigger times: 10
Early stopping.
0 1.3050198003766127 -187.3810298801045
1 -1.0415537645458244 -177.19608324005077
2 0.2677827895095106 -162.12021763693463
3 2.67129674134776 -158.98604846130755
4 3.7616671426803805 -157.58076401159747
5 3.6743987992376788 -156.8354163242861
6 5.50544881640235 -153.4456477258534
7 0.3298437378834933 -138.23300018141344
8 4.969362729869317 -136.9016903684411
9 4.4804150686832145 -130.0623604093787
10 8.48999225674197 -125.01699743043554
11 10.174171922844835 -117.56064723209383
12 8.157362986938097 -106.61264278156179
13 9.931087733712047 -106.34887562579361
14 6.361531798436772 -85.94033419329429
15 11.01897449279204 -85.6843768163284
16 12.135438882745802 -83.54839050364284
17 12.169789037201554 -81.40156491068564
18 9.585159451235086 -67.25891012483117
19 16.66556594101712 -64.73742614056225
20 12.427105887793005 -64.59517229125954
21 15.15285906381905 -40.18888097237036
22 12.201752403110731 -23.754075772294165
23 13.252188793383539 5.848376712772666
24 13.880937386653386 33.975348715294075
25 15.351198359392583 42.30492625590143
26 14.821107681840658 52.74553531088168
27 14.324192192871124 58.760634987774274
28 15.691214606631547 60.4861487219586
29 15.077288289554417 82.33704164389918
30 12.511041885241866 84.68442046815105
31 12.451861917972565 87.7203754226118
32 13.229388375766575 88.41217626317327
33 11.991124234860763 89.11303835091482
34 13.778359753079712 89.94641990631162
35 13.9387913569808 93.6321003319453
36 19.32501421496272 96.79908925678686
37 14.186198913492262 100.17952611313399
38 13.5268889121362 102.74578030549429
39 12.965823822189122 103.71935195099562
40 13.511201847344637 104.69925557203021
41 15.849931105971336 106.99096041080111
42 15.835547671187669 111.3085591795261
43 15.886003581807017 111.83190145510993
44 17.719106389209628 113.00340800823342
45 18.82140981964767 118.46731683423405
46 15.548286200501025 124.5606016172533
47 20.351450111716986 125.86136637734936
48 12.502919799648225 126.59732575889738
49 19.890785929746926 128.67397189759063
50 22.603639151901007 129.08221841696613
51 18.757748301140964 131.499457484949
train accuracy: 0.8909337614187975
validation accuracy: 0.8491704374057315
demos: (540, 200, 32)
demo_rewards: (540,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1431
num val_labels 1431
ModuleList(
  (0): Linear(in_features=32, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 140288
Number of trainable paramters: 140288
device: cuda:3
end of epoch 0: val_loss 0.2438767213371842, val_acc 0.8902865129280224
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.25019755384961134, val_acc 0.8798043326345213
trigger times: 1
end of epoch 2: val_loss 0.25881928815352273, val_acc 0.8791055206149546
trigger times: 2
end of epoch 3: val_loss 0.2895148738993764, val_acc 0.8574423480083857
trigger times: 3
end of epoch 4: val_loss 0.263094068113218, val_acc 0.8777078965758212
trigger times: 4
end of epoch 5: val_loss 0.26200867112299614, val_acc 0.8805031446540881
trigger times: 5
end of epoch 6: val_loss 0.30945023887460443, val_acc 0.86722571628232
trigger times: 6
end of epoch 7: val_loss 0.2702319602953975, val_acc 0.8679245283018868
trigger times: 7
end of epoch 8: val_loss 0.21322069156309462, val_acc 0.9035639412997903
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.2544243237663337, val_acc 0.8798043326345213
trigger times: 1
end of epoch 10: val_loss 0.2231840275767677, val_acc 0.9021663172606569
trigger times: 2
end of epoch 11: val_loss 0.22426762537488126, val_acc 0.9000698812019566
trigger times: 3
end of epoch 12: val_loss 0.2862277834050006, val_acc 0.8805031446540881
trigger times: 4
end of epoch 13: val_loss 0.23545795540005168, val_acc 0.8874912648497554
trigger times: 5
end of epoch 14: val_loss 0.24583149021206654, val_acc 0.8881900768693222
trigger times: 6
end of epoch 15: val_loss 0.2945595735440404, val_acc 0.8665269042627534
trigger times: 7
end of epoch 16: val_loss 0.21958226926099667, val_acc 0.9007686932215234
trigger times: 8
end of epoch 17: val_loss 0.20982119908398766, val_acc 0.9000698812019566
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.21314033989492245, val_acc 0.9084556254367575
trigger times: 1
end of epoch 19: val_loss 0.24183765641789598, val_acc 0.8812019566736548
trigger times: 2
end of epoch 20: val_loss 0.2723021095312337, val_acc 0.8805031446540881
trigger times: 3
end of epoch 21: val_loss 0.2725831795341256, val_acc 0.8812019566736548
trigger times: 4
end of epoch 22: val_loss 0.27110813890657454, val_acc 0.8637316561844863
trigger times: 5
end of epoch 23: val_loss 0.23254550241662292, val_acc 0.9007686932215234
trigger times: 6
end of epoch 24: val_loss 0.22113038534591134, val_acc 0.9056603773584906
trigger times: 7
end of epoch 25: val_loss 0.2415686412026787, val_acc 0.8916841369671559
trigger times: 8
end of epoch 26: val_loss 0.2467541291531251, val_acc 0.8839972047519218
trigger times: 9
end of epoch 27: val_loss 0.22972009275821556, val_acc 0.893780573025856
trigger times: 10
Early stopping.
0 -13.4185451855883 -202.19173244106338
1 -11.311877351254225 -190.21899363900812
2 -10.082112733449321 -176.02327356618886
3 -11.185272701084614 -157.8694620479787
4 -9.36952217228827 -156.8354163242861
5 -8.458290399983525 -156.23561904706384
6 -9.068780393339694 -145.5482550590335
7 -7.234451800073657 -132.4999980274812
8 -8.785721319261938 -130.58063820079244
9 -8.880443225847557 -124.10234840606287
10 -5.55608619582199 -117.56064723209383
11 -5.059882909117732 -106.61264278156179
12 -6.78452142799506 -99.81190786688705
13 -5.511027155298507 -98.78979845084291
14 -6.775014169623319 -97.75429321743087
15 -3.802354963423568 -76.64970632397707
16 -2.875577282975428 -75.04365672708533
17 -2.958679433355428 -64.65679738462214
18 -3.512537126924144 -64.59517229125954
19 -3.488260037440341 -52.74702703769523
20 -2.646944983775029 -16.36038367979373
21 -3.055044025497409 7.066338713494528
22 -2.980016397021245 11.612289242987064
23 -2.042837998640607 33.975348715294075
24 -5.186861939379014 40.428355258377486
25 -1.7064068911713548 51.192000963985954
26 -3.5111734184611123 52.74553531088168
27 -2.9888609215122415 61.164105726422065
28 -2.647417713102186 80.48392305495419
29 -3.0492439628578722 81.09359582871949
30 -0.983027563750511 89.94641990631162
31 -0.8685903665027581 96.79908925678686
32 -2.2870111123193055 96.81072352649034
33 -3.5253363617230207 97.62626806409295
34 -2.3679237003671005 100.17952611313399
35 -1.7557968666078523 102.92516076830748
36 -1.993490886350628 103.71083627769976
37 -1.3276631914231984 103.71935195099562
38 -2.6595748387044296 104.06643808082026
39 -2.595336119116837 108.98656220301545
40 -0.024443032481940463 111.3085591795261
41 0.09595203085336834 111.83190145510993
42 -1.3060897120885784 112.45842855426564
43 -0.9210966695682146 113.49209291788252
44 -0.7183802741055842 113.6146907736559
45 -1.2798794083064422 115.3046570260328
46 -0.34922137105604634 116.20758485182266
47 -0.27222115147742443 117.27388896583807
48 -1.7881064568064176 117.4266484465981
49 0.7848385582910851 121.56783565336603
50 0.9605677277722862 125.86136637734936
51 -0.4331545567838475 126.47410369017034
52 2.9561758889758494 128.67397189759063
53 2.829355748850503 129.08221841696613
train accuracy: 0.8827924932156098
validation accuracy: 0.893780573025856
demos: (560, 200, 32)
demo_rewards: (560,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1540
num val_labels 1540
ModuleList(
  (0): Linear(in_features=32, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 140288
Number of trainable paramters: 140288
device: cuda:1
end of epoch 0: val_loss 0.31203813424233057, val_acc 0.8506493506493507
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.3293980195140888, val_acc 0.8551948051948052
trigger times: 1
end of epoch 2: val_loss 0.32107106526441587, val_acc 0.8532467532467533
trigger times: 2
end of epoch 3: val_loss 0.28392393687366235, val_acc 0.8701298701298701
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.29814427754379946, val_acc 0.8577922077922078
trigger times: 1
end of epoch 5: val_loss 0.28143695418456566, val_acc 0.8688311688311688
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.290996019811457, val_acc 0.862987012987013
trigger times: 1
end of epoch 7: val_loss 0.33442106484842593, val_acc 0.8435064935064935
trigger times: 2
end of epoch 8: val_loss 0.29993814590151646, val_acc 0.8584415584415584
trigger times: 3
end of epoch 9: val_loss 0.3004217793497731, val_acc 0.85
trigger times: 4
end of epoch 10: val_loss 0.3683557987771272, val_acc 0.8357142857142857
trigger times: 5
end of epoch 11: val_loss 0.3027247358137679, val_acc 0.8571428571428571
trigger times: 6
end of epoch 12: val_loss 0.2893075731730912, val_acc 0.8636363636363636
trigger times: 7
end of epoch 13: val_loss 0.2923122111655944, val_acc 0.8584415584415584
trigger times: 8
end of epoch 14: val_loss 0.3368656672339579, val_acc 0.8512987012987013
trigger times: 9
end of epoch 15: val_loss 0.2722292466808452, val_acc 0.8792207792207792
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.3359073901613593, val_acc 0.8474025974025974
trigger times: 1
end of epoch 17: val_loss 0.3138642466763212, val_acc 0.8525974025974026
trigger times: 2
end of epoch 18: val_loss 0.38288755493575677, val_acc 0.8194805194805195
trigger times: 3
end of epoch 19: val_loss 0.40355750865930984, val_acc 0.825974025974026
trigger times: 4
end of epoch 20: val_loss 0.37801103425546145, val_acc 0.8012987012987013
trigger times: 5
end of epoch 21: val_loss 0.2961725635661645, val_acc 0.8616883116883117
trigger times: 6
end of epoch 22: val_loss 0.2922075636823661, val_acc 0.8577922077922078
trigger times: 7
end of epoch 23: val_loss 0.2836072325738928, val_acc 0.8649350649350649
trigger times: 8
end of epoch 24: val_loss 0.3012748311078951, val_acc 0.8519480519480519
trigger times: 9
end of epoch 25: val_loss 0.2934410301278065, val_acc 0.8564935064935065
trigger times: 10
Early stopping.
0 -12.013262705033412 -209.0523138369167
1 -14.94756534404587 -177.19608324005077
2 -4.645898842281895 -171.05558932195746
3 -7.954417001325055 -156.23561904706384
4 -2.054252380505204 -146.8781492057455
5 -6.1573257389245555 -145.5482550590335
6 -2.8857109536911594 -139.8598026529788
7 -1.8592079402515083 -134.93384821211495
8 -2.99494621809572 -121.48582294966032
9 3.49923061314621 -116.02779530304753
10 2.2952693546612863 -100.96710329978177
11 4.017175220884383 -91.36770482027025
12 2.2249404503090773 -88.94899174825389
13 1.40580224187579 -84.95908697742144
14 0.738485712054171 -82.26384066215606
15 2.692070356923068 -73.13396672906846
16 3.9771210856270045 -68.53866917129217
17 1.966663547369535 -66.30313186812843
18 2.230512821107368 -63.684489628253
19 4.596674296145466 -58.66155314643179
20 3.7352469660690986 -19.260698919289915
21 4.560483729335829 15.578356723059361
22 4.079744224611204 54.089525054137525
23 5.096799305872992 60.4861487219586
24 4.375450343010016 81.81757056315458
25 3.4724837479207054 82.33704164389918
26 3.173332064274291 85.74426695968816
27 4.348346132363076 87.51469019637601
28 1.9243465357576497 87.7203754226118
29 5.013943247264251 89.94641990631162
30 2.508094693082967 91.60759786486705
31 4.795316768111661 100.17952611313399
32 3.5602691922395024 102.74578030549429
33 3.7262333929538727 102.92516076830748
34 6.273028284078464 109.2313240297383
35 5.136061927070841 109.78275288448633
36 6.533636601467151 110.50643695333387
37 7.097788136219606 111.83190145510993
38 3.428561474822345 111.96192143127328
39 6.575288653781172 112.1274684873837
40 5.984494316158816 113.6146907736559
41 4.739641527772619 115.3046570260328
42 5.354807414114475 116.28813151931428
43 4.746989697800018 116.81427077665914
44 6.329421496833675 116.96227305693691
45 6.025688264351629 117.71453405712296
46 4.281461686186958 118.15225085070401
47 6.9595749489963055 119.9944732564883
48 6.897153087425977 120.93165361575389
49 7.206806660658913 125.86136637734936
50 6.6305958984885365 126.39803550662174
51 5.750574163801502 126.47410369017034
52 8.526486392394872 128.67397189759063
53 8.738370904204203 129.08221841696613
54 7.782423299504444 130.45510520660062
55 7.585306385764852 131.499457484949
train accuracy: 0.8972403776325345
validation accuracy: 0.8564935064935065
