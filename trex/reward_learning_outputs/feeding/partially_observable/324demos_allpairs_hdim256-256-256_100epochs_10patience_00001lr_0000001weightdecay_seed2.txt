demos: (480, 200, 32)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=32, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 140288
Number of trainable paramters: 140288
device: cuda:0
end of epoch 0: val_loss 0.4480365667833073, val_acc 0.8608156028368794
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.5929861669605042, val_acc 0.8723404255319149
trigger times: 1
end of epoch 2: val_loss 0.6338492856629051, val_acc 0.8670212765957447
trigger times: 2
end of epoch 3: val_loss 0.8301103082481462, val_acc 0.8687943262411347
trigger times: 3
end of epoch 4: val_loss 0.7452122045137185, val_acc 0.8776595744680851
trigger times: 4
end of epoch 5: val_loss 1.0532870169377053, val_acc 0.8563829787234043
trigger times: 5
end of epoch 6: val_loss 0.9989786374784773, val_acc 0.8661347517730497
trigger times: 6
end of epoch 7: val_loss 1.0557861419166228, val_acc 0.8670212765957447
trigger times: 7
end of epoch 8: val_loss 1.023795423103608, val_acc 0.8617021276595744
trigger times: 8
end of epoch 9: val_loss 0.9641922330430772, val_acc 0.8687943262411347
trigger times: 9
end of epoch 10: val_loss 0.9558759424746205, val_acc 0.8661347517730497
trigger times: 10
Early stopping.
0 -71.07162518426776 -202.19173244106338
1 -47.10655493661761 -189.65792573794374
2 -56.05379030946642 -162.12021763693463
3 -38.030731490813196 -146.95623432460457
4 -27.91107986262068 -146.8781492057455
5 -34.936372739844956 -130.0623604093787
6 -29.2983746859245 -120.40988890935725
7 -25.38692183967214 -117.56064723209383
8 -22.57216346124187 -104.4810453495825
9 -19.941985479556024 -97.38983155484487
10 -22.025963021442294 -87.12595597627949
11 6.178436080808751 -63.52410268673754
12 6.410605115815997 -40.18888097237036
13 -8.127924774773419 -25.690221770027268
14 -9.289149966440164 -20.635708086354146
15 -1.4427085826173425 5.848376712772666
16 -5.848249244038016 11.612289242987064
17 -1.5723986462689936 31.292646621686625
18 6.444712360505946 33.975348715294075
19 -1.310226909816265 52.74553531088168
20 4.479258679435588 54.089525054137525
21 -11.600574389798567 66.71193243213483
22 -6.361796890385449 81.09359582871949
23 1.535245479317382 85.74426695968816
24 -5.917121771955863 86.29909444812428
25 4.897158795967698 86.34494544170542
26 -6.872727607144043 91.60759786486705
27 -2.7344792814692482 93.6321003319453
28 23.319018425419927 96.79908925678686
29 6.338104758877307 97.31123430564293
30 -1.7191482759662904 103.69301865868974
31 3.8478544674580917 103.71935195099562
32 7.481415314599872 106.98604693228711
33 -3.879205638775602 107.41417015930337
34 7.67256542481482 108.19351028466765
35 10.416825552470982 109.78275288448633
36 10.328958094585687 113.6146907736559
37 4.792873133905232 116.20758485182266
38 13.298410290852189 116.28813151931428
39 11.447227814933285 116.79577177737987
40 23.39208489295561 118.46731683423405
41 16.52122592739761 119.12944013342411
42 21.02412467612885 119.16687100033135
43 14.878281691111624 120.93165361575389
44 18.051536625251174 121.56783565336603
45 11.760509924788494 124.5606016172533
46 32.15353883756325 128.67397189759063
47 28.238360846065916 131.499457484949
train accuracy: 0.9794939418262432
validation accuracy: 0.8661347517730497
