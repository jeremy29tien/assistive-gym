demos: (480, 200, 32)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=32, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 140288
Number of trainable paramters: 140288
device: cuda:2
end of epoch 0: val_loss 0.30801757249548073, val_acc 0.8546099290780141
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.302758742087317, val_acc 0.8643617021276596
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.2770277760028206, val_acc 0.87677304964539
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.36648357873277676, val_acc 0.8554964539007093
trigger times: 1
end of epoch 4: val_loss 0.24993565998915668, val_acc 0.8794326241134752
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.35948374280348827, val_acc 0.851063829787234
trigger times: 1
end of epoch 6: val_loss 0.4189118578083261, val_acc 0.7987588652482269
trigger times: 2
end of epoch 7: val_loss 0.2971750715062945, val_acc 0.8634751773049646
trigger times: 3
end of epoch 8: val_loss 0.33590143646686416, val_acc 0.850177304964539
trigger times: 4
end of epoch 9: val_loss 0.2664230491851162, val_acc 0.87322695035461
trigger times: 5
end of epoch 10: val_loss 0.2947920846119069, val_acc 0.8625886524822695
trigger times: 6
end of epoch 11: val_loss 0.3746478674719106, val_acc 0.8297872340425532
trigger times: 7
end of epoch 12: val_loss 0.2736840623404945, val_acc 0.8714539007092199
trigger times: 8
end of epoch 13: val_loss 0.25775732867967727, val_acc 0.8776595744680851
trigger times: 9
end of epoch 14: val_loss 0.2737506385733497, val_acc 0.8696808510638298
trigger times: 10
Early stopping.
0 7.237860171124339 -202.19173244106338
1 8.942376039922237 -189.65792573794374
2 7.535268702544272 -162.12021763693463
3 11.211091173812747 -146.95623432460457
4 13.977679140865803 -146.8781492057455
5 14.731427930295467 -130.0623604093787
6 13.17824680916965 -120.40988890935725
7 15.343926191329956 -117.56064723209383
8 14.236445274204016 -104.4810453495825
9 14.990501251071692 -97.38983155484487
10 16.711300000548363 -87.12595597627949
11 20.842376697808504 -63.52410268673754
12 19.86586445197463 -40.18888097237036
13 18.941992357373238 -25.690221770027268
14 18.041194645687938 -20.635708086354146
15 20.08069183304906 5.848376712772666
16 18.471904447302222 11.612289242987064
17 20.852148122045037 31.292646621686625
18 21.18430840037763 33.975348715294075
19 18.255388014018536 52.74553531088168
20 20.634465603157878 54.089525054137525
21 19.189188066869974 66.71193243213483
22 20.063875317573547 81.09359582871949
23 20.016459537670016 85.74426695968816
24 18.17410182952881 86.29909444812428
25 19.66864949092269 86.34494544170542
26 17.739554531872272 91.60759786486705
27 19.530442597344518 93.6321003319453
28 23.534868222628575 96.79908925678686
29 20.931038070470095 97.31123430564293
30 19.75863154232502 103.69301865868974
31 21.62364112958312 103.71935195099562
32 21.47060152515769 106.98604693228711
33 20.16554857417941 107.41417015930337
34 22.282186523079872 108.19351028466765
35 20.949108444154263 109.78275288448633
36 21.7693129805848 113.6146907736559
37 20.930160446092486 116.20758485182266
38 21.978168906643987 116.28813151931428
39 22.248372461646795 116.79577177737987
40 24.19074922800064 118.46731683423405
41 23.144434925168753 119.12944013342411
42 24.14921285584569 119.16687100033135
43 22.970745865255594 120.93165361575389
44 23.759687818586826 121.56783565336603
45 22.661307372152805 124.5606016172533
46 25.76988598331809 128.67397189759063
47 25.521079521626234 131.499457484949
train accuracy: 0.8724343538585024
validation accuracy: 0.8696808510638298
