demos: (480, 200, 32)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=32, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 140288
Number of trainable paramters: 140288
device: cuda:2
end of epoch 0: val_loss 0.3059464836004421, val_acc 0.875
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.30689673618840413, val_acc 0.8643617021276596
trigger times: 1
end of epoch 2: val_loss 0.3137384112260754, val_acc 0.8652482269503546
trigger times: 2
end of epoch 3: val_loss 0.28772594119555667, val_acc 0.8608156028368794
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.30462423276768563, val_acc 0.8563829787234043
trigger times: 1
end of epoch 5: val_loss 1.21304309679955, val_acc 0.75177304964539
trigger times: 2
end of epoch 6: val_loss 0.3647826347961045, val_acc 0.8342198581560284
trigger times: 3
end of epoch 7: val_loss 0.30596784611996375, val_acc 0.8537234042553191
trigger times: 4
end of epoch 8: val_loss 0.40309540501038416, val_acc 0.8554964539007093
trigger times: 5
end of epoch 9: val_loss 0.3007939812200813, val_acc 0.8554964539007093
trigger times: 6
end of epoch 10: val_loss 0.35217394438830524, val_acc 0.8386524822695035
trigger times: 7
end of epoch 11: val_loss 0.42461015032514027, val_acc 0.8067375886524822
trigger times: 8
end of epoch 12: val_loss 0.5397332780532894, val_acc 0.775709219858156
trigger times: 9
end of epoch 13: val_loss 0.379571215419863, val_acc 0.8315602836879432
trigger times: 10
Early stopping.
0 -28.547567486763 -176.02327356618886
1 -26.921330139040947 -171.73538080057904
2 -32.15139275044203 -162.12021763693463
3 -22.332151506096125 -151.03182509904744
4 -23.873572077602148 -144.9053011827279
5 -21.064198788255453 -135.2361570794697
6 -19.775696251541376 -130.60907588284448
7 -18.144947255030274 -123.08440479922548
8 -19.951297838240862 -109.26967476004833
9 -23.05155099928379 -97.75429321743087
10 -16.634164134040475 -93.63303104415972
11 -14.686295121908188 -71.68281387511338
12 -14.719672294333577 -71.51650811905506
13 -17.15147888287902 -67.25891012483117
14 -15.051389761269093 -64.59517229125954
15 -20.33181127719581 -29.04983757741715
16 -14.673190599307418 -26.554830714509592
17 -17.273117134347558 7.066338713494528
18 -14.629027880728245 9.9385357692998
19 -14.399144358932972 21.358512279863625
20 -13.680811379104853 36.38260726753759
21 -13.230941228568554 51.192000963985954
22 -14.151667321100831 58.67680633739908
23 -13.134711598977447 74.36839302338853
24 -15.524598801508546 79.93047048250351
25 -15.10726236179471 89.19536109182386
26 -18.39582630060613 91.60759786486705
27 -14.707856681197882 93.17220181871483
28 -14.487747255712748 94.22803327213148
29 -11.912609184160829 96.79908925678686
30 -13.56284400075674 97.31123430564293
31 -14.270975248888135 101.37202708023295
32 -14.611350797116756 101.55104413527681
33 -14.54565929248929 102.74578030549429
34 -14.841561015695333 106.02351298384437
35 -12.984451020136476 106.45096995781903
36 -12.528309840708971 108.06711384653553
37 -13.858960043638945 109.02633758684364
38 -14.378086183220148 109.51702889237426
39 -15.267356110736728 110.37533862221531
40 -12.719848094508052 110.43094239093074
41 -12.197436705231667 110.59269304108648
42 -13.359527895227075 111.26442060476346
43 -14.328550189733505 111.55240978447834
44 -11.791082818061113 120.93165361575389
45 -11.535138674080372 125.17634239385316
46 -7.3779662027955055 129.08221841696613
47 -9.336100777611136 130.45510520660062
train accuracy: 0.8878951190612697
validation accuracy: 0.8315602836879432
