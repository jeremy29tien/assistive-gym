demos: (480, 200, 32)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=32, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 140288
Number of trainable paramters: 140288
device: cuda:0
end of epoch 0: val_loss 0.2759632523781597, val_acc 0.8847517730496454
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.31901932070556616, val_acc 0.87677304964539
trigger times: 1
end of epoch 2: val_loss 0.27947607040141575, val_acc 0.8714539007092199
trigger times: 2
end of epoch 3: val_loss 0.29473804930695946, val_acc 0.8617021276595744
trigger times: 3
end of epoch 4: val_loss 0.29291005619868893, val_acc 0.8714539007092199
trigger times: 4
end of epoch 5: val_loss 0.31475317232036204, val_acc 0.8714539007092199
trigger times: 5
end of epoch 6: val_loss 0.2797584512520529, val_acc 0.8776595744680851
trigger times: 6
end of epoch 7: val_loss 0.3167300065380583, val_acc 0.8670212765957447
trigger times: 7
end of epoch 8: val_loss 0.28448729153824476, val_acc 0.8794326241134752
trigger times: 8
end of epoch 9: val_loss 0.35214868637347685, val_acc 0.8643617021276596
trigger times: 9
end of epoch 10: val_loss 0.3422732553673607, val_acc 0.8625886524822695
trigger times: 10
Early stopping.
0 6.061272559338249 -193.77058932274844
1 4.591108016349608 -189.65792573794374
2 5.254868319010711 -181.53450727998975
3 5.491118577250745 -180.94031503146633
4 4.897135654231533 -171.07185874229646
5 12.259333804948255 -146.8781492057455
6 11.30643838213291 -135.2361570794697
7 7.787593552609906 -132.32422579815238
8 10.809106112457812 -127.52896259979991
9 13.24726314120926 -119.75955492239184
10 14.54862581123598 -116.02779530304753
11 11.944795036950381 -113.25601936308428
12 13.295673455111682 -108.31590456875345
13 16.870133735239506 -63.684489628253
14 15.491717228200287 -52.141471551227674
15 17.835494162980467 -26.554830714509592
16 16.37814661837183 -7.0071959811727575
17 14.864779521012679 25.69124123513366
18 19.249184001237154 35.213628549995846
19 13.920133859734051 40.428355258377486
20 17.394097425276414 46.20583772082509
21 14.758158409735188 51.192000963985954
22 17.436361625790596 66.21591564279122
23 17.671817339025438 85.90004097928022
24 15.878224636428058 88.41217626317327
25 23.61525304755196 89.56728208081473
26 16.392445270437747 91.60759786486705
27 16.904383507557213 94.47894135375837
28 19.33177989628166 98.94440192500335
29 15.860872340155765 102.0062792742551
30 16.55454007256776 102.20476160553419
31 16.170990103506483 103.01364436404991
32 18.537435521604493 103.71935195099562
33 17.923852795967832 110.33860671277482
34 21.136791938450187 113.86158089834132
35 20.07942972611636 113.96536766392857
36 22.65859989495948 114.38427652291234
37 19.2217250673566 114.64275179580558
38 19.523604492656887 114.66425172742227
39 15.865765595342964 115.70154293965129
40 24.96504291333258 116.43876967111639
41 23.494566368404776 121.26452240374262
42 24.125898842699826 121.8148633500027
43 23.041694102343172 122.31117983367389
44 24.344809751491994 125.86136637734936
45 23.935562094673514 126.33642093404846
46 23.158913695719093 128.38744332488002
47 26.877295156009495 130.93804427787234
train accuracy: 0.886404464319841
validation accuracy: 0.8625886524822695
