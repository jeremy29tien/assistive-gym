demos: (480, 200, 32)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=32, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 140288
Number of trainable paramters: 140288
device: cuda:0
end of epoch 0: val_loss 0.4208466943254168, val_acc 0.8705673758865248
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.4153729170363574, val_acc 0.8838652482269503
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.5876343494974506, val_acc 0.851063829787234
trigger times: 1
end of epoch 3: val_loss 0.6067688241123435, val_acc 0.8625886524822695
trigger times: 2
end of epoch 4: val_loss 0.7124852863080771, val_acc 0.8705673758865248
trigger times: 3
end of epoch 5: val_loss 0.7378140409000828, val_acc 0.8608156028368794
trigger times: 4
end of epoch 6: val_loss 0.7428065420947588, val_acc 0.8643617021276596
trigger times: 5
end of epoch 7: val_loss 0.734605563453946, val_acc 0.8554964539007093
trigger times: 6
end of epoch 8: val_loss 0.8172674504400808, val_acc 0.8670212765957447
trigger times: 7
end of epoch 9: val_loss 0.9084053015431254, val_acc 0.8634751773049646
trigger times: 8
end of epoch 10: val_loss 0.8458357391478815, val_acc 0.8599290780141844
trigger times: 9
end of epoch 11: val_loss 0.861322251234825, val_acc 0.8634751773049646
trigger times: 10
Early stopping.
0 -57.66470479138661 -176.02327356618886
1 -54.60381883941591 -171.73538080057904
2 -59.68459973111749 -162.12021763693463
3 -48.152902219910175 -151.03182509904744
4 -41.92505994043313 -144.9053011827279
5 -35.17128929065075 -135.2361570794697
6 -37.79150375514291 -130.60907588284448
7 -26.050081849331036 -123.08440479922548
8 -36.80731925088912 -109.26967476004833
9 -43.06167999166064 -97.75429321743087
10 -19.170227931812406 -93.63303104415972
11 -15.569439102779143 -71.68281387511338
12 -11.494956364156678 -71.51650811905506
13 -19.786007860675454 -67.25891012483117
14 -7.342175846919417 -64.59517229125954
15 -25.367954862304032 -29.04983757741715
16 -4.09089751157444 -26.554830714509592
17 -16.621626193867996 7.066338713494528
18 -12.011077416595072 9.9385357692998
19 -9.697607460198924 21.358512279863625
20 -11.382657849229872 36.38260726753759
21 -8.251662549213506 51.192000963985954
22 1.505673817358911 58.67680633739908
23 0.5697580834967084 74.36839302338853
24 -9.664920640352648 79.93047048250351
25 -2.256965331340325 89.19536109182386
26 -17.97363745397888 91.60759786486705
27 -7.097785642603412 93.17220181871483
28 -4.073486723937094 94.22803327213148
29 11.544518760521896 96.79908925678686
30 0.1258767326362431 97.31123430564293
31 -0.6751044142874889 101.37202708023295
32 -4.24279997847043 101.55104413527681
33 -3.088802875485271 102.74578030549429
34 -4.016093600774184 106.02351298384437
35 4.021792400395498 106.45096995781903
36 3.1287177857011557 108.06711384653553
37 -8.12626528274268 109.02633758684364
38 0.7851927650626749 109.51702889237426
39 -8.630304680904374 110.37533862221531
40 6.265781311318278 110.43094239093074
41 5.830733167938888 110.59269304108648
42 0.8656970496522263 111.26442060476346
43 -3.979563547298312 111.55240978447834
44 5.564372370950878 120.93165361575389
45 12.239830564940348 125.17634239385316
46 52.56444097409258 129.08221841696613
47 29.55842439783737 130.45510520660062
train accuracy: 0.982532584183771
validation accuracy: 0.8634751773049646
