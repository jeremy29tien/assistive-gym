demos: (480, 200, 32)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=32, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 140288
Number of trainable paramters: 140288
device: cuda:0
end of epoch 0: val_loss 0.35289445463248603, val_acc 0.8838652482269503
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.5154700998956215, val_acc 0.8865248226950354
trigger times: 1
end of epoch 2: val_loss 0.5145062245473283, val_acc 0.8847517730496454
trigger times: 2
end of epoch 3: val_loss 0.5187978107868844, val_acc 0.8838652482269503
trigger times: 3
end of epoch 4: val_loss 0.6639991602149211, val_acc 0.875886524822695
trigger times: 4
end of epoch 5: val_loss 0.6990169555091699, val_acc 0.8812056737588653
trigger times: 5
end of epoch 6: val_loss 0.7794579430974475, val_acc 0.875886524822695
trigger times: 6
end of epoch 7: val_loss 0.7965790817940696, val_acc 0.8812056737588653
trigger times: 7
end of epoch 8: val_loss 0.8036147098186882, val_acc 0.8829787234042553
trigger times: 8
end of epoch 9: val_loss 0.777896502474603, val_acc 0.8882978723404256
trigger times: 9
end of epoch 10: val_loss 0.8789317321416095, val_acc 0.875
trigger times: 10
Early stopping.
0 -35.91331385727972 -193.77058932274844
1 -36.917208772239974 -189.65792573794374
2 -27.94358719745651 -181.53450727998975
3 -26.861191153991967 -180.94031503146633
4 -31.550448583438993 -171.07185874229646
5 -8.031378393643536 -146.8781492057455
6 -12.69514916674234 -135.2361570794697
7 -21.0875229453668 -132.32422579815238
8 -10.725227533024736 -127.52896259979991
9 -4.862624202622101 -119.75955492239184
10 2.7027734770963434 -116.02779530304753
11 -12.393145948648453 -113.25601936308428
12 2.2033878637012094 -108.31590456875345
13 5.275406777378521 -63.684489628253
14 1.7120398069964722 -52.141471551227674
15 11.058376560162287 -26.554830714509592
16 9.083772020414472 -7.0071959811727575
17 8.211370364297181 25.69124123513366
18 24.665634307602886 35.213628549995846
19 11.618202561861835 40.428355258377486
20 11.090351600432768 46.20583772082509
21 6.940385282738134 51.192000963985954
22 18.99745498557604 66.21591564279122
23 18.833542600623332 85.90004097928022
24 10.73802647460252 88.41217626317327
25 31.70845270692371 89.56728208081473
26 7.242391579973628 91.60759786486705
27 20.419131942559034 94.47894135375837
28 23.187737525324337 98.94440192500335
29 15.082970222982112 102.0062792742551
30 12.795444466522895 102.20476160553419
31 5.850804342422634 103.01364436404991
32 22.147152026649565 103.71935195099562
33 14.939329354208894 110.33860671277482
34 25.795919152194983 113.86158089834132
35 28.446224949206226 113.96536766392857
36 30.484936067601666 114.38427652291234
37 20.977118880837224 114.64275179580558
38 25.51005424396135 114.66425172742227
39 9.701713591755833 115.70154293965129
40 43.93517377402168 116.43876967111639
41 39.56375541037414 121.26452240374262
42 38.2854453386899 121.8148633500027
43 41.45266382372938 122.31117983367389
44 35.70009366929298 125.86136637734936
45 35.75665860832669 126.33642093404846
46 32.74602002557367 128.38744332488002
47 57.116057145409286 130.93804427787234
train accuracy: 0.9795512747009135
validation accuracy: 0.875
