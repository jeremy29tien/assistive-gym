demos: (480, 200, 19)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 136960
Number of trainable paramters: 136960
device: cuda:0
end of epoch 0: val_loss 0.08507889998738896, val_acc 0.9583333333333334
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.10234797644395233, val_acc 0.9609929078014184
trigger times: 1
end of epoch 2: val_loss 0.0974809564364316, val_acc 0.9627659574468085
trigger times: 2
end of epoch 3: val_loss 0.07562667460967852, val_acc 0.9689716312056738
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.16208210462210543, val_acc 0.9574468085106383
trigger times: 1
end of epoch 5: val_loss 0.06914988271384406, val_acc 0.9716312056737588
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.1038368305514815, val_acc 0.9680851063829787
trigger times: 1
end of epoch 7: val_loss 0.0904783819084641, val_acc 0.9707446808510638
trigger times: 2
end of epoch 8: val_loss 0.078212427724725, val_acc 0.9671985815602837
trigger times: 3
end of epoch 9: val_loss 0.11289548260974273, val_acc 0.9689716312056738
trigger times: 4
end of epoch 10: val_loss 0.06461564540269346, val_acc 0.9796099290780141
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.15181808481041775, val_acc 0.9636524822695035
trigger times: 1
end of epoch 12: val_loss 0.0949442501670388, val_acc 0.9680851063829787
trigger times: 2
end of epoch 13: val_loss 0.16180018469794494, val_acc 0.9618794326241135
trigger times: 3
end of epoch 14: val_loss 0.0901427018014106, val_acc 0.973404255319149
trigger times: 4
end of epoch 15: val_loss 0.05814186798099817, val_acc 0.9778368794326241
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.06905981184023993, val_acc 0.9698581560283688
trigger times: 1
end of epoch 17: val_loss 0.09938161888519632, val_acc 0.9707446808510638
trigger times: 2
end of epoch 18: val_loss 0.08531564034920865, val_acc 0.9698581560283688
trigger times: 3
end of epoch 19: val_loss 0.0761667801221935, val_acc 0.9725177304964538
trigger times: 4
end of epoch 20: val_loss 0.10899654711723289, val_acc 0.9698581560283688
trigger times: 5
end of epoch 21: val_loss 0.12100266453601687, val_acc 0.9698581560283688
trigger times: 6
end of epoch 22: val_loss 0.06873150674589082, val_acc 0.9725177304964538
trigger times: 7
end of epoch 23: val_loss 0.11927624602563204, val_acc 0.9707446808510638
trigger times: 8
end of epoch 24: val_loss 0.1138203299251747, val_acc 0.9680851063829787
trigger times: 9
end of epoch 25: val_loss 0.0841296902076507, val_acc 0.973404255319149
trigger times: 10
Early stopping.
0 -273.55121183395386 -181.59621720143082
1 -216.85731798410416 -151.71751520071632
2 -214.4961519241333 -147.47910081225547
3 -184.88379049301147 -133.76056380009578
4 -157.20768240117468 -117.07649082709086
5 -158.44694678857923 -111.99728318755238
6 -136.57730975188315 -96.08342159541479
7 -90.78622288070619 -67.56114110862306
8 -88.9045696639223 -64.68081889915388
9 -77.91718975594267 -61.275273342250266
10 -72.01140802213922 -55.44719879154653
11 -57.48504554480314 -47.635037654385464
12 -43.35299198678695 -42.93517995772881
13 -41.8041314301081 -34.935912968324274
14 21.194860951043665 1.293113865583746
15 80.05423565488309 37.008960077453615
16 150.59647770808078 77.62286246422622
17 150.14795578084886 79.3595955035673
18 170.20242902939208 94.19977594187397
19 176.14447130123153 94.60034688743727
20 175.3129825469805 94.84413775624779
21 174.0026907029096 95.93127096706885
22 173.90551527775824 97.31185243955532
23 185.79678625706583 97.75965500483397
24 176.41412668582052 99.706470840702
25 171.9796630446799 100.73191429849227
26 178.773498817347 100.75917839256617
27 193.31234424188733 105.18568330899544
28 199.51667994179297 107.05778838409489
29 194.10482431470882 107.70203301945793
30 195.98260741808917 108.28588915868846
31 200.44537887140177 111.56177998492792
32 201.78401752735954 111.89996322956685
33 202.05160796816926 112.05237740392577
34 203.01536491815932 113.21987800793073
35 201.00687251612544 113.54624416393074
36 208.9499563917052 113.91500729297944
37 211.87841212388594 116.07692970774649
38 211.72984935808927 116.10590321293648
39 210.76722718239762 117.22159336730809
40 208.8684610738419 119.00744636937388
41 218.78857575450093 119.6581196203716
42 220.85124356218148 122.2662021465837
43 223.4743049897952 123.74388483204041
44 223.60094192577526 124.11350914107612
45 221.96421527885832 125.19854714485804
46 235.13802038878202 128.07431344844193
47 236.7905474730651 129.05523536383924
train accuracy: 0.994763597446776
validation accuracy: 0.973404255319149
