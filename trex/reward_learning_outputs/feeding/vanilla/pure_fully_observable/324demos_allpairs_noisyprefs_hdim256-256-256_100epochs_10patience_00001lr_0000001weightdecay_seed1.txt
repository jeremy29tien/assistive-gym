demos: (480, 200, 19)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 136960
Number of trainable paramters: 136960
device: cuda:1
end of epoch 0: val_loss 0.08230823367018378, val_acc 0.9716312056737588
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.0729461895573738, val_acc 0.973404255319149
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.07202617615064921, val_acc 0.9769503546099291
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.09300145562077766, val_acc 0.973404255319149
trigger times: 1
end of epoch 4: val_loss 0.06680617583323492, val_acc 0.9725177304964538
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.04909163388006204, val_acc 0.9813829787234043
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.051733336397603476, val_acc 0.9787234042553191
trigger times: 1
end of epoch 7: val_loss 0.051618656174880734, val_acc 0.975177304964539
trigger times: 2
end of epoch 8: val_loss 0.04846407714007926, val_acc 0.9787234042553191
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.04490493271750934, val_acc 0.9813829787234043
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.06256979878016775, val_acc 0.975177304964539
trigger times: 1
end of epoch 11: val_loss 0.048206588105623505, val_acc 0.9831560283687943
trigger times: 2
end of epoch 12: val_loss 0.062459450724892035, val_acc 0.976063829787234
trigger times: 3
end of epoch 13: val_loss 0.04824261277983406, val_acc 0.9796099290780141
trigger times: 4
end of epoch 14: val_loss 0.05092221134470133, val_acc 0.9840425531914894
trigger times: 5
end of epoch 15: val_loss 0.044498158020993404, val_acc 0.9849290780141844
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.05358028794894972, val_acc 0.9778368794326241
trigger times: 1
end of epoch 17: val_loss 0.05387527171966688, val_acc 0.9787234042553191
trigger times: 2
end of epoch 18: val_loss 0.052134242350766066, val_acc 0.9787234042553191
trigger times: 3
end of epoch 19: val_loss 0.03691904882495129, val_acc 0.9849290780141844
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.061565066029865764, val_acc 0.9778368794326241
trigger times: 1
end of epoch 21: val_loss 0.04905186313031263, val_acc 0.9796099290780141
trigger times: 2
end of epoch 22: val_loss 0.03972003879069837, val_acc 0.9840425531914894
trigger times: 3
end of epoch 23: val_loss 0.048764771536572556, val_acc 0.9796099290780141
trigger times: 4
end of epoch 24: val_loss 0.04105510998172789, val_acc 0.9849290780141844
trigger times: 5
end of epoch 25: val_loss 0.0500392290029213, val_acc 0.9813829787234043
trigger times: 6
end of epoch 26: val_loss 0.05159523586816392, val_acc 0.9813829787234043
trigger times: 7
end of epoch 27: val_loss 0.043409614668673026, val_acc 0.9813829787234043
trigger times: 8
end of epoch 28: val_loss 0.049639478685130474, val_acc 0.9813829787234043
trigger times: 9
end of epoch 29: val_loss 0.0512836565316233, val_acc 0.9804964539007093
trigger times: 10
Early stopping.
0 -183.96524289250374 -231.98610141016846
1 -125.8239957690239 -180.38100907229617
2 -107.64476290345192 -159.58559605250935
3 -113.06327928602695 -158.91948622182267
4 -95.37550449743867 -144.22340051424408
5 -78.46670084446669 -119.33378180391257
6 -82.70050254557282 -118.93014436813327
7 -71.56531460676342 -111.99728318755238
8 -56.05422929527049 -102.05729730987049
9 -40.28191701788455 -81.78273143585434
10 -49.431039633986074 -81.15486081250215
11 -38.97120793387876 -62.06616489925748
12 -26.84753598354291 -59.021462360023776
13 -29.30690222881094 -57.48973202587643
14 -24.304260047036223 -54.2864062956764
15 -22.129639982857043 -25.99016205153756
16 0.436738638789393 -18.600859293147124
17 6.022609427251155 -9.810796929333884
18 15.659447927144356 -2.58279350160679
19 36.89508552616462 34.523887505208855
20 45.391120912725455 38.85215532611078
21 51.18420451959537 44.83371463291592
22 65.82080970241805 64.53271247267233
23 78.15215195310884 78.39646093135971
24 84.37228091066936 85.11659264541795
25 93.03670279512153 94.60034688743727
26 93.86153471108992 97.31185243955532
27 93.01121779531968 97.75965500483397
28 98.23541450648918 98.47049183311258
29 98.01275802345481 100.53051433426002
30 96.28742428963596 102.03454348571637
31 98.96566082147183 103.89263364013634
32 100.21397059725132 104.89456244575071
33 101.74188985917135 105.33214185027923
34 98.10771310456767 107.03854266018475
35 102.28769869497046 108.39772785012178
36 103.31463627837365 109.54910495619963
37 105.8793472466059 111.56177998492792
38 106.18551135234884 111.92292565511607
39 107.03616436450102 112.02235057401518
40 106.07794235527399 112.32257133288935
41 105.29806111718062 113.2348172145928
42 107.09212639048928 113.8274283128223
43 110.84957503981423 116.41091851337076
44 111.84195413836278 117.06664939486333
45 109.61422734425287 117.10824747619725
46 116.85098753024067 122.23868990266085
47 115.93221191594785 127.2098650437575
train accuracy: 0.9849405649199251
validation accuracy: 0.9804964539007093
