demos: (480, 200, 19)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 93096
num train_labels 93096
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 136960
Number of trainable paramters: 136960
device: cuda:0
end of epoch 0: val_loss 0.09485016462634208, val_acc 0.9592198581560284
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.1153010052451794, val_acc 0.9601063829787234
trigger times: 1
end of epoch 2: val_loss 0.07492856662377985, val_acc 0.9689716312056738
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.061397755661960346, val_acc 0.976063829787234
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.07846007709558663, val_acc 0.9725177304964538
trigger times: 1
end of epoch 5: val_loss 0.055140932968373996, val_acc 0.973404255319149
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.06539606452657791, val_acc 0.9769503546099291
trigger times: 1
end of epoch 7: val_loss 0.08575172750295361, val_acc 0.9671985815602837
trigger times: 2
end of epoch 8: val_loss 0.06012663554015446, val_acc 0.9769503546099291
trigger times: 3
end of epoch 9: val_loss 0.07074531871049036, val_acc 0.9725177304964538
trigger times: 4
end of epoch 10: val_loss 0.09032525106576693, val_acc 0.973404255319149
trigger times: 5
end of epoch 11: val_loss 0.056211063611004114, val_acc 0.9796099290780141
trigger times: 6
end of epoch 12: val_loss 0.08115542085956841, val_acc 0.973404255319149
trigger times: 7
end of epoch 13: val_loss 0.1188542552777353, val_acc 0.9716312056737588
trigger times: 8
end of epoch 14: val_loss 0.04661143114838967, val_acc 0.9840425531914894
trigger times: 0
saving model weights...
end of epoch 15: val_loss 0.08258244000360478, val_acc 0.9778368794326241
trigger times: 1
end of epoch 16: val_loss 0.06657258839956356, val_acc 0.9787234042553191
trigger times: 2
end of epoch 17: val_loss 0.06130770296107992, val_acc 0.9822695035460993
trigger times: 3
end of epoch 18: val_loss 0.08343638188903134, val_acc 0.9769503546099291
trigger times: 4
end of epoch 19: val_loss 0.11540661925356474, val_acc 0.976063829787234
trigger times: 5
end of epoch 20: val_loss 0.07585396436355601, val_acc 0.9769503546099291
trigger times: 6
end of epoch 21: val_loss 0.054480872633102626, val_acc 0.9822695035460993
trigger times: 7
end of epoch 22: val_loss 0.08302056483551094, val_acc 0.974290780141844
trigger times: 8
end of epoch 23: val_loss 0.050416488650293835, val_acc 0.9831560283687943
trigger times: 9
end of epoch 24: val_loss 0.06424150143720671, val_acc 0.9796099290780141
trigger times: 10
Early stopping.
0 -362.2896902561188 -181.59621720143082
1 -295.3967968225479 -151.71751520071632
2 -285.22232389450073 -147.47910081225547
3 -249.21480411291122 -133.76056380009578
4 -218.33247611974366 -117.07649082709086
5 -219.7072765380144 -111.99728318755238
6 -187.01866710186005 -96.08342159541479
7 -129.35507996939123 -67.56114110862306
8 -127.5754240420647 -64.68081889915388
9 -112.18665828183293 -61.275273342250266
10 -103.25156830018386 -55.44719879154653
11 -92.45181195251644 -47.635037654385464
12 -68.4702432397753 -42.93517995772881
13 -62.0722176800482 -34.935912968324274
14 17.269697961281054 1.293113865583746
15 101.43033931590617 37.008960077453615
16 187.75863884738646 77.62286246422622
17 184.76267722062767 79.3595955035673
18 215.66556022316217 94.19977594187397
19 213.1082909014076 94.60034688743727
20 216.4191539194435 94.84413775624779
21 216.05788230500184 95.93127096706885
22 221.1936437934637 97.31185243955532
23 219.6877562836744 97.75965500483397
24 225.62014489807189 99.706470840702
25 224.44456421770155 100.73191429849227
26 228.32268891227432 100.75917839256617
27 244.65902077849023 105.18568330899544
28 242.92797544691712 107.05778838409489
29 246.2132972124964 107.70203301945793
30 241.1098577901721 108.28588915868846
31 251.92141724657267 111.56177998492792
32 255.43843407463282 111.89996322956685
33 246.7400338849984 112.05237740392577
34 253.80408491007984 113.21987800793073
35 247.21612752694637 113.54624416393074
36 261.80438593029976 113.91500729297944
37 261.68895998969674 116.07692970774649
38 261.590114868246 116.10590321293648
39 265.31030894536525 117.22159336730809
40 263.3920709064696 119.00744636937388
41 267.9077675510198 119.6581196203716
42 275.6814564707456 122.2662021465837
43 278.2472712327726 123.74388483204041
44 275.242156165652 124.11350914107612
45 278.0567727778107 125.19854714485804
46 286.6462801329326 128.07431344844193
47 290.0542325472925 129.05523536383924
train accuracy: 0.9929642519549712
validation accuracy: 0.9796099290780141
