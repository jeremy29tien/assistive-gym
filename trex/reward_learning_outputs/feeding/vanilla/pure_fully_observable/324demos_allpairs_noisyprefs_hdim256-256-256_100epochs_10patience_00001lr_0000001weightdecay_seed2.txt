demos: (480, 200, 19)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 136960
Number of trainable paramters: 136960
device: cuda:1
end of epoch 0: val_loss 0.07734543522371043, val_acc 0.9663120567375887
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.2342868971489859, val_acc 0.9370567375886525
trigger times: 1
end of epoch 2: val_loss 0.083233501265856, val_acc 0.9654255319148937
trigger times: 2
end of epoch 3: val_loss 0.11770859387593241, val_acc 0.9609929078014184
trigger times: 3
end of epoch 4: val_loss 0.07970437776579928, val_acc 0.9654255319148937
trigger times: 4
end of epoch 5: val_loss 0.07974270700157374, val_acc 0.9636524822695035
trigger times: 5
end of epoch 6: val_loss 0.058935118104320006, val_acc 0.9716312056737588
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.07984268057193362, val_acc 0.9645390070921985
trigger times: 1
end of epoch 8: val_loss 0.07865629395593542, val_acc 0.9663120567375887
trigger times: 2
end of epoch 9: val_loss 0.10029250016613697, val_acc 0.9654255319148937
trigger times: 3
end of epoch 10: val_loss 0.08490208729932944, val_acc 0.9671985815602837
trigger times: 4
end of epoch 11: val_loss 0.062128724246465906, val_acc 0.973404255319149
trigger times: 5
end of epoch 12: val_loss 0.16382088844233714, val_acc 0.9583333333333334
trigger times: 6
end of epoch 13: val_loss 0.05862686830354661, val_acc 0.975177304964539
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.052810365213253066, val_acc 0.973404255319149
trigger times: 0
saving model weights...
end of epoch 15: val_loss 0.09015509046688994, val_acc 0.9680851063829787
trigger times: 1
end of epoch 16: val_loss 0.07799760959196843, val_acc 0.9645390070921985
trigger times: 2
end of epoch 17: val_loss 0.048095473437525335, val_acc 0.9769503546099291
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.05763983503938789, val_acc 0.9698581560283688
trigger times: 1
end of epoch 19: val_loss 0.0672400637063271, val_acc 0.9698581560283688
trigger times: 2
end of epoch 20: val_loss 0.07140751158227772, val_acc 0.9663120567375887
trigger times: 3
end of epoch 21: val_loss 0.05215176355579525, val_acc 0.976063829787234
trigger times: 4
end of epoch 22: val_loss 0.07039324484513937, val_acc 0.9716312056737588
trigger times: 5
end of epoch 23: val_loss 0.067112667873026, val_acc 0.9707446808510638
trigger times: 6
end of epoch 24: val_loss 0.05627952763595781, val_acc 0.974290780141844
trigger times: 7
end of epoch 25: val_loss 0.062356314062992024, val_acc 0.9725177304964538
trigger times: 8
end of epoch 26: val_loss 0.06230099095845398, val_acc 0.9698581560283688
trigger times: 9
end of epoch 27: val_loss 0.04923167485177435, val_acc 0.973404255319149
trigger times: 10
Early stopping.
0 -140.8675381243229 -181.59621720143082
1 -117.03045183420181 -151.71751520071632
2 -113.15581241250038 -147.47910081225547
3 -99.31728491187096 -133.76056380009578
4 -87.66039807442576 -117.07649082709086
5 -90.19566241651773 -111.99728318755238
6 -77.8750287797302 -96.08342159541479
7 -56.35382005944848 -67.56114110862306
8 -54.73575691937003 -64.68081889915388
9 -49.745432591298595 -61.275273342250266
10 -46.61198161076754 -55.44719879154653
11 -41.13074632314965 -47.635037654385464
12 -35.98005454335362 -42.93517995772881
13 -33.71655700262636 -34.935912968324274
14 -8.476702298037708 1.293113865583746
15 21.988592004810926 37.008960077453615
16 50.91028578905389 77.62286246422622
17 50.626250859349966 79.3595955035673
18 60.297715155407786 94.19977594187397
19 62.96647773857694 94.60034688743727
20 63.68286392465234 94.84413775624779
21 61.72626072866842 95.93127096706885
22 62.32015270367265 97.31185243955532
23 65.32479453925043 97.75965500483397
24 64.00775287114084 99.706470840702
25 62.62897627335042 100.73191429849227
26 65.16264317161404 100.75917839256617
27 70.52308930642903 105.18568330899544
28 73.00428245903458 107.05778838409489
29 71.54648306779563 107.70203301945793
30 73.20978143531829 108.28588915868846
31 74.53145059850067 111.56177998492792
32 74.2139656925574 111.89996322956685
33 74.18784268503077 112.05237740392577
34 75.29943812638521 113.21987800793073
35 73.34452113928273 113.54624416393074
36 77.69209803116973 113.91500729297944
37 77.42626390233636 116.07692970774649
38 79.16320298204664 116.10590321293648
39 77.1511753918603 117.22159336730809
40 77.01094795158133 119.00744636937388
41 80.8532338552177 119.6581196203716
42 81.66538521135226 122.2662021465837
43 83.09720522887073 123.74388483204041
44 83.42489651357755 124.11350914107612
45 82.83911900804378 125.19854714485804
46 88.64963739179075 128.07431344844193
47 89.30038749810774 129.05523536383924
train accuracy: 0.9860107785804381
validation accuracy: 0.973404255319149
