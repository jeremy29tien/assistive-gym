demos: (480, 200, 19)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 136960
Number of trainable paramters: 136960
device: cuda:0
end of epoch 0: val_loss 0.061232680329453566, val_acc 0.9769503546099291
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.05215749752371254, val_acc 0.9804964539007093
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.04971563082194319, val_acc 0.9822695035460993
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.056410009615185185, val_acc 0.9787234042553191
trigger times: 1
end of epoch 4: val_loss 0.049074989298303055, val_acc 0.9796099290780141
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.04557299379920066, val_acc 0.9822695035460993
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.06320390883453808, val_acc 0.9787234042553191
trigger times: 1
end of epoch 7: val_loss 0.04104804878650465, val_acc 0.9867021276595744
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.056608400997275495, val_acc 0.9867021276595744
trigger times: 1
end of epoch 9: val_loss 0.05791500090930871, val_acc 0.9849290780141844
trigger times: 2
end of epoch 10: val_loss 0.03805976291406634, val_acc 0.9875886524822695
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.04931882628789993, val_acc 0.9849290780141844
trigger times: 1
end of epoch 12: val_loss 0.05161525677624846, val_acc 0.9867021276595744
trigger times: 2
end of epoch 13: val_loss 0.045565822396600536, val_acc 0.9858156028368794
trigger times: 3
end of epoch 14: val_loss 0.056379165917241, val_acc 0.9875886524822695
trigger times: 4
end of epoch 15: val_loss 0.04695463562676948, val_acc 0.9893617021276596
trigger times: 5
end of epoch 16: val_loss 0.04250691483241107, val_acc 0.9911347517730497
trigger times: 6
end of epoch 17: val_loss 0.07119391861972495, val_acc 0.9849290780141844
trigger times: 7
end of epoch 18: val_loss 0.06995751653801571, val_acc 0.9831560283687943
trigger times: 8
end of epoch 19: val_loss 0.04856407830180273, val_acc 0.9893617021276596
trigger times: 9
end of epoch 20: val_loss 0.05066763644054114, val_acc 0.9893617021276596
trigger times: 10
Early stopping.
0 -294.83190101385117 -199.20912126544414
1 -272.54871612787247 -181.59621720143082
2 -245.60901260375977 -164.41151030608535
3 -239.38965159654617 -161.32605513368566
4 -202.94690877199173 -151.5634741258087
5 -190.09070974588394 -133.76056380009578
6 -189.87574036419392 -128.90054905506827
7 -178.72952111065388 -119.90296557087576
8 -177.77299363911152 -119.33378180391257
9 -176.04637545347214 -114.59035433631308
10 -151.02249118685722 -108.00922275200763
11 -122.39053403958678 -89.08705457223661
12 -114.32612602040172 -80.25399517559948
13 -100.0988606903702 -75.17483043591994
14 -100.22581998258829 -73.76362433676923
15 -87.99028798565269 -66.19170494681471
16 -85.37608356028795 -62.40117935748126
17 -58.778090700507164 -25.99016205153756
18 3.283951146528125 -0.07804593106747401
19 73.55693618580699 38.15087589300597
20 77.27578769996762 43.68426122331964
21 107.15981748327613 63.953351629706646
22 115.51785116363317 64.53271247267233
23 123.3426193613559 74.60390063743114
24 129.1349293962121 81.25241929740568
25 142.54575104638934 85.92277304180317
26 155.13850262016058 92.69937930010626
27 163.91410481370986 98.94672478897846
28 161.58395381085575 99.706470840702
29 167.44418720249087 103.1234668642953
30 172.8395187947899 105.49846404467546
31 169.05430254153907 106.70740032169537
32 179.44699922949076 110.39507157450213
33 183.06498801894486 110.82324768128066
34 177.00407396815717 111.56177998492792
35 184.74043958075345 112.21095617362457
36 183.7851442648098 112.73015958534099
37 190.8964756699279 113.05192791883351
38 196.5761135239154 117.07619652114646
39 195.84171714633703 117.34027665325667
40 186.83020668663085 117.83642486880923
41 201.31259041838348 119.20032127761252
42 203.07333531603217 119.43302815589446
43 196.60409001260996 120.15025122129617
44 203.3254944011569 122.63062574324714
45 204.23745712265372 125.78956526488335
46 210.16079254448414 125.85283998079782
47 211.56025625299662 126.58765053715479
train accuracy: 0.9945533769063181
validation accuracy: 0.9893617021276596
