demos: (480, 200, 19)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 93096
num train_labels 93096
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 136960
Number of trainable paramters: 136960
device: cuda:0
end of epoch 0: val_loss 0.11592613864932819, val_acc 0.9574468085106383
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.05165676125443137, val_acc 0.9831560283687943
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.05745548382882568, val_acc 0.976063829787234
trigger times: 1
end of epoch 3: val_loss 0.06021070134738804, val_acc 0.9796099290780141
trigger times: 2
end of epoch 4: val_loss 0.04906317934618216, val_acc 0.9831560283687943
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.04077135382915983, val_acc 0.9822695035460993
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.072091384784634, val_acc 0.9831560283687943
trigger times: 1
end of epoch 7: val_loss 0.05701160378449658, val_acc 0.9813829787234043
trigger times: 2
end of epoch 8: val_loss 0.04488697153004558, val_acc 0.9813829787234043
trigger times: 3
end of epoch 9: val_loss 0.03291042996750444, val_acc 0.9858156028368794
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.03354560028801135, val_acc 0.9893617021276596
trigger times: 1
end of epoch 11: val_loss 0.038037507339709606, val_acc 0.9867021276595744
trigger times: 2
end of epoch 12: val_loss 0.0606234470158347, val_acc 0.9787234042553191
trigger times: 3
end of epoch 13: val_loss 0.06435561778635848, val_acc 0.9822695035460993
trigger times: 4
end of epoch 14: val_loss 0.024531098977020008, val_acc 0.9884751773049646
trigger times: 0
saving model weights...
end of epoch 15: val_loss 0.04545038691703244, val_acc 0.9831560283687943
trigger times: 1
end of epoch 16: val_loss 0.03722756289550043, val_acc 0.9867021276595744
trigger times: 2
end of epoch 17: val_loss 0.04184990987990861, val_acc 0.9858156028368794
trigger times: 3
end of epoch 18: val_loss 0.03297082423079504, val_acc 0.9858156028368794
trigger times: 4
end of epoch 19: val_loss 0.07195815960905567, val_acc 0.9813829787234043
trigger times: 5
end of epoch 20: val_loss 0.04356151997111854, val_acc 0.9875886524822695
trigger times: 6
end of epoch 21: val_loss 0.05219548280914389, val_acc 0.9884751773049646
trigger times: 7
end of epoch 22: val_loss 0.03795904747978141, val_acc 0.9902482269503546
trigger times: 8
end of epoch 23: val_loss 0.05726171789199742, val_acc 0.9849290780141844
trigger times: 9
end of epoch 24: val_loss 0.0380813574458904, val_acc 0.9867021276595744
trigger times: 10
Early stopping.
0 -409.3606307506561 -231.98610141016846
1 -298.9231367111206 -180.38100907229617
2 -259.2036799788475 -159.58559605250935
3 -264.40813916921616 -158.91948622182267
4 -231.6915905624628 -144.22340051424408
5 -188.63004529476166 -119.33378180391257
6 -187.3883998952806 -118.93014436813327
7 -174.97875335533172 -111.99728318755238
8 -133.531666894909 -102.05729730987049
9 -115.26143342256546 -81.78273143585434
10 -117.83930874569342 -81.15486081250215
11 -86.68259402853437 -62.06616489925748
12 -68.73386260657571 -59.021462360023776
13 -67.91731888067443 -57.48973202587643
14 -57.46010306675453 -54.2864062956764
15 -52.077085463679396 -25.99016205153756
16 -8.025895362952724 -18.600859293147124
17 17.105027652607532 -9.810796929333884
18 34.20618129556533 -2.58279350160679
19 95.54989628016483 34.523887505208855
20 108.97910509316716 38.85215532611078
21 119.18080809782259 44.83371463291592
22 163.56203531322535 64.53271247267233
23 179.38706673297565 78.39646093135971
24 202.26704475854058 85.11659264541795
25 218.76435389192193 94.60034688743727
26 216.600030201138 97.31185243955532
27 221.9687660849013 97.75965500483397
28 224.9898987991619 98.47049183311258
29 224.10455761279445 100.53051433426002
30 229.4718235246837 102.03454348571637
31 230.52850967936683 103.89263364013634
32 227.88634122058284 104.89456244575071
33 232.81349771062378 105.33214185027923
34 237.24034903675783 107.03854266018475
35 243.08099317667075 108.39772785012178
36 238.47031758655794 109.54910495619963
37 245.74829820194282 111.56177998492792
38 248.44581749720965 111.92292565511607
39 247.14374247100204 112.02235057401518
40 245.19097800622694 112.32257133288935
41 245.60220204666257 113.2348172145928
42 249.59835448034573 113.8274283128223
43 253.74196470086463 116.41091851337076
44 264.3953810071689 117.06664939486333
45 258.8902846064302 117.10824747619725
46 273.02494959678734 122.23868990266085
47 273.5285973753198 127.2098650437575
train accuracy: 0.9949944143679642
validation accuracy: 0.9867021276595744
