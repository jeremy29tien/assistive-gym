demos: (480, 200, 19)
demo_rewards: (480,)
sorted_train_rewards: [-238.95654684 -231.98610141 -227.53118916 -214.41770129 -204.41223781
 -202.98814303 -199.15784785 -194.89175614 -191.99381123 -189.66740242
 -189.30789369 -187.81566641 -186.07313034 -181.8612155  -181.79899293
 -181.71412786 -180.38100907 -178.40264539 -173.32301622 -173.14447233
 -172.4637427  -168.12866719 -167.81358357 -167.24712713 -165.91483205
 -164.52171291 -163.62780324 -159.92833715 -159.58559605 -158.91948622
 -158.75876908 -157.0327098  -156.13206541 -153.82504331 -153.47830902
 -153.21950984 -152.58553553 -151.7175152  -151.09962807 -149.73158479
 -148.41006071 -148.33843128 -147.49844234 -147.47910081 -145.49135313
 -144.22340051 -141.54946899 -141.44949148 -139.97173932 -139.20066403
 -138.6625699  -138.61924236 -135.77101015 -134.29983302 -129.471792
 -124.72954457 -123.82897183 -123.2975087  -122.42507947 -122.29313461
 -121.35361905 -118.93014437 -118.60870047 -117.07649083 -111.99728319
 -111.95416879 -111.82456558 -111.55938485 -108.07859913 -107.38796212
 -106.01366403 -104.75705754 -104.35997319 -102.05729731 -102.0525638
 -100.79888461 -100.69005607 -100.4999298   -99.86993771  -98.5192976
  -98.42391453  -97.56763316  -96.12840786  -96.0834216   -95.78629767
  -95.72791887  -94.51194096  -94.32183068  -93.8760127   -92.02798597
  -91.76959412  -91.32231658  -89.88535086  -89.11512657  -88.66738786
  -84.8047471   -84.23707176  -81.78273144  -81.16405315  -81.15486081
  -79.8622669   -79.61668855  -78.68748156  -78.65722865  -76.94529937
  -76.44940192  -73.15902797  -71.12319254  -71.09815897  -70.58579775
  -67.56114111  -67.46716696  -66.28614381  -64.6808189   -63.86052285
  -63.43050581  -62.57075967  -62.0661649   -61.57054741  -61.27527334
  -59.04296906  -59.02146236  -57.48973203  -55.44719879  -54.2864063
  -53.62138787  -52.51760365  -51.33455244  -49.67503573  -49.23936644
  -47.63503765  -42.93517996  -42.00460094  -36.77131616  -34.93591297
  -31.42868743  -30.45715645  -18.60085929  -12.69846948   -9.81079693
   -8.34444432   -2.5827935     1.29311387    6.73145762    8.88422653
   17.47400651   20.84127083   23.76776644   24.74176634   26.81939256
   28.59038505   28.74539544   29.134852     33.40103908   34.52388751
   34.63659789   36.56286209   37.00896008   38.85215533   41.73677653
   44.16899732   44.83371463   45.89140537   45.89757504   46.64228055
   47.85272731   48.47037657   50.67138421   52.66182944   58.62776652
   58.80512153   59.1605071    61.46047505   62.2276965    62.74203687
   64.71853682   68.10323921   74.28757855   75.9794571    77.18749523
   77.22391005   77.62286246   78.39646093   78.48236646   79.3595955
   80.59224254   80.75730686   80.86895054   82.97123402   83.75674978
   83.8185762    83.85070653   84.42166728   85.11659265   85.90627549
   85.9404124    86.12496906   86.94904961   87.16655718   87.39561451
   87.76959527   88.21510984   88.36493556   89.8320723    91.28103493
   92.06396174   92.17709635   92.20159701   92.54666017   92.91490936
   93.35905088   93.51419927   93.76842576   94.19977594   94.60034689
   94.66769797   94.84413776   95.22886022   95.81984721   95.93127097
   96.22657096   96.34488228   97.22752342   97.31185244   97.47398698
   97.57657135   97.759655     98.06349695   98.47049183   98.74913733
   99.03122063   99.26897218   99.34565222   99.44048172   99.83148701
  100.53051433  100.55714032  100.7319143   100.75917839  100.7977088
  100.8424906   101.38435155  101.53789448  101.60764068  101.6189997
  101.63310347  102.03454349  102.08829402  102.57644569  103.16890322
  103.25402632  103.31728348  103.40232019  103.4744081   103.4933436
  103.50798167  103.89263364  103.90973367  104.34759312  104.37734949
  104.52083972  104.55501087  104.61737038  104.61967789  104.62521378
  104.76951507  104.77847229  104.8937362   104.89456245  105.05849118
  105.18568331  105.33214185  105.65716252  105.7074157   105.8045272
  106.04955685  106.1552944   106.39425528  106.46206933  106.49396275
  106.54515087  106.74622735  107.03854266  107.05778838  107.16829123
  107.21504828  107.21554847  107.2881685   107.30236866  107.70203302
  107.87670194  107.96147703  108.02820541  108.28588916  108.28858048
  108.35312869  108.39772785  108.56292436  108.72088283  108.79935704
  109.02340032  109.16071209  109.39131335  109.47694977  109.54910496
  109.70911379  109.77726109  109.82650918  110.1003944   110.19789288
  110.3173157   110.33010371  110.49514575  110.52608422  110.54482596
  110.59225047  111.34321125  111.47773403  111.4953737   111.53285156
  111.63038806  111.89996323  111.92292566  111.93947388  112.02235057
  112.0523774   112.19312071  112.21535466  112.32257133  112.39899308
  112.42779759  112.6005029   112.70127527  113.02300081  113.04770713
  113.16421742  113.18709107  113.21987801  113.23481721  113.2991647
  113.34619285  113.4220841   113.4347277   113.46527872  113.49823135
  113.54624416  113.58961069  113.79846893  113.79907099  113.8202641
  113.82742831  113.88963525  113.91500729  113.95902096  113.98972603
  114.13587883  114.14258872  114.19910711  114.22253204  114.37721831
  114.5362235   114.78023701  114.80547636  115.09443674  115.10785836
  115.13846065  115.17650337  115.96446031  116.07692971  116.10590321
  116.12798659  116.21690546  116.32952487  116.41091851  116.47915487
  116.91082189  116.9302937   117.06664939  117.07877633  117.09472388
  117.10824748  117.17575755  117.20868444  117.22159337  117.26925374
  117.39102298  117.60763941  117.69961545  117.7200324   117.74026154
  117.74588406  117.83090322  117.84339654  117.91029171  118.13142264
  118.662252    118.81864792  118.99369545  119.00744637  119.29471869
  119.48536709  119.65811962  119.72511129  120.34331779  120.35314049
  120.36794816  120.72084447  121.25240045  121.58223675  121.80949928
  122.23562738  122.2386899   122.26620215  122.41528706  122.47519798
  122.54844906  122.73860873  123.64827134  123.74388483  124.11350914
  124.1812956   124.64400215  125.19854714  126.13114011  126.46991181
  126.9066037   126.93719164  126.97860983  127.20986504  127.30803269
  128.07431345  129.05523536]
sorted_val_rewards: [-1.99209121e+02 -1.81596217e+02 -1.64411510e+02 -1.61326055e+02
 -1.51563474e+02 -1.33760564e+02 -1.28900549e+02 -1.19902966e+02
 -1.19333782e+02 -1.14590354e+02 -1.08009223e+02 -8.90870546e+01
 -8.02539952e+01 -7.51748304e+01 -7.37636243e+01 -6.61917049e+01
 -6.24011794e+01 -2.59901621e+01 -7.80459311e-02  3.81508759e+01
  4.36842612e+01  6.39533516e+01  6.45327125e+01  7.46039006e+01
  8.12524193e+01  8.59227730e+01  9.26993793e+01  9.89467248e+01
  9.97064708e+01  1.03123467e+02  1.05498464e+02  1.06707400e+02
  1.10395072e+02  1.10823248e+02  1.11561780e+02  1.12210956e+02
  1.12730160e+02  1.13051928e+02  1.17076197e+02  1.17340277e+02
  1.17836425e+02  1.19200321e+02  1.19433028e+02  1.20150251e+02
  1.22630626e+02  1.25789565e+02  1.25852840e+02  1.26587651e+02]
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:1
end of epoch 0: val_loss 0.07289315082298568, val_acc 0.9707446808510638
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.07314794635636876, val_acc 0.9725177304964538
trigger times: 1
end of epoch 2: val_loss 0.1769137256504283, val_acc 0.973404255319149
trigger times: 2
end of epoch 3: val_loss 0.10383494799161753, val_acc 0.976063829787234
trigger times: 3
end of epoch 4: val_loss 0.04372063335782521, val_acc 0.9840425531914894
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.0668206629737742, val_acc 0.9822695035460993
trigger times: 1
end of epoch 6: val_loss 0.07357573110334611, val_acc 0.9769503546099291
trigger times: 2
end of epoch 7: val_loss 0.03407341279642018, val_acc 0.9840425531914894
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.06431869688626632, val_acc 0.975177304964539
trigger times: 1
end of epoch 9: val_loss 0.047782031868688006, val_acc 0.9875886524822695
trigger times: 2
end of epoch 10: val_loss 0.10376884074994801, val_acc 0.9796099290780141
trigger times: 3
end of epoch 11: val_loss 0.0563194663864911, val_acc 0.9804964539007093
trigger times: 4
end of epoch 12: val_loss 0.061004744286949825, val_acc 0.9804964539007093
trigger times: 5
end of epoch 13: val_loss 0.05248249392961535, val_acc 0.9840425531914894
trigger times: 6
end of epoch 14: val_loss 0.0525319783099441, val_acc 0.9840425531914894
trigger times: 7
end of epoch 15: val_loss 0.05358573521657663, val_acc 0.9831560283687943
trigger times: 8
end of epoch 16: val_loss 0.05261629713020044, val_acc 0.9858156028368794
trigger times: 9
end of epoch 17: val_loss 0.052380540058837036, val_acc 0.9840425531914894
trigger times: 10
Early stopping.
0 -183.36545395851135 -199.20912126544414
1 -171.1441442668438 -181.59621720143082
2 -143.78459665179253 -164.41151030608535
3 -152.1319239139557 -161.32605513368566
4 -128.8429275304079 -151.5634741258087
5 -126.11098359525204 -133.76056380009578
6 -134.6697531491518 -128.90054905506827
7 -125.95485282316804 -119.90296557087576
8 -119.59516176581383 -119.33378180391257
9 -114.33973366208375 -114.59035433631308
10 -104.99747982621193 -108.00922275200763
11 -92.29843042988796 -89.08705457223661
12 -89.53197651356459 -80.25399517559948
13 -76.08081270381808 -75.17483043591994
14 -79.31813343148679 -73.76362433676923
15 -70.46893935604021 -66.19170494681471
16 -69.46621434949338 -62.40117935748126
17 -49.765752874314785 -25.99016205153756
18 -7.029424307867885 -0.07804593106747401
19 34.06968519138172 38.15087589300597
20 37.49779763352126 43.68426122331964
21 58.88614949956536 63.953351629706646
22 63.260393970645964 64.53271247267233
23 61.43964779842645 74.60390063743114
24 72.94669712241739 81.25241929740568
25 80.10869239270687 85.92277304180317
26 85.08965008612722 92.69937930010626
27 93.86330197611824 98.94672478897846
28 92.65782597195357 99.706470840702
29 99.21545298118144 103.1234668642953
30 100.31318201497197 105.49846404467546
31 96.99425387568772 106.70740032169537
32 101.71732857031748 110.39507157450213
33 105.2157318922691 110.82324768128066
34 104.99288001493551 111.56177998492792
35 108.74738315911964 112.21095617362457
36 110.11159647174645 112.73015958534099
37 108.79523745225742 113.05192791883351
38 115.1846912006149 117.07619652114646
39 116.72552344878204 117.34027665325667
40 114.16023293463513 117.83642486880923
41 115.0123843983747 119.20032127761252
42 117.94781243545003 119.43302815589446
43 117.66792773560155 120.15025122129617
44 118.00314097083174 122.63062574324714
45 117.86917547462508 125.78956526488335
46 124.21405927045271 125.85283998079782
47 119.52188216231298 126.58765053715479
train accuracy: 0.9815126050420168
validation accuracy: 0.9840425531914894
