demos: (480, 200, 19)
demo_rewards: (480,)
sorted_train_rewards: [-2.38956547e+02 -2.31986101e+02 -2.27531189e+02 -2.14417701e+02
 -2.04412238e+02 -2.02988143e+02 -1.99209121e+02 -1.99157848e+02
 -1.94891756e+02 -1.91993811e+02 -1.89667402e+02 -1.89307894e+02
 -1.87815666e+02 -1.86073130e+02 -1.81861215e+02 -1.81798993e+02
 -1.81714128e+02 -1.80381009e+02 -1.78402645e+02 -1.73323016e+02
 -1.73144472e+02 -1.72463743e+02 -1.68128667e+02 -1.67813584e+02
 -1.67247127e+02 -1.65914832e+02 -1.64521713e+02 -1.64411510e+02
 -1.63627803e+02 -1.61326055e+02 -1.59928337e+02 -1.59585596e+02
 -1.58919486e+02 -1.58758769e+02 -1.57032710e+02 -1.56132065e+02
 -1.53825043e+02 -1.53478309e+02 -1.53219510e+02 -1.52585536e+02
 -1.51563474e+02 -1.51099628e+02 -1.49731585e+02 -1.48410061e+02
 -1.48338431e+02 -1.47498442e+02 -1.45491353e+02 -1.44223401e+02
 -1.41549469e+02 -1.41449491e+02 -1.39971739e+02 -1.39200664e+02
 -1.38662570e+02 -1.38619242e+02 -1.35771010e+02 -1.34299833e+02
 -1.29471792e+02 -1.28900549e+02 -1.24729545e+02 -1.23828972e+02
 -1.23297509e+02 -1.22425079e+02 -1.22293135e+02 -1.21353619e+02
 -1.19902966e+02 -1.19333782e+02 -1.18930144e+02 -1.18608700e+02
 -1.14590354e+02 -1.11954169e+02 -1.11824566e+02 -1.11559385e+02
 -1.08078599e+02 -1.08009223e+02 -1.07387962e+02 -1.06013664e+02
 -1.04757058e+02 -1.04359973e+02 -1.02057297e+02 -1.02052564e+02
 -1.00798885e+02 -1.00690056e+02 -1.00499930e+02 -9.98699377e+01
 -9.85192976e+01 -9.84239145e+01 -9.75676332e+01 -9.61284079e+01
 -9.57862977e+01 -9.57279189e+01 -9.45119410e+01 -9.43218307e+01
 -9.38760127e+01 -9.20279860e+01 -9.17695941e+01 -9.13223166e+01
 -8.98853509e+01 -8.91151266e+01 -8.90870546e+01 -8.86673879e+01
 -8.48047471e+01 -8.42370718e+01 -8.17827314e+01 -8.11640532e+01
 -8.11548608e+01 -8.02539952e+01 -7.98622669e+01 -7.96166886e+01
 -7.86874816e+01 -7.86572287e+01 -7.69452994e+01 -7.64494019e+01
 -7.51748304e+01 -7.37636243e+01 -7.31590280e+01 -7.11231925e+01
 -7.10981590e+01 -7.05857977e+01 -6.74671670e+01 -6.62861438e+01
 -6.61917049e+01 -6.38605228e+01 -6.34305058e+01 -6.25707597e+01
 -6.24011794e+01 -6.20661649e+01 -6.15705474e+01 -5.90429691e+01
 -5.90214624e+01 -5.74897320e+01 -5.42864063e+01 -5.36213879e+01
 -5.25176037e+01 -5.13345524e+01 -4.96750357e+01 -4.92393664e+01
 -4.20046009e+01 -3.67713162e+01 -3.14286874e+01 -3.04571565e+01
 -2.59901621e+01 -1.86008593e+01 -1.26984695e+01 -9.81079693e+00
 -8.34444432e+00 -2.58279350e+00 -7.80459311e-02  6.73145762e+00
  8.88422653e+00  1.74740065e+01  2.08412708e+01  2.37677664e+01
  2.47417663e+01  2.68193926e+01  2.85903851e+01  2.87453954e+01
  2.91348520e+01  3.34010391e+01  3.45238875e+01  3.46365979e+01
  3.65628621e+01  3.81508759e+01  3.88521553e+01  4.17367765e+01
  4.36842612e+01  4.41689973e+01  4.48337146e+01  4.58914054e+01
  4.58975750e+01  4.66422805e+01  4.78527273e+01  4.84703766e+01
  5.06713842e+01  5.26618294e+01  5.86277665e+01  5.88051215e+01
  5.91605071e+01  6.14604751e+01  6.22276965e+01  6.27420369e+01
  6.39533516e+01  6.45327125e+01  6.47185368e+01  6.81032392e+01
  7.42875786e+01  7.46039006e+01  7.59794571e+01  7.71874952e+01
  7.72239101e+01  7.83964609e+01  7.84823665e+01  8.05922425e+01
  8.07573069e+01  8.08689505e+01  8.12524193e+01  8.29712340e+01
  8.37567498e+01  8.38185762e+01  8.38507065e+01  8.44216673e+01
  8.51165926e+01  8.59062755e+01  8.59227730e+01  8.59404124e+01
  8.61249691e+01  8.69490496e+01  8.71665572e+01  8.73956145e+01
  8.77695953e+01  8.82151098e+01  8.83649356e+01  8.98320723e+01
  9.12810349e+01  9.20639617e+01  9.21770964e+01  9.22015970e+01
  9.25466602e+01  9.26993793e+01  9.29149094e+01  9.33590509e+01
  9.35141993e+01  9.37684258e+01  9.46676980e+01  9.52288602e+01
  9.58198472e+01  9.62265710e+01  9.63448823e+01  9.72275234e+01
  9.74739870e+01  9.75765713e+01  9.80634969e+01  9.84704918e+01
  9.87491373e+01  9.89467248e+01  9.90312206e+01  9.92689722e+01
  9.93456522e+01  9.94404817e+01  9.98314870e+01  1.00530514e+02
  1.00557140e+02  1.00797709e+02  1.00842491e+02  1.01384352e+02
  1.01537894e+02  1.01607641e+02  1.01619000e+02  1.01633103e+02
  1.02034543e+02  1.02088294e+02  1.02576446e+02  1.03123467e+02
  1.03168903e+02  1.03254026e+02  1.03317283e+02  1.03402320e+02
  1.03474408e+02  1.03493344e+02  1.03507982e+02  1.03892634e+02
  1.03909734e+02  1.04347593e+02  1.04377349e+02  1.04520840e+02
  1.04555011e+02  1.04617370e+02  1.04619678e+02  1.04625214e+02
  1.04769515e+02  1.04778472e+02  1.04893736e+02  1.04894562e+02
  1.05058491e+02  1.05332142e+02  1.05498464e+02  1.05657163e+02
  1.05707416e+02  1.05804527e+02  1.06049557e+02  1.06155294e+02
  1.06394255e+02  1.06462069e+02  1.06493963e+02  1.06545151e+02
  1.06707400e+02  1.06746227e+02  1.07038543e+02  1.07168291e+02
  1.07215048e+02  1.07215548e+02  1.07288168e+02  1.07302369e+02
  1.07876702e+02  1.07961477e+02  1.08028205e+02  1.08288580e+02
  1.08353129e+02  1.08397728e+02  1.08562924e+02  1.08720883e+02
  1.08799357e+02  1.09023400e+02  1.09160712e+02  1.09391313e+02
  1.09476950e+02  1.09549105e+02  1.09709114e+02  1.09777261e+02
  1.09826509e+02  1.10100394e+02  1.10197893e+02  1.10317316e+02
  1.10330104e+02  1.10395072e+02  1.10495146e+02  1.10526084e+02
  1.10544826e+02  1.10592250e+02  1.10823248e+02  1.11343211e+02
  1.11477734e+02  1.11495374e+02  1.11532852e+02  1.11630388e+02
  1.11922926e+02  1.11939474e+02  1.12022351e+02  1.12193121e+02
  1.12210956e+02  1.12215355e+02  1.12322571e+02  1.12398993e+02
  1.12427798e+02  1.12600503e+02  1.12701275e+02  1.12730160e+02
  1.13023001e+02  1.13047707e+02  1.13051928e+02  1.13164217e+02
  1.13187091e+02  1.13234817e+02  1.13299165e+02  1.13346193e+02
  1.13422084e+02  1.13434728e+02  1.13465279e+02  1.13498231e+02
  1.13589611e+02  1.13798469e+02  1.13799071e+02  1.13820264e+02
  1.13827428e+02  1.13889635e+02  1.13959021e+02  1.13989726e+02
  1.14135879e+02  1.14142589e+02  1.14199107e+02  1.14222532e+02
  1.14377218e+02  1.14536223e+02  1.14780237e+02  1.14805476e+02
  1.15094437e+02  1.15107858e+02  1.15138461e+02  1.15176503e+02
  1.15964460e+02  1.16127987e+02  1.16216905e+02  1.16329525e+02
  1.16410919e+02  1.16479155e+02  1.16910822e+02  1.16930294e+02
  1.17066649e+02  1.17076197e+02  1.17078776e+02  1.17094724e+02
  1.17108247e+02  1.17175758e+02  1.17208684e+02  1.17269254e+02
  1.17340277e+02  1.17391023e+02  1.17607639e+02  1.17699615e+02
  1.17720032e+02  1.17740262e+02  1.17745884e+02  1.17830903e+02
  1.17836425e+02  1.17843397e+02  1.17910292e+02  1.18131423e+02
  1.18662252e+02  1.18818648e+02  1.18993695e+02  1.19200321e+02
  1.19294719e+02  1.19433028e+02  1.19485367e+02  1.19725111e+02
  1.20150251e+02  1.20343318e+02  1.20353140e+02  1.20367948e+02
  1.20720844e+02  1.21252400e+02  1.21582237e+02  1.21809499e+02
  1.22235627e+02  1.22238690e+02  1.22415287e+02  1.22475198e+02
  1.22548449e+02  1.22630626e+02  1.22738609e+02  1.23648271e+02
  1.24181296e+02  1.24644002e+02  1.25789565e+02  1.25852840e+02
  1.26131140e+02  1.26469912e+02  1.26587651e+02  1.26906604e+02
  1.26937192e+02  1.26978610e+02  1.27209865e+02  1.27308033e+02]
sorted_val_rewards: [-181.5962172  -151.7175152  -147.47910081 -133.7605638  -117.07649083
 -111.99728319  -96.0834216   -67.56114111  -64.6808189   -61.27527334
  -55.44719879  -47.63503765  -42.93517996  -34.93591297    1.29311387
   37.00896008   77.62286246   79.3595955    94.19977594   94.60034689
   94.84413776   95.93127097   97.31185244   97.759655     99.70647084
  100.7319143   100.75917839  105.18568331  107.05778838  107.70203302
  108.28588916  111.56177998  111.89996323  112.0523774   113.21987801
  113.54624416  113.91500729  116.07692971  116.10590321  117.22159337
  119.00744637  119.65811962  122.26620215  123.74388483  124.11350914
  125.19854714  128.07431345  129.05523536]
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:1
end of epoch 0: val_loss 0.10993171772682664, val_acc 0.9530141843971631
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.06279537053507625, val_acc 0.974290780141844
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.11185362099711943, val_acc 0.9636524822695035
trigger times: 1
end of epoch 3: val_loss 0.06455570096060352, val_acc 0.9725177304964538
trigger times: 2
end of epoch 4: val_loss 0.09475346631408572, val_acc 0.9680851063829787
trigger times: 3
end of epoch 5: val_loss 0.07785495339443624, val_acc 0.974290780141844
trigger times: 4
end of epoch 6: val_loss 0.10492483989028355, val_acc 0.9609929078014184
trigger times: 5
end of epoch 7: val_loss 0.09654907706053505, val_acc 0.975177304964539
trigger times: 6
end of epoch 8: val_loss 0.0616893379329697, val_acc 0.976063829787234
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.06797274832066806, val_acc 0.976063829787234
trigger times: 1
end of epoch 10: val_loss 0.16155971526197177, val_acc 0.9618794326241135
trigger times: 2
end of epoch 11: val_loss 0.07267449968609499, val_acc 0.975177304964539
trigger times: 3
end of epoch 12: val_loss 0.0601450036518133, val_acc 0.9796099290780141
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.06516227969694685, val_acc 0.9787234042553191
trigger times: 1
end of epoch 14: val_loss 0.06315182570705095, val_acc 0.976063829787234
trigger times: 2
end of epoch 15: val_loss 0.07385979989179146, val_acc 0.9769503546099291
trigger times: 3
end of epoch 16: val_loss 0.0728478552493419, val_acc 0.9769503546099291
trigger times: 4
end of epoch 17: val_loss 0.05860212361282407, val_acc 0.9778368794326241
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.06629624713561873, val_acc 0.9796099290780141
trigger times: 1
end of epoch 19: val_loss 0.06906655915387813, val_acc 0.975177304964539
trigger times: 2
end of epoch 20: val_loss 0.09312099074951492, val_acc 0.973404255319149
trigger times: 3
end of epoch 21: val_loss 0.07865353861321267, val_acc 0.9778368794326241
trigger times: 4
end of epoch 22: val_loss 0.09151382786574422, val_acc 0.976063829787234
trigger times: 5
end of epoch 23: val_loss 0.0688348622362124, val_acc 0.9796099290780141
trigger times: 6
end of epoch 24: val_loss 0.054573017997188666, val_acc 0.9849290780141844
trigger times: 0
saving model weights...
end of epoch 25: val_loss 0.07063971937027545, val_acc 0.9769503546099291
trigger times: 1
end of epoch 26: val_loss 0.10555880473094532, val_acc 0.976063829787234
trigger times: 2
end of epoch 27: val_loss 0.17258832373500596, val_acc 0.9689716312056738
trigger times: 3
end of epoch 28: val_loss 0.16647238722141772, val_acc 0.9663120567375887
trigger times: 4
end of epoch 29: val_loss 0.13917071957983992, val_acc 0.9725177304964538
trigger times: 5
end of epoch 30: val_loss 0.10887579409134658, val_acc 0.9769503546099291
trigger times: 6
end of epoch 31: val_loss 0.13852826958587364, val_acc 0.9707446808510638
trigger times: 7
end of epoch 32: val_loss 0.2078950561367184, val_acc 0.9592198581560284
trigger times: 8
end of epoch 33: val_loss 0.13365025912461664, val_acc 0.9707446808510638
trigger times: 9
end of epoch 34: val_loss 0.08200699845077278, val_acc 0.9778368794326241
trigger times: 10
Early stopping.
0 -180.10193259269 -181.59621720143082
1 -122.3777077421546 -151.71751520071632
2 -139.8053206950426 -147.47910081225547
3 -123.29242048412561 -133.76056380009578
4 -122.21231351327151 -117.07649082709086
5 -98.01324849203229 -111.99728318755238
6 -85.25037603871897 -96.08342159541479
7 -53.89058673102409 -67.56114110862306
8 -45.90859196707606 -64.68081889915388
9 -42.401051570661366 -61.275273342250266
10 -35.789549296721816 -55.44719879154653
11 -35.65506149083376 -47.635037654385464
12 -18.129248274490237 -42.93517995772881
13 -27.704376560635865 -34.935912968324274
14 18.39562771189958 1.293113865583746
15 74.03291185107082 37.008960077453615
16 118.65752790402621 77.62286246422622
17 120.84757303912193 79.3595955035673
18 128.13170112017542 94.19977594187397
19 132.42341309599578 94.60034688743727
20 130.7201967407018 94.84413775624779
21 135.12678254488856 95.93127096706885
22 135.0745195345953 97.31185243955532
23 131.75509587023407 97.75965500483397
24 136.35238994844258 99.706470840702
25 135.93712684325874 100.73191429849227
26 139.11735001485795 100.75917839256617
27 140.3066470520571 105.18568330899544
28 149.47859337273985 107.05778838409489
29 149.30314614623785 107.70203301945793
30 153.6260573612526 108.28588915868846
31 158.39707606192678 111.56177998492792
32 156.3950660470873 111.89996322956685
33 157.1195178227499 112.05237740392577
34 159.10820147115737 113.21987800793073
35 152.39449254516512 113.54624416393074
36 161.21010508947074 113.91500729297944
37 161.66226224787533 116.07692970774649
38 162.6498688212596 116.10590321293648
39 158.77138735726476 117.22159336730809
40 164.34811928402632 119.00744636937388
41 163.41676958464086 119.6581196203716
42 166.46126585267484 122.2662021465837
43 166.26020944444463 123.74388483204041
44 171.27110343053937 124.11350914107612
45 168.40306394454092 125.19854714485804
46 172.73569885548204 128.07431344844193
47 172.71654619462788 129.05523536383924
train accuracy: 0.9918767507002801
validation accuracy: 0.9778368794326241
