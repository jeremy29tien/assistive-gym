demos: (480, 200, 19)
demo_rewards: (480,)
sorted_train_rewards: [-2.38956547e+02 -2.31986101e+02 -2.27531189e+02 -2.04412238e+02
 -2.02988143e+02 -1.91993811e+02 -1.89667402e+02 -1.89307894e+02
 -1.87815666e+02 -1.86073130e+02 -1.81861215e+02 -1.81798993e+02
 -1.81714128e+02 -1.81596217e+02 -1.80381009e+02 -1.78402645e+02
 -1.73323016e+02 -1.73144472e+02 -1.72463743e+02 -1.68128667e+02
 -1.67813584e+02 -1.64521713e+02 -1.64411510e+02 -1.63627803e+02
 -1.61326055e+02 -1.59928337e+02 -1.59585596e+02 -1.58919486e+02
 -1.58758769e+02 -1.57032710e+02 -1.56132065e+02 -1.53478309e+02
 -1.53219510e+02 -1.51717515e+02 -1.51563474e+02 -1.49731585e+02
 -1.48410061e+02 -1.48338431e+02 -1.47498442e+02 -1.47479101e+02
 -1.45491353e+02 -1.44223401e+02 -1.41549469e+02 -1.41449491e+02
 -1.39971739e+02 -1.39200664e+02 -1.38662570e+02 -1.35771010e+02
 -1.34299833e+02 -1.33760564e+02 -1.29471792e+02 -1.28900549e+02
 -1.24729545e+02 -1.23828972e+02 -1.22425079e+02 -1.22293135e+02
 -1.21353619e+02 -1.19902966e+02 -1.19333782e+02 -1.18930144e+02
 -1.18608700e+02 -1.17076491e+02 -1.14590354e+02 -1.11997283e+02
 -1.11954169e+02 -1.11824566e+02 -1.11559385e+02 -1.08078599e+02
 -1.08009223e+02 -1.07387962e+02 -1.06013664e+02 -1.04757058e+02
 -1.04359973e+02 -1.02057297e+02 -1.02052564e+02 -1.00798885e+02
 -1.00690056e+02 -1.00499930e+02 -9.98699377e+01 -9.85192976e+01
 -9.84239145e+01 -9.75676332e+01 -9.60834216e+01 -9.57862977e+01
 -9.57279189e+01 -9.45119410e+01 -9.43218307e+01 -9.38760127e+01
 -9.20279860e+01 -9.17695941e+01 -9.13223166e+01 -8.98853509e+01
 -8.91151266e+01 -8.90870546e+01 -8.86673879e+01 -8.48047471e+01
 -8.42370718e+01 -8.17827314e+01 -8.11640532e+01 -8.11548608e+01
 -8.02539952e+01 -7.98622669e+01 -7.86874816e+01 -7.86572287e+01
 -7.64494019e+01 -7.37636243e+01 -7.31590280e+01 -7.11231925e+01
 -7.10981590e+01 -7.05857977e+01 -6.75611411e+01 -6.74671670e+01
 -6.62861438e+01 -6.61917049e+01 -6.46808189e+01 -6.38605228e+01
 -6.25707597e+01 -6.24011794e+01 -6.20661649e+01 -6.15705474e+01
 -6.12752733e+01 -5.90429691e+01 -5.90214624e+01 -5.74897320e+01
 -5.54471988e+01 -5.42864063e+01 -5.36213879e+01 -5.25176037e+01
 -5.13345524e+01 -4.96750357e+01 -4.92393664e+01 -4.76350377e+01
 -4.29351800e+01 -4.20046009e+01 -3.67713162e+01 -3.49359130e+01
 -3.14286874e+01 -3.04571565e+01 -2.59901621e+01 -1.86008593e+01
 -1.26984695e+01 -9.81079693e+00 -8.34444432e+00 -2.58279350e+00
 -7.80459311e-02  1.29311387e+00  6.73145762e+00  8.88422653e+00
  1.74740065e+01  2.08412708e+01  2.37677664e+01  2.47417663e+01
  2.68193926e+01  2.85903851e+01  2.87453954e+01  2.91348520e+01
  3.46365979e+01  3.65628621e+01  3.70089601e+01  3.88521553e+01
  4.17367765e+01  4.36842612e+01  4.41689973e+01  4.48337146e+01
  4.58914054e+01  4.58975750e+01  4.66422805e+01  4.78527273e+01
  4.84703766e+01  5.06713842e+01  5.26618294e+01  5.88051215e+01
  5.91605071e+01  6.14604751e+01  6.22276965e+01  6.27420369e+01
  6.47185368e+01  6.81032392e+01  7.42875786e+01  7.46039006e+01
  7.59794571e+01  7.71874952e+01  7.72239101e+01  7.76228625e+01
  7.83964609e+01  7.93595955e+01  8.05922425e+01  8.07573069e+01
  8.08689505e+01  8.12524193e+01  8.29712340e+01  8.37567498e+01
  8.38185762e+01  8.38507065e+01  8.44216673e+01  8.51165926e+01
  8.59062755e+01  8.59227730e+01  8.59404124e+01  8.61249691e+01
  8.69490496e+01  8.71665572e+01  8.73956145e+01  8.77695953e+01
  8.98320723e+01  9.12810349e+01  9.20639617e+01  9.21770964e+01
  9.22015970e+01  9.26993793e+01  9.29149094e+01  9.33590509e+01
  9.35141993e+01  9.37684258e+01  9.41997759e+01  9.46003469e+01
  9.46676980e+01  9.52288602e+01  9.58198472e+01  9.59312710e+01
  9.62265710e+01  9.63448823e+01  9.72275234e+01  9.74739870e+01
  9.75765713e+01  9.77596550e+01  9.80634969e+01  9.84704918e+01
  9.87491373e+01  9.89467248e+01  9.90312206e+01  9.92689722e+01
  9.93456522e+01  9.94404817e+01  9.97064708e+01  9.98314870e+01
  1.00530514e+02  1.00557140e+02  1.00731914e+02  1.00759178e+02
  1.00842491e+02  1.01384352e+02  1.01537894e+02  1.01607641e+02
  1.01619000e+02  1.01633103e+02  1.02034543e+02  1.02088294e+02
  1.02576446e+02  1.03123467e+02  1.03168903e+02  1.03254026e+02
  1.03317283e+02  1.03402320e+02  1.03474408e+02  1.03493344e+02
  1.03507982e+02  1.03892634e+02  1.04347593e+02  1.04520840e+02
  1.04555011e+02  1.04617370e+02  1.04619678e+02  1.04769515e+02
  1.04778472e+02  1.04893736e+02  1.04894562e+02  1.05185683e+02
  1.05332142e+02  1.05498464e+02  1.05657163e+02  1.05707416e+02
  1.05804527e+02  1.06049557e+02  1.06394255e+02  1.06462069e+02
  1.06493963e+02  1.06545151e+02  1.06707400e+02  1.07038543e+02
  1.07057788e+02  1.07168291e+02  1.07215048e+02  1.07215548e+02
  1.07288168e+02  1.07302369e+02  1.07702033e+02  1.07876702e+02
  1.07961477e+02  1.08028205e+02  1.08285889e+02  1.08288580e+02
  1.08353129e+02  1.08397728e+02  1.08720883e+02  1.09023400e+02
  1.09160712e+02  1.09391313e+02  1.09476950e+02  1.09709114e+02
  1.09777261e+02  1.09826509e+02  1.10100394e+02  1.10197893e+02
  1.10317316e+02  1.10330104e+02  1.10395072e+02  1.10495146e+02
  1.10526084e+02  1.10544826e+02  1.10592250e+02  1.10823248e+02
  1.11343211e+02  1.11495374e+02  1.11532852e+02  1.11561780e+02
  1.11630388e+02  1.11899963e+02  1.11922926e+02  1.11939474e+02
  1.12022351e+02  1.12052377e+02  1.12193121e+02  1.12210956e+02
  1.12215355e+02  1.12322571e+02  1.12398993e+02  1.12427798e+02
  1.12701275e+02  1.12730160e+02  1.13023001e+02  1.13047707e+02
  1.13051928e+02  1.13164217e+02  1.13187091e+02  1.13219878e+02
  1.13234817e+02  1.13299165e+02  1.13346193e+02  1.13434728e+02
  1.13465279e+02  1.13498231e+02  1.13546244e+02  1.13589611e+02
  1.13798469e+02  1.13799071e+02  1.13820264e+02  1.13827428e+02
  1.13889635e+02  1.13915007e+02  1.13959021e+02  1.13989726e+02
  1.14142589e+02  1.14199107e+02  1.14222532e+02  1.14377218e+02
  1.14536223e+02  1.14780237e+02  1.14805476e+02  1.15094437e+02
  1.15107858e+02  1.15138461e+02  1.15176503e+02  1.15964460e+02
  1.16076930e+02  1.16105903e+02  1.16127987e+02  1.16216905e+02
  1.16329525e+02  1.16410919e+02  1.16479155e+02  1.16910822e+02
  1.16930294e+02  1.17066649e+02  1.17076197e+02  1.17078776e+02
  1.17094724e+02  1.17108247e+02  1.17175758e+02  1.17208684e+02
  1.17221593e+02  1.17269254e+02  1.17340277e+02  1.17607639e+02
  1.17699615e+02  1.17720032e+02  1.17740262e+02  1.17745884e+02
  1.17830903e+02  1.17843397e+02  1.18131423e+02  1.18662252e+02
  1.18818648e+02  1.18993695e+02  1.19007446e+02  1.19200321e+02
  1.19294719e+02  1.19433028e+02  1.19485367e+02  1.19658120e+02
  1.20150251e+02  1.20343318e+02  1.20353140e+02  1.20367948e+02
  1.20720844e+02  1.21252400e+02  1.21582237e+02  1.21809499e+02
  1.22235627e+02  1.22238690e+02  1.22266202e+02  1.22415287e+02
  1.22475198e+02  1.22548449e+02  1.22630626e+02  1.22738609e+02
  1.23648271e+02  1.23743885e+02  1.24181296e+02  1.24644002e+02
  1.25198547e+02  1.25789565e+02  1.25852840e+02  1.26131140e+02
  1.26469912e+02  1.26587651e+02  1.26906604e+02  1.26937192e+02
  1.26978610e+02  1.27209865e+02  1.28074313e+02  1.29055235e+02]
sorted_val_rewards: [-214.41770129 -199.20912127 -199.15784785 -194.89175614 -167.24712713
 -165.91483205 -153.82504331 -152.58553553 -151.09962807 -138.61924236
 -123.2975087   -96.12840786  -79.61668855  -76.94529937  -75.17483044
  -63.43050581   33.40103908   34.52388751   38.15087589   58.62776652
   63.95335163   64.53271247   78.48236646   88.21510984   88.36493556
   92.54666017   94.84413776   97.31185244  100.7977088   103.90973367
  104.37734949  104.62521378  105.05849118  106.1552944   106.74622735
  108.56292436  108.79935704  109.54910496  111.47773403  112.6005029
  113.4220841   114.13587883  117.39102298  117.83642487  117.91029171
  119.72511129  124.11350914  127.30803269]
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:3
end of epoch 0: val_loss 0.05874227048700475, val_acc 0.9804964539007093
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.04846902458063404, val_acc 0.9840425531914894
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.04226412701924991, val_acc 0.9849290780141844
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.05851637247056233, val_acc 0.9796099290780141
trigger times: 1
end of epoch 4: val_loss 0.1203022141310328, val_acc 0.9725177304964538
trigger times: 2
end of epoch 5: val_loss 0.06671602387236394, val_acc 0.9778368794326241
trigger times: 3
end of epoch 6: val_loss 0.06567887913911753, val_acc 0.976063829787234
trigger times: 4
end of epoch 7: val_loss 0.04271657491271838, val_acc 0.9858156028368794
trigger times: 5
end of epoch 8: val_loss 0.05946883991092955, val_acc 0.9831560283687943
trigger times: 6
end of epoch 9: val_loss 0.05023839835691631, val_acc 0.9849290780141844
trigger times: 7
end of epoch 10: val_loss 0.041336524915026804, val_acc 0.9875886524822695
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.07281085363263871, val_acc 0.9804964539007093
trigger times: 1
end of epoch 12: val_loss 0.06413429506554275, val_acc 0.9796099290780141
trigger times: 2
end of epoch 13: val_loss 0.057040273952300144, val_acc 0.9822695035460993
trigger times: 3
end of epoch 14: val_loss 0.053949152637937065, val_acc 0.9840425531914894
trigger times: 4
end of epoch 15: val_loss 0.048173299372644154, val_acc 0.9893617021276596
trigger times: 5
end of epoch 16: val_loss 0.07018088736817951, val_acc 0.9831560283687943
trigger times: 6
end of epoch 17: val_loss 0.06417603712763965, val_acc 0.9849290780141844
trigger times: 7
end of epoch 18: val_loss 0.10461829815413753, val_acc 0.9778368794326241
trigger times: 8
end of epoch 19: val_loss 0.0755197807851533, val_acc 0.9840425531914894
trigger times: 9
end of epoch 20: val_loss 0.10349773826862647, val_acc 0.975177304964539
trigger times: 10
Early stopping.
0 -218.28230106830597 -214.417701286285
1 -228.49413639307022 -199.20912126544414
2 -173.80396556854248 -199.15784784598026
3 -185.45924715697765 -194.8917561390424
4 -144.27414137125015 -167.2471271326611
5 -157.91428750753403 -165.91483205068334
6 -135.40268890932202 -153.8250433077314
7 -134.0952197611332 -152.58553553277824
8 -142.49134046584368 -151.09962806671584
9 -124.58800421282649 -138.61924235846436
10 -99.08785957843065 -123.29750870197792
11 -68.80639431928284 -96.12840786180749
12 -57.389518504729494 -79.61668855255952
13 -56.863977529865224 -76.94529937266098
14 -45.59134834050201 -75.17483043591994
15 -37.88469453994185 -63.43050580727678
16 92.8416348984465 33.401039078259274
17 94.79026840877486 34.523887505208855
18 102.48107248230372 38.15087589300597
19 127.65548925145413 58.627766523049665
20 134.8474113335833 63.953351629706646
21 139.34284325983026 64.53271247267233
22 145.26276395027526 78.48236646312385
23 164.5970880400855 88.2151098413278
24 161.92803918692516 88.36493556294752
25 169.06390711152926 92.54666016860398
26 175.6182009014301 94.84413775624779
27 175.27854313072748 97.31185243955532
28 178.90198494211654 100.7977087983452
29 186.99867651506793 103.90973367320053
30 185.92104180442402 104.37734948926527
31 184.05197737191338 104.62521378176316
32 177.19302238244563 105.0584911836046
33 186.22386505641043 106.15529439510486
34 184.80901057994924 106.74622734614397
35 190.69129057612736 108.56292436467679
36 188.8172115701891 108.79935704378586
37 188.1323843152495 109.54910495619963
38 192.365337039344 111.47773402735328
39 192.1146517721936 112.60050290438329
40 197.5101942422043 113.42208410212889
41 195.02300778403878 114.13587883143343
42 199.88569921720773 117.39102298242703
43 201.8342631063424 117.83642486880923
44 199.46534715828602 117.9102917094035
45 199.1262830120977 119.72511129460356
46 205.28734691394493 124.11350914107612
47 209.58077955187764 127.30803269453892
train accuracy: 0.9881512059014639
validation accuracy: 0.975177304964539
