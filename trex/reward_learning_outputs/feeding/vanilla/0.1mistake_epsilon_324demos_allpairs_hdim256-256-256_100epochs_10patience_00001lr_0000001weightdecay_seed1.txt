demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:0
end of epoch 0: val_loss 0.3255270138268973, val_acc 0.875886524822695
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.283742044938695, val_acc 0.8856382978723404
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.29396104533731005, val_acc 0.8962765957446809
trigger times: 1
end of epoch 3: val_loss 0.30249647149409115, val_acc 0.8865248226950354
trigger times: 2
end of epoch 4: val_loss 0.3348685789495006, val_acc 0.899822695035461
trigger times: 3
end of epoch 5: val_loss 0.3088662354155866, val_acc 0.8909574468085106
trigger times: 4
end of epoch 6: val_loss 0.3124065432703314, val_acc 0.8962765957446809
trigger times: 5
end of epoch 7: val_loss 0.3155310474033137, val_acc 0.8838652482269503
trigger times: 6
end of epoch 8: val_loss 0.3167215117755146, val_acc 0.8856382978723404
trigger times: 7
end of epoch 9: val_loss 0.3262037003852703, val_acc 0.8820921985815603
trigger times: 8
end of epoch 10: val_loss 0.3316907758788612, val_acc 0.8696808510638298
trigger times: 9
end of epoch 11: val_loss 0.2916622772938341, val_acc 0.8865248226950354
trigger times: 10
Early stopping.
0 -3.188424924854189 -231.98610141016846
1 -2.3197448605205864 -180.38100907229617
2 -1.656786446692422 -159.58559605250935
3 -1.805316307261819 -158.91948622182267
4 -1.2830997432611184 -144.22340051424408
5 -1.2814722463226644 -119.33378180391257
6 -1.3759283569670515 -118.93014436813327
7 -1.1457104167056968 -111.99728318755238
8 -0.37277975503820926 -102.05729730987049
9 -0.46516331244492903 -81.78273143585434
10 -0.8894777830719249 -81.15486081250215
11 -0.29479844935121946 -62.06616489925748
12 -0.43084059581451584 -59.021462360023776
13 0.08182086830493063 -57.48973202587643
14 -0.7868344795133453 -54.2864062956764
15 0.3909309085283894 -25.99016205153756
16 -0.31165607393631944 -18.600859293147124
17 0.3319013870495837 -9.810796929333884
18 0.4036436106543988 -2.58279350160679
19 0.2610237456392497 34.523887505208855
20 0.47632206264097476 38.85215532611078
21 1.1811433523253072 44.83371463291592
22 2.366470908484189 64.53271247267233
23 2.3836200947116595 78.39646093135971
24 2.0486213497060817 85.11659264541795
25 2.0801591619092505 94.60034688743727
26 1.2706579174264334 97.31185243955532
27 2.329896492694388 97.75965500483397
28 1.4339190007012803 98.47049183311258
29 1.3344814503216185 100.53051433426002
30 2.1822265231130586 102.03454348571637
31 1.5194738480640808 103.89263364013634
32 2.022755819474696 104.89456244575071
33 2.1374121227127034 105.33214185027923
34 1.8229009926944855 107.03854266018475
35 2.391221959638642 108.39772785012178
36 1.629282562607841 109.54910495619963
37 2.0276547906687483 111.56177998492792
38 3.009396766021382 111.92292565511607
39 2.54801777593093 112.02235057401518
40 2.6345874682010617 112.32257133288935
41 2.158057692045986 113.2348172145928
42 2.481814368045889 113.8274283128223
43 3.023115093150409 116.41091851337076
44 2.390134161527385 117.06664939486333
45 2.1980776514392346 117.10824747619725
46 3.5875672888760164 122.23868990266085
47 2.177327516648802 127.2098650437575
train accuracy: 0.874001452432825
validation accuracy: 0.8865248226950354
