demos: (480, 200, 40)
demo_rewards: (480,)
sorted_train_rewards: [-2.38956547e+02 -2.31986101e+02 -2.27531189e+02 -2.14417701e+02
 -2.04412238e+02 -1.99209121e+02 -1.99157848e+02 -1.94891756e+02
 -1.91993811e+02 -1.89667402e+02 -1.87815666e+02 -1.86073130e+02
 -1.81861215e+02 -1.81798993e+02 -1.81714128e+02 -1.81596217e+02
 -1.80381009e+02 -1.78402645e+02 -1.73323016e+02 -1.72463743e+02
 -1.68128667e+02 -1.67813584e+02 -1.67247127e+02 -1.65914832e+02
 -1.64521713e+02 -1.64411510e+02 -1.63627803e+02 -1.61326055e+02
 -1.59928337e+02 -1.59585596e+02 -1.58919486e+02 -1.58758769e+02
 -1.57032710e+02 -1.56132065e+02 -1.53825043e+02 -1.53478309e+02
 -1.53219510e+02 -1.52585536e+02 -1.51717515e+02 -1.51563474e+02
 -1.51099628e+02 -1.49731585e+02 -1.48410061e+02 -1.47498442e+02
 -1.47479101e+02 -1.45491353e+02 -1.44223401e+02 -1.41549469e+02
 -1.41449491e+02 -1.39971739e+02 -1.39200664e+02 -1.38662570e+02
 -1.38619242e+02 -1.35771010e+02 -1.34299833e+02 -1.33760564e+02
 -1.29471792e+02 -1.28900549e+02 -1.24729545e+02 -1.23828972e+02
 -1.23297509e+02 -1.22425079e+02 -1.22293135e+02 -1.21353619e+02
 -1.19902966e+02 -1.19333782e+02 -1.18930144e+02 -1.18608700e+02
 -1.17076491e+02 -1.14590354e+02 -1.11997283e+02 -1.11954169e+02
 -1.11824566e+02 -1.11559385e+02 -1.08009223e+02 -1.07387962e+02
 -1.06013664e+02 -1.04757058e+02 -1.02057297e+02 -1.02052564e+02
 -1.00798885e+02 -1.00690056e+02 -1.00499930e+02 -9.98699377e+01
 -9.85192976e+01 -9.84239145e+01 -9.75676332e+01 -9.61284079e+01
 -9.60834216e+01 -9.57862977e+01 -9.57279189e+01 -9.45119410e+01
 -9.43218307e+01 -9.38760127e+01 -9.20279860e+01 -9.17695941e+01
 -9.13223166e+01 -8.98853509e+01 -8.90870546e+01 -8.86673879e+01
 -8.48047471e+01 -8.42370718e+01 -8.17827314e+01 -8.11640532e+01
 -8.11548608e+01 -8.02539952e+01 -7.98622669e+01 -7.96166886e+01
 -7.86874816e+01 -7.86572287e+01 -7.69452994e+01 -7.64494019e+01
 -7.51748304e+01 -7.37636243e+01 -7.31590280e+01 -7.10981590e+01
 -6.75611411e+01 -6.46808189e+01 -6.38605228e+01 -6.34305058e+01
 -6.24011794e+01 -6.20661649e+01 -6.15705474e+01 -6.12752733e+01
 -5.90429691e+01 -5.90214624e+01 -5.74897320e+01 -5.54471988e+01
 -5.42864063e+01 -5.25176037e+01 -5.13345524e+01 -4.96750357e+01
 -4.92393664e+01 -4.76350377e+01 -4.29351800e+01 -4.20046009e+01
 -3.49359130e+01 -3.14286874e+01 -3.04571565e+01 -2.59901621e+01
 -1.86008593e+01 -1.26984695e+01 -8.34444432e+00 -2.58279350e+00
 -7.80459311e-02  1.29311387e+00  8.88422653e+00  1.74740065e+01
  2.08412708e+01  2.37677664e+01  2.47417663e+01  2.68193926e+01
  2.85903851e+01  2.87453954e+01  2.91348520e+01  3.34010391e+01
  3.46365979e+01  3.65628621e+01  3.81508759e+01  3.88521553e+01
  4.17367765e+01  4.36842612e+01  4.41689973e+01  4.48337146e+01
  4.58914054e+01  4.58975750e+01  4.66422805e+01  4.78527273e+01
  4.84703766e+01  5.06713842e+01  5.86277665e+01  5.88051215e+01
  5.91605071e+01  6.14604751e+01  6.22276965e+01  6.27420369e+01
  6.39533516e+01  6.45327125e+01  6.81032392e+01  7.46039006e+01
  7.59794571e+01  7.71874952e+01  7.72239101e+01  7.76228625e+01
  7.83964609e+01  7.84823665e+01  7.93595955e+01  8.05922425e+01
  8.08689505e+01  8.12524193e+01  8.29712340e+01  8.37567498e+01
  8.38185762e+01  8.38507065e+01  8.51165926e+01  8.59062755e+01
  8.59227730e+01  8.59404124e+01  8.61249691e+01  8.69490496e+01
  8.71665572e+01  8.73956145e+01  8.77695953e+01  8.82151098e+01
  8.83649356e+01  8.98320723e+01  9.12810349e+01  9.20639617e+01
  9.21770964e+01  9.25466602e+01  9.26993793e+01  9.29149094e+01
  9.33590509e+01  9.35141993e+01  9.37684258e+01  9.46003469e+01
  9.46676980e+01  9.48441378e+01  9.52288602e+01  9.58198472e+01
  9.59312710e+01  9.62265710e+01  9.63448823e+01  9.72275234e+01
  9.74739870e+01  9.75765713e+01  9.77596550e+01  9.80634969e+01
  9.84704918e+01  9.87491373e+01  9.89467248e+01  9.90312206e+01
  9.92689722e+01  9.93456522e+01  9.97064708e+01  9.98314870e+01
  1.00530514e+02  1.00557140e+02  1.00731914e+02  1.00759178e+02
  1.00797709e+02  1.00842491e+02  1.01607641e+02  1.01619000e+02
  1.01633103e+02  1.02034543e+02  1.02088294e+02  1.02576446e+02
  1.03123467e+02  1.03168903e+02  1.03254026e+02  1.03317283e+02
  1.03402320e+02  1.03474408e+02  1.03493344e+02  1.03507982e+02
  1.03892634e+02  1.03909734e+02  1.04347593e+02  1.04377349e+02
  1.04555011e+02  1.04617370e+02  1.04619678e+02  1.04625214e+02
  1.04778472e+02  1.04893736e+02  1.04894562e+02  1.05058491e+02
  1.05185683e+02  1.05332142e+02  1.05657163e+02  1.05707416e+02
  1.05804527e+02  1.06049557e+02  1.06155294e+02  1.06394255e+02
  1.06462069e+02  1.06493963e+02  1.06545151e+02  1.06707400e+02
  1.06746227e+02  1.07038543e+02  1.07057788e+02  1.07168291e+02
  1.07215048e+02  1.07215548e+02  1.07288168e+02  1.07302369e+02
  1.07702033e+02  1.07876702e+02  1.07961477e+02  1.08028205e+02
  1.08285889e+02  1.08288580e+02  1.08353129e+02  1.08562924e+02
  1.08799357e+02  1.09023400e+02  1.09160712e+02  1.09391313e+02
  1.09476950e+02  1.09549105e+02  1.09709114e+02  1.09777261e+02
  1.09826509e+02  1.10100394e+02  1.10197893e+02  1.10317316e+02
  1.10330104e+02  1.10395072e+02  1.10526084e+02  1.10544826e+02
  1.10592250e+02  1.10823248e+02  1.11343211e+02  1.11477734e+02
  1.11495374e+02  1.11532852e+02  1.11630388e+02  1.11899963e+02
  1.12022351e+02  1.12052377e+02  1.12193121e+02  1.12210956e+02
  1.12215355e+02  1.12322571e+02  1.12398993e+02  1.12427798e+02
  1.12600503e+02  1.12701275e+02  1.12730160e+02  1.13023001e+02
  1.13047707e+02  1.13051928e+02  1.13164217e+02  1.13187091e+02
  1.13219878e+02  1.13234817e+02  1.13299165e+02  1.13346193e+02
  1.13422084e+02  1.13434728e+02  1.13465279e+02  1.13498231e+02
  1.13546244e+02  1.13589611e+02  1.13798469e+02  1.13799071e+02
  1.13820264e+02  1.13827428e+02  1.13889635e+02  1.13915007e+02
  1.13989726e+02  1.14135879e+02  1.14142589e+02  1.14199107e+02
  1.14222532e+02  1.14377218e+02  1.14536223e+02  1.14780237e+02
  1.14805476e+02  1.15094437e+02  1.15138461e+02  1.15176503e+02
  1.15964460e+02  1.16076930e+02  1.16105903e+02  1.16127987e+02
  1.16329525e+02  1.16410919e+02  1.16479155e+02  1.16910822e+02
  1.16930294e+02  1.17066649e+02  1.17076197e+02  1.17078776e+02
  1.17094724e+02  1.17108247e+02  1.17175758e+02  1.17208684e+02
  1.17269254e+02  1.17340277e+02  1.17391023e+02  1.17607639e+02
  1.17699615e+02  1.17720032e+02  1.17740262e+02  1.17745884e+02
  1.17830903e+02  1.17836425e+02  1.17843397e+02  1.17910292e+02
  1.18131423e+02  1.18662252e+02  1.18818648e+02  1.18993695e+02
  1.19007446e+02  1.19200321e+02  1.19433028e+02  1.19485367e+02
  1.19658120e+02  1.19725111e+02  1.20150251e+02  1.20343318e+02
  1.20353140e+02  1.20367948e+02  1.20720844e+02  1.21252400e+02
  1.21582237e+02  1.21809499e+02  1.22235627e+02  1.22238690e+02
  1.22415287e+02  1.22475198e+02  1.22548449e+02  1.22630626e+02
  1.22738609e+02  1.23648271e+02  1.23743885e+02  1.24113509e+02
  1.24181296e+02  1.25198547e+02  1.25789565e+02  1.25852840e+02
  1.26469912e+02  1.26587651e+02  1.26906604e+02  1.26937192e+02
  1.26978610e+02  1.27209865e+02  1.27308033e+02  1.29055235e+02]
sorted_val_rewards: [-202.98814303 -189.30789369 -173.14447233 -148.33843128 -108.07859913
 -104.35997319  -89.11512657  -71.12319254  -70.58579775  -67.46716696
  -66.28614381  -66.19170495  -62.57075967  -53.62138787  -36.77131616
   -9.81079693    6.73145762   34.52388751   37.00896008   52.66182944
   64.71853682   74.28757855   80.75730686   84.42166728   92.20159701
   94.19977594   97.31185244   99.44048172  101.38435155  101.53789448
  104.52083972  104.76951507  105.49846404  108.39772785  108.72088283
  110.49514575  111.56177998  111.92292566  111.93947388  113.95902096
  115.10785836  116.21690546  117.22159337  119.29471869  122.26620215
  124.64400215  126.13114011  128.07431345]
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13568
Number of trainable paramters: 13568
device: cuda:2
end of epoch 0: val_loss 0.08805656808602129, val_acc 0.9618794326241135
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.04595646878715005, val_acc 0.9787234042553191
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.1429027696180355, val_acc 0.9574468085106383
trigger times: 1
end of epoch 3: val_loss 0.04483207279587175, val_acc 0.9858156028368794
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.03160958749386627, val_acc 0.9875886524822695
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.0454137322098727, val_acc 0.9804964539007093
trigger times: 1
end of epoch 6: val_loss 0.041356605049658816, val_acc 0.9849290780141844
trigger times: 2
end of epoch 7: val_loss 0.03608431280369665, val_acc 0.9858156028368794
trigger times: 3
end of epoch 8: val_loss 0.03892624253342046, val_acc 0.9867021276595744
trigger times: 4
end of epoch 9: val_loss 0.04430071269136466, val_acc 0.9893617021276596
trigger times: 5
end of epoch 10: val_loss 0.04796926805885771, val_acc 0.9840425531914894
trigger times: 6
end of epoch 11: val_loss 0.04137257623594233, val_acc 0.9858156028368794
trigger times: 7
end of epoch 12: val_loss 0.03741468275714089, val_acc 0.9893617021276596
trigger times: 8
end of epoch 13: val_loss 0.0538527469598374, val_acc 0.9822695035460993
trigger times: 9
end of epoch 14: val_loss 0.03718003731021963, val_acc 0.9849290780141844
trigger times: 10
Early stopping.
0 -195.62897258996964 -202.98814302567817
1 -185.71840396523476 -189.30789369177842
2 -164.7205557078123 -173.14447232862128
3 -140.5538586229086 -148.33843127678543
4 -79.48402867597179 -108.07859913110896
5 -90.38593367554131 -104.35997318645043
6 -66.8054785777349 -89.1151265675109
7 -49.467218410689384 -71.12319254298748
8 -55.30049845481699 -70.58579774869999
9 -45.28887744154781 -67.46716696484383
10 -46.70603927562479 -66.28614380764132
11 -39.984236401622184 -66.19170494681471
12 -35.56413268891629 -62.57075966723164
13 -31.32204837538302 -53.62138787480117
14 -23.83152880311536 -36.7713161616789
15 11.579415718559176 -9.810796929333884
16 24.897169468691573 6.731457620981826
17 54.01508706616005 34.523887505208855
18 57.93711555423215 37.008960077453615
19 78.49029774265364 52.66182943811517
20 97.92151438049041 64.71853682193498
21 98.55357214482501 74.28757855101101
22 106.68738202640088 80.75730686238391
23 114.86237079283455 84.42166727797465
24 114.66738585196435 92.20159700530384
25 115.7994445564691 94.19977594187397
26 121.13458478543907 97.31185243955532
27 121.97426803782582 99.44048171944873
28 128.58452060670243 101.3843515497597
29 127.09563306035125 101.53789447776006
30 128.35838938358938 104.52083971933268
31 131.6354671290319 104.76951506839714
32 133.03456772083882 105.49846404467546
33 136.50037153065205 108.39772785012178
34 139.7399370658677 108.72088282670852
35 140.29972523951437 110.49514574919637
36 138.73151429265272 111.56177998492792
37 138.144340375904 111.92292565511607
38 140.52185386861674 111.9394738760642
39 145.6270370950224 113.9590209581156
40 144.92040752491448 115.10785836441461
41 144.73787513328716 116.21690546133462
42 144.27302980073728 117.22159336730809
43 146.92314574070042 119.29471868535566
44 153.96978195320116 122.2662021465837
45 154.72441262053326 124.6440021529154
46 158.81336837797426 126.13114010913584
47 161.46900843805633 128.07431344844193
train accuracy: 0.9842334594656577
validation accuracy: 0.9849290780141844
