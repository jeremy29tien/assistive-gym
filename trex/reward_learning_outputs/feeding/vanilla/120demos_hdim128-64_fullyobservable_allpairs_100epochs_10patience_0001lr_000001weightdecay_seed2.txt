demos: (480, 200, 40)
demo_rewards: (480,)
sorted_train_rewards: [-2.38956547e+02 -2.31986101e+02 -2.27531189e+02 -2.14417701e+02
 -2.04412238e+02 -2.02988143e+02 -1.99209121e+02 -1.99157848e+02
 -1.94891756e+02 -1.91993811e+02 -1.89667402e+02 -1.89307894e+02
 -1.87815666e+02 -1.86073130e+02 -1.81861215e+02 -1.81798993e+02
 -1.81714128e+02 -1.80381009e+02 -1.78402645e+02 -1.73323016e+02
 -1.73144472e+02 -1.72463743e+02 -1.68128667e+02 -1.67813584e+02
 -1.67247127e+02 -1.65914832e+02 -1.64521713e+02 -1.64411510e+02
 -1.63627803e+02 -1.61326055e+02 -1.59928337e+02 -1.59585596e+02
 -1.58919486e+02 -1.58758769e+02 -1.57032710e+02 -1.56132065e+02
 -1.53825043e+02 -1.53478309e+02 -1.53219510e+02 -1.52585536e+02
 -1.51563474e+02 -1.51099628e+02 -1.49731585e+02 -1.48410061e+02
 -1.48338431e+02 -1.47498442e+02 -1.45491353e+02 -1.44223401e+02
 -1.41549469e+02 -1.41449491e+02 -1.39971739e+02 -1.39200664e+02
 -1.38662570e+02 -1.38619242e+02 -1.35771010e+02 -1.34299833e+02
 -1.29471792e+02 -1.28900549e+02 -1.24729545e+02 -1.23828972e+02
 -1.23297509e+02 -1.22425079e+02 -1.22293135e+02 -1.21353619e+02
 -1.19902966e+02 -1.19333782e+02 -1.18930144e+02 -1.18608700e+02
 -1.14590354e+02 -1.11954169e+02 -1.11824566e+02 -1.11559385e+02
 -1.08078599e+02 -1.08009223e+02 -1.07387962e+02 -1.06013664e+02
 -1.04757058e+02 -1.04359973e+02 -1.02057297e+02 -1.02052564e+02
 -1.00798885e+02 -1.00690056e+02 -1.00499930e+02 -9.98699377e+01
 -9.85192976e+01 -9.84239145e+01 -9.75676332e+01 -9.61284079e+01
 -9.57862977e+01 -9.57279189e+01 -9.45119410e+01 -9.43218307e+01
 -9.38760127e+01 -9.20279860e+01 -9.17695941e+01 -9.13223166e+01
 -8.98853509e+01 -8.91151266e+01 -8.90870546e+01 -8.86673879e+01
 -8.48047471e+01 -8.42370718e+01 -8.17827314e+01 -8.11640532e+01
 -8.11548608e+01 -8.02539952e+01 -7.98622669e+01 -7.96166886e+01
 -7.86874816e+01 -7.86572287e+01 -7.69452994e+01 -7.64494019e+01
 -7.51748304e+01 -7.37636243e+01 -7.31590280e+01 -7.11231925e+01
 -7.10981590e+01 -7.05857977e+01 -6.74671670e+01 -6.62861438e+01
 -6.61917049e+01 -6.38605228e+01 -6.34305058e+01 -6.25707597e+01
 -6.24011794e+01 -6.20661649e+01 -6.15705474e+01 -5.90429691e+01
 -5.90214624e+01 -5.74897320e+01 -5.42864063e+01 -5.36213879e+01
 -5.25176037e+01 -5.13345524e+01 -4.96750357e+01 -4.92393664e+01
 -4.20046009e+01 -3.67713162e+01 -3.14286874e+01 -3.04571565e+01
 -2.59901621e+01 -1.86008593e+01 -1.26984695e+01 -9.81079693e+00
 -8.34444432e+00 -2.58279350e+00 -7.80459311e-02  6.73145762e+00
  8.88422653e+00  1.74740065e+01  2.08412708e+01  2.37677664e+01
  2.47417663e+01  2.68193926e+01  2.85903851e+01  2.87453954e+01
  2.91348520e+01  3.34010391e+01  3.45238875e+01  3.46365979e+01
  3.65628621e+01  3.81508759e+01  3.88521553e+01  4.17367765e+01
  4.36842612e+01  4.41689973e+01  4.48337146e+01  4.58914054e+01
  4.58975750e+01  4.66422805e+01  4.78527273e+01  4.84703766e+01
  5.06713842e+01  5.26618294e+01  5.86277665e+01  5.88051215e+01
  5.91605071e+01  6.14604751e+01  6.22276965e+01  6.27420369e+01
  6.39533516e+01  6.45327125e+01  6.47185368e+01  6.81032392e+01
  7.42875786e+01  7.46039006e+01  7.59794571e+01  7.71874952e+01
  7.72239101e+01  7.83964609e+01  7.84823665e+01  8.05922425e+01
  8.07573069e+01  8.08689505e+01  8.12524193e+01  8.29712340e+01
  8.37567498e+01  8.38185762e+01  8.38507065e+01  8.44216673e+01
  8.51165926e+01  8.59062755e+01  8.59227730e+01  8.59404124e+01
  8.61249691e+01  8.69490496e+01  8.71665572e+01  8.73956145e+01
  8.77695953e+01  8.82151098e+01  8.83649356e+01  8.98320723e+01
  9.12810349e+01  9.20639617e+01  9.21770964e+01  9.22015970e+01
  9.25466602e+01  9.26993793e+01  9.29149094e+01  9.33590509e+01
  9.35141993e+01  9.37684258e+01  9.46676980e+01  9.52288602e+01
  9.58198472e+01  9.62265710e+01  9.63448823e+01  9.72275234e+01
  9.74739870e+01  9.75765713e+01  9.80634969e+01  9.84704918e+01
  9.87491373e+01  9.89467248e+01  9.90312206e+01  9.92689722e+01
  9.93456522e+01  9.94404817e+01  9.98314870e+01  1.00530514e+02
  1.00557140e+02  1.00797709e+02  1.00842491e+02  1.01384352e+02
  1.01537894e+02  1.01607641e+02  1.01619000e+02  1.01633103e+02
  1.02034543e+02  1.02088294e+02  1.02576446e+02  1.03123467e+02
  1.03168903e+02  1.03254026e+02  1.03317283e+02  1.03402320e+02
  1.03474408e+02  1.03493344e+02  1.03507982e+02  1.03892634e+02
  1.03909734e+02  1.04347593e+02  1.04377349e+02  1.04520840e+02
  1.04555011e+02  1.04617370e+02  1.04619678e+02  1.04625214e+02
  1.04769515e+02  1.04778472e+02  1.04893736e+02  1.04894562e+02
  1.05058491e+02  1.05332142e+02  1.05498464e+02  1.05657163e+02
  1.05707416e+02  1.05804527e+02  1.06049557e+02  1.06155294e+02
  1.06394255e+02  1.06462069e+02  1.06493963e+02  1.06545151e+02
  1.06707400e+02  1.06746227e+02  1.07038543e+02  1.07168291e+02
  1.07215048e+02  1.07215548e+02  1.07288168e+02  1.07302369e+02
  1.07876702e+02  1.07961477e+02  1.08028205e+02  1.08288580e+02
  1.08353129e+02  1.08397728e+02  1.08562924e+02  1.08720883e+02
  1.08799357e+02  1.09023400e+02  1.09160712e+02  1.09391313e+02
  1.09476950e+02  1.09549105e+02  1.09709114e+02  1.09777261e+02
  1.09826509e+02  1.10100394e+02  1.10197893e+02  1.10317316e+02
  1.10330104e+02  1.10395072e+02  1.10495146e+02  1.10526084e+02
  1.10544826e+02  1.10592250e+02  1.10823248e+02  1.11343211e+02
  1.11477734e+02  1.11495374e+02  1.11532852e+02  1.11630388e+02
  1.11922926e+02  1.11939474e+02  1.12022351e+02  1.12193121e+02
  1.12210956e+02  1.12215355e+02  1.12322571e+02  1.12398993e+02
  1.12427798e+02  1.12600503e+02  1.12701275e+02  1.12730160e+02
  1.13023001e+02  1.13047707e+02  1.13051928e+02  1.13164217e+02
  1.13187091e+02  1.13234817e+02  1.13299165e+02  1.13346193e+02
  1.13422084e+02  1.13434728e+02  1.13465279e+02  1.13498231e+02
  1.13589611e+02  1.13798469e+02  1.13799071e+02  1.13820264e+02
  1.13827428e+02  1.13889635e+02  1.13959021e+02  1.13989726e+02
  1.14135879e+02  1.14142589e+02  1.14199107e+02  1.14222532e+02
  1.14377218e+02  1.14536223e+02  1.14780237e+02  1.14805476e+02
  1.15094437e+02  1.15107858e+02  1.15138461e+02  1.15176503e+02
  1.15964460e+02  1.16127987e+02  1.16216905e+02  1.16329525e+02
  1.16410919e+02  1.16479155e+02  1.16910822e+02  1.16930294e+02
  1.17066649e+02  1.17076197e+02  1.17078776e+02  1.17094724e+02
  1.17108247e+02  1.17175758e+02  1.17208684e+02  1.17269254e+02
  1.17340277e+02  1.17391023e+02  1.17607639e+02  1.17699615e+02
  1.17720032e+02  1.17740262e+02  1.17745884e+02  1.17830903e+02
  1.17836425e+02  1.17843397e+02  1.17910292e+02  1.18131423e+02
  1.18662252e+02  1.18818648e+02  1.18993695e+02  1.19200321e+02
  1.19294719e+02  1.19433028e+02  1.19485367e+02  1.19725111e+02
  1.20150251e+02  1.20343318e+02  1.20353140e+02  1.20367948e+02
  1.20720844e+02  1.21252400e+02  1.21582237e+02  1.21809499e+02
  1.22235627e+02  1.22238690e+02  1.22415287e+02  1.22475198e+02
  1.22548449e+02  1.22630626e+02  1.22738609e+02  1.23648271e+02
  1.24181296e+02  1.24644002e+02  1.25789565e+02  1.25852840e+02
  1.26131140e+02  1.26469912e+02  1.26587651e+02  1.26906604e+02
  1.26937192e+02  1.26978610e+02  1.27209865e+02  1.27308033e+02]
sorted_val_rewards: [-181.5962172  -151.7175152  -147.47910081 -133.7605638  -117.07649083
 -111.99728319  -96.0834216   -67.56114111  -64.6808189   -61.27527334
  -55.44719879  -47.63503765  -42.93517996  -34.93591297    1.29311387
   37.00896008   77.62286246   79.3595955    94.19977594   94.60034689
   94.84413776   95.93127097   97.31185244   97.759655     99.70647084
  100.7319143   100.75917839  105.18568331  107.05778838  107.70203302
  108.28588916  111.56177998  111.89996323  112.0523774   113.21987801
  113.54624416  113.91500729  116.07692971  116.10590321  117.22159337
  119.00744637  119.65811962  122.26620215  123.74388483  124.11350914
  125.19854714  128.07431345  129.05523536]
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13568
Number of trainable paramters: 13568
device: cuda:0
end of epoch 0: val_loss 0.1282715637056999, val_acc 0.9459219858156028
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.10111195560309737, val_acc 0.9512411347517731
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.11899429610027061, val_acc 0.9468085106382979
trigger times: 1
end of epoch 3: val_loss 0.14711540140281254, val_acc 0.9361702127659575
trigger times: 2
end of epoch 4: val_loss 0.09909591333463802, val_acc 0.9601063829787234
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.14592313289159795, val_acc 0.950354609929078
trigger times: 1
end of epoch 6: val_loss 0.13408135589917358, val_acc 0.9485815602836879
trigger times: 2
end of epoch 7: val_loss 0.2260331462838058, val_acc 0.9432624113475178
trigger times: 3
end of epoch 8: val_loss 0.1554001735638367, val_acc 0.9476950354609929
trigger times: 4
end of epoch 9: val_loss 0.10735034292500902, val_acc 0.9556737588652482
trigger times: 5
end of epoch 10: val_loss 0.09987943204935774, val_acc 0.9636524822695035
trigger times: 6
end of epoch 11: val_loss 0.14331640972631407, val_acc 0.9565602836879432
trigger times: 7
end of epoch 12: val_loss 0.11426484597407421, val_acc 0.9654255319148937
trigger times: 8
end of epoch 13: val_loss 0.1046004868262139, val_acc 0.9698581560283688
trigger times: 9
end of epoch 14: val_loss 0.11340045520405793, val_acc 0.9609929078014184
trigger times: 10
Early stopping.
0 -141.98548993468285 -181.59621720143082
1 -94.83967566490173 -151.71751520071632
2 -118.32449892163277 -147.47910081225547
3 -82.71067778766155 -133.76056380009578
4 -74.67509593721479 -117.07649082709086
5 -80.39562380686402 -111.99728318755238
6 -69.55798130156472 -96.08342159541479
7 -53.00312527641654 -67.56114110862306
8 -43.999425074551255 -64.68081889915388
9 -42.881123173981905 -61.275273342250266
10 -43.2173662125133 -55.44719879154653
11 -37.28720746561885 -47.635037654385464
12 -37.57485708035529 -42.93517995772881
13 -36.3640804272145 -34.935912968324274
14 -18.982781902886927 1.293113865583746
15 11.232990234624594 37.008960077453615
16 32.22718349471688 77.62286246422622
17 39.3000960778445 79.3595955035673
18 39.067471135407686 94.19977594187397
19 42.346776181831956 94.60034688743727
20 40.57314318418503 94.84413775624779
21 41.58912720531225 95.93127096706885
22 42.88315138220787 97.31185243955532
23 46.95534388627857 97.75965500483397
24 42.66744799539447 99.706470840702
25 42.210305182263255 100.73191429849227
26 42.2285411413759 100.75917839256617
27 47.39370428957045 105.18568330899544
28 55.133862557355314 107.05778838409489
29 56.418076982721686 107.70203301945793
30 52.19855306856334 108.28588915868846
31 57.94097851868719 111.56177998492792
32 56.38776405248791 111.89996322956685
33 55.39570621214807 112.05237740392577
34 58.58407039567828 113.21987800793073
35 53.91600241139531 113.54624416393074
36 61.77891966077732 113.91500729297944
37 60.2002549469471 116.07692970774649
38 60.43353138491511 116.10590321293648
39 59.177323369309306 117.22159336730809
40 58.37544939853251 119.00744636937388
41 64.72246265038848 119.6581196203716
42 62.199113043025136 122.2662021465837
43 63.74644367676228 123.74388483204041
44 64.04328206460923 124.11350914107612
45 64.05622326547746 125.19854714485804
46 66.83756600227207 128.07431344844193
47 68.60061787161976 129.05523536383924
train accuracy: 0.9838935574229691
validation accuracy: 0.9609929078014184
