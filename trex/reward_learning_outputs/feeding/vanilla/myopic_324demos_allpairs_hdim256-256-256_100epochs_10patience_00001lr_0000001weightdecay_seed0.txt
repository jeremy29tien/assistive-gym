demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:0
end of epoch 0: val_loss 0.38279132604405985, val_acc 0.8546099290780141
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.566043719264998, val_acc 0.8439716312056738
trigger times: 1
end of epoch 2: val_loss 0.5940075090618249, val_acc 0.8634751773049646
trigger times: 2
end of epoch 3: val_loss 0.5995862381382673, val_acc 0.8599290780141844
trigger times: 3
end of epoch 4: val_loss 0.6244680861788076, val_acc 0.8599290780141844
trigger times: 4
end of epoch 5: val_loss 0.5850582417805453, val_acc 0.8670212765957447
trigger times: 5
end of epoch 6: val_loss 0.6675485492820027, val_acc 0.8696808510638298
trigger times: 6
end of epoch 7: val_loss 0.7182822123774666, val_acc 0.875
trigger times: 7
end of epoch 8: val_loss 0.7507042305539146, val_acc 0.8599290780141844
trigger times: 8
end of epoch 9: val_loss 0.8602788265093718, val_acc 0.8625886524822695
trigger times: 9
end of epoch 10: val_loss 0.8289713452220124, val_acc 0.8599290780141844
trigger times: 10
Early stopping.
0 -32.73650899488712 -83.10503687115533
1 -36.722603189758956 -83.07289674094267
2 -30.385616260231473 -76.4856324830352
3 -21.023206347308587 -72.61089590850949
4 -9.711138619517442 -62.94678160926442
5 -10.970528929610737 -61.599632106712804
6 -15.316105420555687 -53.43084916795998
7 -10.412444254092406 -52.48336322615227
8 -14.98278726101853 -50.110592720448466
9 -2.690310066798702 -44.915106835748865
10 -12.320472095452715 -41.19320493975289
11 1.71177521685604 -36.129891479025325
12 -0.35566208462114446 -31.43929204678134
13 3.0296709935646504 -30.49237209332938
14 10.86412052961532 -26.804049524146976
15 7.059080127422931 -22.61035493165442
16 19.810104739270173 -19.989535946621125
17 5.2123044595937245 -16.263221507933835
18 21.097063495661132 -1.4683078297939751
19 15.48941410909174 0.8708878797099722
20 7.515385558246635 1.1295272782332058
21 24.817765345214866 3.104591746256169
22 35.75675766612403 7.259936024263311
23 34.25523174193222 8.251987606811664
24 27.78325145493727 8.40272097310634
25 28.13830063707428 9.404507288186723
26 29.94713192843483 11.867062786240995
27 26.150735730305314 11.946020959962041
28 37.99014929909026 12.440748958805557
29 39.19976300999406 12.534743823564117
30 35.54307930218056 12.980226119341165
31 34.67964037851198 13.206806044306314
32 31.45979842628003 13.774801723886249
33 40.21110146254068 14.70014395722675
34 31.519986650615465 15.044128982488495
35 32.300940524903126 15.096721219707026
36 31.1149980808259 15.153795620584578
37 30.12236822998966 15.478119681443221
38 33.73724386934191 15.530967597808012
39 34.56699500209652 15.679318927234501
40 23.576705329469405 26.66102142908317
41 35.98601456085453 26.775312268993602
42 44.6257501860382 29.95144625325653
43 35.74292202235665 34.522162694388285
44 46.56204492773395 39.982802060548565
45 36.3006545608514 43.84857194375945
46 24.643271558656124 48.10259907398566
47 37.71007346379338 61.042600141007
train accuracy: 0.9800481596147231
validation accuracy: 0.8599290780141844
