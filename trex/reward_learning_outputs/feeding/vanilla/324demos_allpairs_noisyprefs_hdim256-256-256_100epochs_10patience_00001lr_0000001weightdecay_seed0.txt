demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:0
end of epoch 0: val_loss 0.04178933021042605, val_acc 0.9822695035460993
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.048526040618110215, val_acc 0.9849290780141844
trigger times: 1
end of epoch 2: val_loss 0.04415750073981585, val_acc 0.9813829787234043
trigger times: 2
end of epoch 3: val_loss 0.040490750802549894, val_acc 0.9822695035460993
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.03278363581024623, val_acc 0.9867021276595744
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.025005641046907636, val_acc 0.9911347517730497
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.02804141749677875, val_acc 0.9884751773049646
trigger times: 1
end of epoch 7: val_loss 0.05185092774288486, val_acc 0.9831560283687943
trigger times: 2
end of epoch 8: val_loss 0.03169928171294692, val_acc 0.9875886524822695
trigger times: 3
end of epoch 9: val_loss 0.03906571709605379, val_acc 0.9831560283687943
trigger times: 4
end of epoch 10: val_loss 0.024270546186483628, val_acc 0.9867021276595744
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.031083053437161567, val_acc 0.9884751773049646
trigger times: 1
end of epoch 12: val_loss 0.037925944663627434, val_acc 0.9840425531914894
trigger times: 2
end of epoch 13: val_loss 0.025409164374737954, val_acc 0.9929078014184397
trigger times: 3
end of epoch 14: val_loss 0.038490742864729546, val_acc 0.9867021276595744
trigger times: 4
end of epoch 15: val_loss 0.027026015842728058, val_acc 0.9867021276595744
trigger times: 5
end of epoch 16: val_loss 0.05455204431616673, val_acc 0.9822695035460993
trigger times: 6
end of epoch 17: val_loss 0.02357194197341679, val_acc 0.9884751773049646
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.019855047023111343, val_acc 0.9946808510638298
trigger times: 0
saving model weights...
end of epoch 19: val_loss 0.04551511970213185, val_acc 0.9875886524822695
trigger times: 1
end of epoch 20: val_loss 0.03396779033737809, val_acc 0.9840425531914894
trigger times: 2
end of epoch 21: val_loss 0.029940982979134893, val_acc 0.9893617021276596
trigger times: 3
end of epoch 22: val_loss 0.030174436884091077, val_acc 0.9875886524822695
trigger times: 4
end of epoch 23: val_loss 0.02555208037890681, val_acc 0.9902482269503546
trigger times: 5
end of epoch 24: val_loss 0.022557634290504187, val_acc 0.9920212765957447
trigger times: 6
end of epoch 25: val_loss 0.0248391527872947, val_acc 0.9893617021276596
trigger times: 7
end of epoch 26: val_loss 0.02116768421514086, val_acc 0.9911347517730497
trigger times: 8
end of epoch 27: val_loss 0.02290255331759725, val_acc 0.9911347517730497
trigger times: 9
end of epoch 28: val_loss 0.022824553883662554, val_acc 0.9884751773049646
trigger times: 10
Early stopping.
0 -154.1990379691124 -199.20912126544414
1 -130.88695645332336 -181.59621720143082
2 -114.45757104456425 -164.41151030608535
3 -110.9772634729743 -161.32605513368566
4 -97.20632977038622 -151.5634741258087
5 -94.061139754951 -133.76056380009578
6 -82.12161131203175 -128.90054905506827
7 -78.1762833953835 -119.90296557087576
8 -78.28209856897593 -119.33378180391257
9 -74.3271480849944 -114.59035433631308
10 -65.97330518439412 -108.00922275200763
11 -47.34747698751744 -89.08705457223661
12 -45.57251147925854 -80.25399517559948
13 -34.795206337206764 -75.17483043591994
14 -35.36334274057299 -73.76362433676923
15 -33.38926783017814 -66.19170494681471
16 -28.349703240179224 -62.40117935748126
17 -19.54627033986617 -25.99016205153756
18 12.303297149715945 -0.07804593106747401
19 47.00068778696004 38.15087589300597
20 47.55752443196252 43.68426122331964
21 58.5388897935336 63.953351629706646
22 66.08035067316086 64.53271247267233
23 69.67313596603344 74.60390063743114
24 73.64586370048346 81.25241929740568
25 78.18830125383101 85.92277304180317
26 82.51100791268982 92.69937930010626
27 86.05094522303261 98.94672478897846
28 87.42215670726728 99.706470840702
29 91.33765246486291 103.1234668642953
30 94.19477193971397 105.49846404467546
31 90.66056614380796 106.70740032169537
32 94.06678224448115 110.39507157450213
33 97.88111023412785 110.82324768128066
34 96.04191124346107 111.56177998492792
35 97.93300965195522 112.21095617362457
36 99.8031502221711 112.73015958534099
37 100.82432116405107 113.05192791883351
38 103.50186021672562 117.07619652114646
39 103.24186347304203 117.34027665325667
40 102.27397947059944 117.83642486880923
41 104.94543746375712 119.20032127761252
42 107.63764359662309 119.43302815589446
43 105.28564283778542 120.15025122129617
44 106.88480714161415 122.63062574324714
45 113.25448559017968 125.78956526488335
46 110.91180492728017 125.85283998079782
47 113.08526607585372 126.58765053715479
train accuracy: 0.9854756717501816
validation accuracy: 0.9884751773049646
