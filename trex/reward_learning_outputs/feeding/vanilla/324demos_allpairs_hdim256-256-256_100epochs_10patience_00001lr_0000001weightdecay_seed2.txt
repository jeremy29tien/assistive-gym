demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:0
end of epoch 0: val_loss 0.07620636932235138, val_acc 0.9698581560283688
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.08122941750172268, val_acc 0.9680851063829787
trigger times: 1
end of epoch 2: val_loss 0.10717415186233796, val_acc 0.9654255319148937
trigger times: 2
end of epoch 3: val_loss 0.09143993817799176, val_acc 0.9680851063829787
trigger times: 3
end of epoch 4: val_loss 0.10609130860867257, val_acc 0.9680851063829787
trigger times: 4
end of epoch 5: val_loss 0.06909548651838796, val_acc 0.9716312056737588
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.08430656506454777, val_acc 0.9716312056737588
trigger times: 1
end of epoch 7: val_loss 0.09054150881548327, val_acc 0.9707446808510638
trigger times: 2
end of epoch 8: val_loss 0.06974613730695196, val_acc 0.9716312056737588
trigger times: 3
end of epoch 9: val_loss 0.11619127155240247, val_acc 0.9671985815602837
trigger times: 4
end of epoch 10: val_loss 0.09665072057489743, val_acc 0.9725177304964538
trigger times: 5
end of epoch 11: val_loss 0.10794524614169554, val_acc 0.9671985815602837
trigger times: 6
end of epoch 12: val_loss 0.14710766719280946, val_acc 0.9636524822695035
trigger times: 7
end of epoch 13: val_loss 0.15745205578430993, val_acc 0.9698581560283688
trigger times: 8
end of epoch 14: val_loss 0.13291805213136354, val_acc 0.9671985815602837
trigger times: 9
end of epoch 15: val_loss 0.09715463910023196, val_acc 0.9716312056737588
trigger times: 10
Early stopping.
0 -183.1481989622116 -181.59621720143082
1 -160.77140602469444 -151.71751520071632
2 -157.65539646148682 -147.47910081225547
3 -139.6997789889574 -133.76056380009578
4 -117.35782302357256 -117.07649082709086
5 -116.27096074819565 -111.99728318755238
6 -95.13017241749913 -96.08342159541479
7 -72.16580567404162 -67.56114110862306
8 -65.52614817733411 -64.68081889915388
9 -59.94271911215037 -61.275273342250266
10 -56.47103363764472 -55.44719879154653
11 -49.31790947262198 -47.635037654385464
12 -39.54773825407028 -42.93517995772881
13 -37.670131933584344 -34.935912968324274
14 4.50258408440277 1.293113865583746
15 47.360738727264106 37.008960077453615
16 88.94067146023735 77.62286246422622
17 93.01062448415905 79.3595955035673
18 106.1966592570534 94.19977594187397
19 114.70464753173292 94.60034688743727
20 105.31442514574155 94.84413775624779
21 105.3092472718563 95.93127096706885
22 106.38577083550626 97.31185243955532
23 111.46192111616256 97.75965500483397
24 111.22167966817506 99.706470840702
25 110.7073210834642 100.73191429849227
26 110.08013629727066 100.75917839256617
27 120.59715474606492 105.18568330899544
28 125.3986138345208 107.05778838409489
29 125.37970813474385 107.70203301945793
30 126.77570323855616 108.28588915868846
31 126.7892333730124 111.56177998492792
32 130.79535137454513 111.89996322956685
33 128.91490656882524 112.05237740392577
34 133.10339378216304 113.21987800793073
35 124.26268678763881 113.54624416393074
36 137.93250976505806 113.91500729297944
37 133.65958599676378 116.07692970774649
38 138.5226011457271 116.10590321293648
39 133.90292093728203 117.22159336730809
40 137.3034713094239 119.00744636937388
41 139.13310359371826 119.6581196203716
42 140.7422068360611 122.2662021465837
43 140.54399574373383 123.74388483204041
44 146.53054922167212 124.11350914107612
45 145.27405079454184 125.19854714485804
46 149.15572513826191 128.07431344844193
47 149.72520949785394 129.05523536383924
train accuracy: 0.9898329702251271
validation accuracy: 0.9716312056737588
