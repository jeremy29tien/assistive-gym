demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 46655
num train_labels 46655
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:1
end of epoch 0: val_loss 0.06718322333967384, val_acc 0.9778368794326241
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.0813658323027351, val_acc 0.9796099290780141
trigger times: 1
end of epoch 2: val_loss 0.11996044202575791, val_acc 0.976063829787234
trigger times: 2
end of epoch 3: val_loss 0.12420719570469295, val_acc 0.9778368794326241
trigger times: 3
end of epoch 4: val_loss 0.13387838905451327, val_acc 0.9787234042553191
trigger times: 4
end of epoch 5: val_loss 0.14034113294166609, val_acc 0.9769503546099291
trigger times: 5
end of epoch 6: val_loss 0.1264667947787195, val_acc 0.9787234042553191
trigger times: 6
end of epoch 7: val_loss 0.1705614848605182, val_acc 0.9769503546099291
trigger times: 7
end of epoch 8: val_loss 0.17768596366528272, val_acc 0.9787234042553191
trigger times: 8
end of epoch 9: val_loss 0.18009787454231918, val_acc 0.9796099290780141
trigger times: 9
end of epoch 10: val_loss 0.17148011909334093, val_acc 0.9787234042553191
trigger times: 10
Early stopping.
0 -254.54027092456818 -199.20912126544414
1 -227.46236819028854 -181.59621720143082
2 -190.88398388028145 -164.41151030608535
3 -195.32174830138683 -161.32605513368566
4 -174.5293666422367 -151.5634741258087
5 -132.5332423481159 -133.76056380009578
6 -176.27732238173485 -128.90054905506827
7 -148.16822202317417 -119.90296557087576
8 -152.74104797840118 -119.33378180391257
9 -139.78413377841935 -114.59035433631308
10 -139.45620621740818 -108.00922275200763
11 -101.25321967364289 -89.08705457223661
12 -97.0478474535048 -80.25399517559948
13 -85.73178400337929 -75.17483043591994
14 -83.71447408106178 -73.76362433676923
15 -104.51380139933462 -66.19170494681471
16 -69.65040043837507 -62.40117935748126
17 -55.532022047264036 -25.99016205153756
18 -5.400203187484294 -0.07804593106747401
19 53.839362083468586 38.15087589300597
20 53.04369354667142 43.68426122331964
21 73.24516552995192 63.953351629706646
22 81.3663730507833 64.53271247267233
23 86.19650622084737 74.60390063743114
24 99.3971883545164 81.25241929740568
25 104.61236172553618 85.92277304180317
26 111.98943240946392 92.69937930010626
27 116.85352686217811 98.94672478897846
28 120.329826007277 99.706470840702
29 129.14575950970175 103.1234668642953
30 131.46919732922106 105.49846404467546
31 123.99871345248539 106.70740032169537
32 134.73610158654628 110.39507157450213
33 137.95682167410268 110.82324768128066
34 138.75125152629334 111.56177998492792
35 138.28983130841516 112.21095617362457
36 140.11785611318192 112.73015958534099
37 139.5795172530925 113.05192791883351
38 146.26687462526024 117.07619652114646
39 146.53404697414953 117.34027665325667
40 144.13909893737582 117.83642486880923
41 150.0480615002307 119.20032127761252
42 153.58684599965636 119.43302815589446
43 149.7317048163677 120.15025122129617
44 151.67671115942358 122.63062574324714
45 161.31217633516644 125.78956526488335
46 156.37368278746726 125.85283998079782
47 159.7010014404368 126.58765053715479
train accuracy: 0.9929268031293538
validation accuracy: 0.9787234042553191
