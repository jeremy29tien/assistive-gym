demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:1
end of epoch 0: val_loss 0.49311613691945744, val_acc 0.8147163120567376
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.5315036794283492, val_acc 0.8439716312056738
trigger times: 1
end of epoch 2: val_loss 0.7091971398023676, val_acc 0.8430851063829787
trigger times: 2
end of epoch 3: val_loss 0.7838384119833682, val_acc 0.8421985815602837
trigger times: 3
end of epoch 4: val_loss 0.791537000652166, val_acc 0.8413120567375887
trigger times: 4
end of epoch 5: val_loss 0.9255209750449906, val_acc 0.8368794326241135
trigger times: 5
end of epoch 6: val_loss 0.9905059679315568, val_acc 0.8421985815602837
trigger times: 6
end of epoch 7: val_loss 0.9173761890395395, val_acc 0.8342198581560284
trigger times: 7
end of epoch 8: val_loss 0.818853944305217, val_acc 0.851063829787234
trigger times: 8
end of epoch 9: val_loss 1.0080081546600606, val_acc 0.8333333333333334
trigger times: 9
end of epoch 10: val_loss 1.2397772848214463, val_acc 0.8324468085106383
trigger times: 10
Early stopping.
0 -69.75517371669412 -83.07289674094267
1 -46.36595390131697 -70.65356329387014
2 -49.582595617626794 -67.71626288370264
3 -28.369096651440486 -66.34992305980889
4 -37.02254156023264 -62.94678160926442
5 -25.86969287926331 -39.119770934487114
6 -20.310975934378803 -30.79171473562066
7 -14.993801997625269 -28.993461669518965
8 -12.761897712596692 -26.382994978384094
9 -10.712290692608804 -26.229224848884144
10 -4.093659340869635 -22.149301711698293
11 -19.095406642532907 -20.855418275184427
12 -13.460014617245179 -19.74491466779722
13 -11.484184974397067 -17.959759559711195
14 -4.638997609610669 -3.1401895771432784
15 -1.2022837114636786 -0.8730694103789531
16 4.700159050466027 3.104591746256169
17 4.213603905169293 3.186351055822506
18 7.156466243555769 3.6327051713012217
19 4.018887607846409 3.956864279956715
20 15.757076799636707 9.29804753523344
21 18.527441853890195 10.050903823508037
22 10.472575476160273 10.710369268899473
23 8.01839349817601 11.21322308560861
24 20.14539572549984 13.375016089392837
25 10.300556887174025 14.330527816265443
26 26.717652084073052 14.397878282737931
27 21.864116582786664 15.531321967262155
28 20.890918605262414 18.047114564920395
29 19.681432247394696 18.15569731861594
30 17.529211829591077 19.758123164052364
31 19.107009385479614 20.393798918513156
32 31.077375178516377 20.514433356441174
33 25.61521551187616 22.498403208490302
34 22.193103599362075 23.262522776487653
35 23.984376114211045 23.406448136793067
36 35.503623471886385 23.52325790405773
37 27.682664915104397 23.630336232363803
38 8.724616438383237 25.035160824597252
39 15.626888473168947 27.069822422935054
40 22.439641938835848 33.07984120525562
41 22.592171720287297 33.820949362017764
42 18.536323858890682 34.522162694388285
43 18.34541164827533 34.61079961480401
44 33.88092639762908 41.46966173687643
45 18.750332956784405 41.937045691049875
46 7.415372930467129 45.291354923429104
47 16.548938562511466 53.92406801758195
train accuracy: 0.9788632802048695
validation accuracy: 0.8324468085106383
