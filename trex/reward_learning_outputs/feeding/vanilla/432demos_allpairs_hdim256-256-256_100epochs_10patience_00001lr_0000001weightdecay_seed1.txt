demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 93096
num train_labels 93096
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:2
end of epoch 0: val_loss 0.07707241245208545, val_acc 0.9671985815602837
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.05412110496739038, val_acc 0.9796099290780141
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.05986100008549372, val_acc 0.974290780141844
trigger times: 1
end of epoch 3: val_loss 0.057676899866945235, val_acc 0.9804964539007093
trigger times: 2
end of epoch 4: val_loss 0.041874750153877364, val_acc 0.9849290780141844
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.06250876642185162, val_acc 0.9796099290780141
trigger times: 1
end of epoch 6: val_loss 0.08018459491706408, val_acc 0.9778368794326241
trigger times: 2
end of epoch 7: val_loss 0.053315046480120816, val_acc 0.9822695035460993
trigger times: 3
end of epoch 8: val_loss 0.07769866532732699, val_acc 0.975177304964539
trigger times: 4
end of epoch 9: val_loss 0.05191950330968565, val_acc 0.9804964539007093
trigger times: 5
end of epoch 10: val_loss 0.052244522375768064, val_acc 0.9813829787234043
trigger times: 6
end of epoch 11: val_loss 0.04248019763532169, val_acc 0.9849290780141844
trigger times: 7
end of epoch 12: val_loss 0.061880648440431486, val_acc 0.9804964539007093
trigger times: 8
end of epoch 13: val_loss 0.11172603106125402, val_acc 0.974290780141844
trigger times: 9
end of epoch 14: val_loss 0.04587193460648377, val_acc 0.9875886524822695
trigger times: 10
Early stopping.
0 -270.7360407114029 -231.98610141016846
1 -190.00534945726395 -180.38100907229617
2 -174.20845964550972 -159.58559605250935
3 -178.91338196396828 -158.91948622182267
4 -144.3822459280491 -144.22340051424408
5 -119.03586284071207 -119.33378180391257
6 -113.16318747028708 -118.93014436813327
7 -108.91483234614134 -111.99728318755238
8 -71.06690028018784 -102.05729730987049
9 -74.57705491408706 -81.78273143585434
10 -69.11195743229473 -81.15486081250215
11 -41.424436820321716 -62.06616489925748
12 -34.79649571864866 -59.021462360023776
13 -33.30493658350315 -57.48973202587643
14 -34.914413106045686 -54.2864062956764
15 -20.615382038871758 -25.99016205153756
16 10.002322427346371 -18.600859293147124
17 24.581712891696952 -9.810796929333884
18 33.145718268235214 -2.58279350160679
19 76.19486550986767 34.523887505208855
20 87.94581551523879 38.85215532611078
21 88.44767768774182 44.83371463291592
22 124.18194656493142 64.53271247267233
23 130.0279796179384 78.39646093135971
24 148.94989782897756 85.11659264541795
25 159.51395309274085 94.60034688743727
26 155.93662881548516 97.31185243955532
27 160.28236655553337 97.75965500483397
28 154.92117179511115 98.47049183311258
29 163.83914004964754 100.53051433426002
30 164.65410117531428 102.03454348571637
31 166.00359717709944 103.89263364013634
32 168.02770702377893 104.89456244575071
33 171.54686951998156 105.33214185027923
34 165.4874736852944 107.03854266018475
35 176.72629820695147 108.39772785012178
36 171.85194087633863 109.54910495619963
37 176.9018405668321 111.56177998492792
38 177.9124011185486 111.92292565511607
39 176.99087885278277 112.02235057401518
40 177.15434691903647 112.32257133288935
41 180.17395155096892 113.2348172145928
42 180.80187338939868 113.8274283128223
43 185.22892946522916 116.41091851337076
44 186.82815212651622 117.06664939486333
45 187.33376854413655 117.10824747619725
46 192.7981417994015 122.23868990266085
47 199.19778127083555 127.2098650437575
train accuracy: 0.989505456732835
validation accuracy: 0.9875886524822695
