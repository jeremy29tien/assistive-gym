demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 46331
num train_labels 46331
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:0
end of epoch 0: val_loss 0.08547255093766863, val_acc 0.9671985815602837
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.07408699982330356, val_acc 0.9671985815602837
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.06721538627369589, val_acc 0.9716312056737588
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.1400712968217843, val_acc 0.9574468085106383
trigger times: 1
end of epoch 4: val_loss 0.0833881873436759, val_acc 0.9725177304964538
trigger times: 2
end of epoch 5: val_loss 0.1013487400914669, val_acc 0.9725177304964538
trigger times: 3
end of epoch 6: val_loss 0.11351343737997424, val_acc 0.9680851063829787
trigger times: 4
end of epoch 7: val_loss 0.09333121578615715, val_acc 0.9707446808510638
trigger times: 5
end of epoch 8: val_loss 0.11848030640026443, val_acc 0.9716312056737588
trigger times: 6
end of epoch 9: val_loss 0.09648272107283604, val_acc 0.974290780141844
trigger times: 7
end of epoch 10: val_loss 0.06335045332156018, val_acc 0.9813829787234043
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.08467009815964878, val_acc 0.974290780141844
trigger times: 1
end of epoch 12: val_loss 0.1225047531172933, val_acc 0.9689716312056738
trigger times: 2
end of epoch 13: val_loss 0.07749310340735868, val_acc 0.9796099290780141
trigger times: 3
end of epoch 14: val_loss 0.07998556806827783, val_acc 0.9804964539007093
trigger times: 4
end of epoch 15: val_loss 0.14838964924622128, val_acc 0.9707446808510638
trigger times: 5
end of epoch 16: val_loss 0.07822993237424657, val_acc 0.9822695035460993
trigger times: 6
end of epoch 17: val_loss 0.14204427101134492, val_acc 0.9725177304964538
trigger times: 7
end of epoch 18: val_loss 0.11383803486451051, val_acc 0.975177304964539
trigger times: 8
end of epoch 19: val_loss 0.09472181172117189, val_acc 0.976063829787234
trigger times: 9
end of epoch 20: val_loss 0.10728021288254987, val_acc 0.976063829787234
trigger times: 10
Early stopping.
0 -298.2703883051872 -181.59621720143082
1 -211.50893884897232 -151.71751520071632
2 -233.71086418628693 -147.47910081225547
3 -181.23809982836246 -133.76056380009578
4 -183.21025946736336 -117.07649082709086
5 -171.52577209472656 -111.99728318755238
6 -154.77765457704663 -96.08342159541479
7 -119.83142741024494 -67.56114110862306
8 -109.62205832079053 -64.68081889915388
9 -101.71162623912096 -61.275273342250266
10 -99.39286886900663 -55.44719879154653
11 -84.17160854488611 -47.635037654385464
12 -71.87918642163277 -42.93517995772881
13 -69.97173274308443 -34.935912968324274
14 -12.626267423853278 1.293113865583746
15 45.98358865082264 37.008960077453615
16 120.68489803373814 77.62286246422622
17 121.85943546891212 79.3595955035673
18 143.30771102011204 94.19977594187397
19 152.5523514598608 94.60034688743727
20 147.3395412862301 94.84413775624779
21 145.30006429553032 95.93127096706885
22 145.5649419873953 97.31185243955532
23 150.72623421251774 97.75965500483397
24 152.10711211711168 99.706470840702
25 150.94884504377842 100.73191429849227
26 154.34635590016842 100.75917839256617
27 162.45349579583853 105.18568330899544
28 170.13606258505024 107.05778838409489
29 168.89341633021832 107.70203301945793
30 170.72993756085634 108.28588915868846
31 172.08113057911396 111.56177998492792
32 176.06283842446283 111.89996322956685
33 175.96846633404493 112.05237740392577
34 176.9432441815734 113.21987800793073
35 169.8496792241931 113.54624416393074
36 187.22249246388674 113.91500729297944
37 182.28466876596212 116.07692970774649
38 190.28111484274268 116.10590321293648
39 180.96541050449014 117.22159336730809
40 183.56889574974775 119.00744636937388
41 189.98499139025807 119.6581196203716
42 190.59321148693562 122.2662021465837
43 193.46695160120726 123.74388483204041
44 196.7954773157835 124.11350914107612
45 196.32008039951324 125.19854714485804
46 203.9764019921422 128.07431344844193
47 204.8243871703744 129.05523536383924
train accuracy: 0.9944313742418683
validation accuracy: 0.976063829787234
