demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:2
end of epoch 0: val_loss 0.2584442778322053, val_acc 0.9406028368794326
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.3426168757401645, val_acc 0.9086879432624113
trigger times: 1
end of epoch 2: val_loss 0.2779947608735756, val_acc 0.9264184397163121
trigger times: 2
end of epoch 3: val_loss 0.2204749048838197, val_acc 0.9326241134751773
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.29378492871286177, val_acc 0.9175531914893617
trigger times: 1
end of epoch 5: val_loss 0.2693504561985519, val_acc 0.9148936170212766
trigger times: 2
end of epoch 6: val_loss 0.2630964013818463, val_acc 0.9228723404255319
trigger times: 3
end of epoch 7: val_loss 0.29570395626699064, val_acc 0.9060283687943262
trigger times: 4
end of epoch 8: val_loss 0.254189660789194, val_acc 0.9210992907801419
trigger times: 5
end of epoch 9: val_loss 0.2589822495544035, val_acc 0.9308510638297872
trigger times: 6
end of epoch 10: val_loss 0.2654894032651403, val_acc 0.9131205673758865
trigger times: 7
end of epoch 11: val_loss 0.24803505613663027, val_acc 0.9184397163120568
trigger times: 8
end of epoch 12: val_loss 0.26760820131038865, val_acc 0.9131205673758865
trigger times: 9
end of epoch 13: val_loss 0.26730256839382877, val_acc 0.9193262411347518
trigger times: 10
Early stopping.
0 -1.6003811256232439 -199.20912126544414
1 -1.5578740286728134 -181.59621720143082
2 -1.0958021866317722 -164.41151030608535
3 -1.2067734001029748 -161.32605513368566
4 -1.243757454722072 -151.5634741258087
5 -0.8851979044848122 -133.76056380009578
6 -1.1757544609536126 -128.90054905506827
7 -1.2275836719054496 -119.90296557087576
8 -0.8233685639861505 -119.33378180391257
9 -0.6202120067318901 -114.59035433631308
10 -0.6040659007121576 -108.00922275200763
11 -0.5620824714715127 -89.08705457223661
12 -0.4496187525219284 -80.25399517559948
13 -0.3108506998469238 -75.17483043591994
14 -0.2968658292084001 -73.76362433676923
15 -0.028664447647315683 -66.19170494681471
16 0.10471574720577337 -62.40117935748126
17 0.21881748012674507 -25.99016205153756
18 -0.01450684083829401 -0.07804593106747401
19 0.9359832419941085 38.15087589300597
20 1.972394334472483 43.68426122331964
21 0.7850717713372433 63.953351629706646
22 2.740707480304991 64.53271247267233
23 1.7670471929159248 74.60390063743114
24 1.06273850332218 81.25241929740568
25 1.8052739779013791 85.92277304180317
26 1.3126221063139383 92.69937930010626
27 1.6567475413321517 98.94672478897846
28 1.705097399848455 99.706470840702
29 1.6223949283084949 103.1234668642953
30 2.1510138030789676 105.49846404467546
31 1.428344866646512 106.70740032169537
32 2.146083489933517 110.39507157450213
33 2.330879064720648 110.82324768128066
34 1.5550248057115823 111.56177998492792
35 2.7832127502697404 112.21095617362457
36 2.619689192484657 112.73015958534099
37 2.835623652637878 113.05192791883351
38 3.257933095250337 117.07619652114646
39 3.0250349057168933 117.34027665325667
40 2.487237103025109 117.83642486880923
41 2.423302842922567 119.20032127761252
42 2.7379373124131234 119.43302815589446
43 2.948590849788161 120.15025122129617
44 2.6303103115060367 122.63062574324714
45 3.420488924413803 125.78956526488335
46 3.7548233017732855 125.85283998079782
47 3.6744294682721375 126.58765053715479
train accuracy: 0.8784734166571112
validation accuracy: 0.9193262411347518
