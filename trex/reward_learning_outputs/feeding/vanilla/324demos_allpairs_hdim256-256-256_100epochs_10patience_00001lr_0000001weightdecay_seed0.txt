demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:2
end of epoch 0: val_loss 0.05921671746347605, val_acc 0.976063829787234
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.03808429978892719, val_acc 0.9804964539007093
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.0502744354478538, val_acc 0.9787234042553191
trigger times: 1
end of epoch 3: val_loss 0.04198316561669958, val_acc 0.9840425531914894
trigger times: 2
end of epoch 4: val_loss 0.03685839576731978, val_acc 0.9867021276595744
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.03813469641526033, val_acc 0.9849290780141844
trigger times: 1
end of epoch 6: val_loss 0.04799055252355331, val_acc 0.9831560283687943
trigger times: 2
end of epoch 7: val_loss 0.03710473056499446, val_acc 0.9867021276595744
trigger times: 3
end of epoch 8: val_loss 0.046561052096090384, val_acc 0.9840425531914894
trigger times: 4
end of epoch 9: val_loss 0.046738317915174285, val_acc 0.9840425531914894
trigger times: 5
end of epoch 10: val_loss 0.0382396598431219, val_acc 0.9858156028368794
trigger times: 6
end of epoch 11: val_loss 0.0494001346676589, val_acc 0.9840425531914894
trigger times: 7
end of epoch 12: val_loss 0.0366599126123852, val_acc 0.9875886524822695
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.042116156146315614, val_acc 0.9875886524822695
trigger times: 1
end of epoch 14: val_loss 0.057785261398359, val_acc 0.9849290780141844
trigger times: 2
end of epoch 15: val_loss 0.037586353900153885, val_acc 0.9893617021276596
trigger times: 3
end of epoch 16: val_loss 0.04035562546040007, val_acc 0.9867021276595744
trigger times: 4
end of epoch 17: val_loss 0.05111378823307369, val_acc 0.9867021276595744
trigger times: 5
end of epoch 18: val_loss 0.05863554438539332, val_acc 0.9867021276595744
trigger times: 6
end of epoch 19: val_loss 0.037784369878560674, val_acc 0.9875886524822695
trigger times: 7
end of epoch 20: val_loss 0.041097034807130534, val_acc 0.9858156028368794
trigger times: 8
end of epoch 21: val_loss 0.03524341766907691, val_acc 0.9893617021276596
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.04963968433024467, val_acc 0.9875886524822695
trigger times: 1
end of epoch 23: val_loss 0.05116799777766976, val_acc 0.9867021276595744
trigger times: 2
end of epoch 24: val_loss 0.06159931821543746, val_acc 0.9849290780141844
trigger times: 3
end of epoch 25: val_loss 0.0488228468551673, val_acc 0.9858156028368794
trigger times: 4
end of epoch 26: val_loss 0.04694946566023628, val_acc 0.9867021276595744
trigger times: 5
end of epoch 27: val_loss 0.042480437255795644, val_acc 0.9867021276595744
trigger times: 6
end of epoch 28: val_loss 0.04815528857229856, val_acc 0.9884751773049646
trigger times: 7
end of epoch 29: val_loss 0.062090856863348254, val_acc 0.9893617021276596
trigger times: 8
end of epoch 30: val_loss 0.04489633373849032, val_acc 0.9858156028368794
trigger times: 9
end of epoch 31: val_loss 0.06666221275862447, val_acc 0.9849290780141844
trigger times: 10
Early stopping.
0 -292.72979295253754 -199.20912126544414
1 -265.5057263970375 -181.59621720143082
2 -235.68286156654358 -164.41151030608535
3 -238.8861933350563 -161.32605513368566
4 -187.1932983547449 -151.5634741258087
5 -210.54265375062823 -133.76056380009578
6 -167.31074849143624 -128.90054905506827
7 -157.84560544867418 -119.90296557087576
8 -159.38353884220123 -119.33378180391257
9 -157.26608931738883 -114.59035433631308
10 -133.9185080975294 -108.00922275200763
11 -100.36694289837033 -89.08705457223661
12 -93.62532424461097 -80.25399517559948
13 -77.95041808788665 -75.17483043591994
14 -75.93693297053687 -73.76362433676923
15 -80.48934043687768 -66.19170494681471
16 -64.33448513789335 -62.40117935748126
17 -41.477195132290944 -25.99016205153756
18 21.994760546367615 -0.07804593106747401
19 87.0492288832902 38.15087589300597
20 83.82328169647371 43.68426122331964
21 115.51420158124529 63.953351629706646
22 125.74656033655629 64.53271247267233
23 130.2607056896668 74.60390063743114
24 142.79953609165386 81.25241929740568
25 149.38141034543514 85.92277304180317
26 161.36317834630609 92.69937930010626
27 168.19903440237977 98.94672478897846
28 169.62584185763262 99.706470840702
29 175.23637159878854 103.1234668642953
30 181.26013922970742 105.49846404467546
31 174.4068884253502 106.70740032169537
32 184.85295256739482 110.39507157450213
33 189.46520771840005 110.82324768128066
34 186.59908928821096 111.56177998492792
35 190.24533147085458 112.21095617362457
36 192.99409783352166 112.73015958534099
37 193.65202017175034 113.05192791883351
38 200.6765010235831 117.07619652114646
39 200.45562273892574 117.34027665325667
40 197.34480387892108 117.83642486880923
41 202.6917192041874 119.20032127761252
42 208.89776032796362 119.43302815589446
43 202.57613655549358 120.15025122129617
44 206.97470731058274 122.63062574324714
45 216.56700745318085 125.78956526488335
46 211.3220647610724 125.85283998079782
47 216.0729057189019 126.58765053715479
train accuracy: 0.9934831632458051
validation accuracy: 0.9849290780141844
