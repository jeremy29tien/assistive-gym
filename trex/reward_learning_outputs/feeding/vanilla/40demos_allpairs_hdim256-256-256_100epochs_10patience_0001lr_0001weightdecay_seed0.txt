demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 780
num train_labels 780
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:0
end of epoch 0: val_loss 0.5874477940785371, val_acc 0.7987588652482269
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.4024320015618679, val_acc 0.8235815602836879
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.5942455523035228, val_acc 0.8218085106382979
trigger times: 1
end of epoch 3: val_loss 0.43312183260164216, val_acc 0.8581560283687943
trigger times: 2
end of epoch 4: val_loss 0.4821028591558681, val_acc 0.851063829787234
trigger times: 3
end of epoch 5: val_loss 0.3855404444144207, val_acc 0.8785460992907801
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.5031478760924764, val_acc 0.8794326241134752
trigger times: 1
end of epoch 7: val_loss 0.40672570023506904, val_acc 0.8661347517730497
trigger times: 2
end of epoch 8: val_loss 0.38267942252422726, val_acc 0.87677304964539
trigger times: 0
saving model weights...
end of epoch 9: val_loss 1.3897890266656316, val_acc 0.8572695035460993
trigger times: 1
end of epoch 10: val_loss 0.3628447749619685, val_acc 0.8971631205673759
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.3720212297042963, val_acc 0.9086879432624113
trigger times: 1
end of epoch 12: val_loss 0.4315850079689627, val_acc 0.9157801418439716
trigger times: 2
end of epoch 13: val_loss 0.5429329428179775, val_acc 0.9033687943262412
trigger times: 3
end of epoch 14: val_loss 0.8873792875695988, val_acc 0.87677304964539
trigger times: 4
end of epoch 15: val_loss 0.3219021150412921, val_acc 0.9140070921985816
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.3979923855382038, val_acc 0.899822695035461
trigger times: 1
end of epoch 17: val_loss 0.46744090593109133, val_acc 0.9033687943262412
trigger times: 2
end of epoch 18: val_loss 1.1383415849697387, val_acc 0.8714539007092199
trigger times: 3
end of epoch 19: val_loss 0.3730969766037077, val_acc 0.9148936170212766
trigger times: 4
end of epoch 20: val_loss 0.3566762913744053, val_acc 0.8909574468085106
trigger times: 5
end of epoch 21: val_loss 0.27209321074308, val_acc 0.9069148936170213
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.30767028621023035, val_acc 0.9175531914893617
trigger times: 1
end of epoch 23: val_loss 0.5112035458760325, val_acc 0.8838652482269503
trigger times: 2
end of epoch 24: val_loss 0.3233153538059863, val_acc 0.9131205673758865
trigger times: 3
end of epoch 25: val_loss 0.8463855678344043, val_acc 0.8865248226950354
trigger times: 4
end of epoch 26: val_loss 0.36006480149469394, val_acc 0.9113475177304965
trigger times: 5
end of epoch 27: val_loss 1.1638362669742077, val_acc 0.875886524822695
trigger times: 6
end of epoch 28: val_loss 0.6780821912046389, val_acc 0.8608156028368794
trigger times: 7
end of epoch 29: val_loss 0.5594551745364255, val_acc 0.9140070921985816
trigger times: 8
end of epoch 30: val_loss 0.3524062414422061, val_acc 0.9113475177304965
trigger times: 9
end of epoch 31: val_loss 0.2810891985149572, val_acc 0.9157801418439716
trigger times: 10
Early stopping.
0 -48.815554693341255 -199.20912126544414
1 -56.08324398100376 -181.59621720143082
2 -44.29963202774525 -164.41151030608535
3 -51.10959567129612 -161.32605513368566
4 -38.5715437233448 -151.5634741258087
5 -45.50394758582115 -133.76056380009578
6 -42.50775697827339 -128.90054905506827
7 -44.30635993927717 -119.90296557087576
8 -50.627140410244465 -119.33378180391257
9 -48.365000218153 -114.59035433631308
10 -43.011011742055416 -108.00922275200763
11 -44.46519412845373 -89.08705457223661
12 -47.16927865892649 -80.25399517559948
13 -38.412704069167376 -75.17483043591994
14 -40.9210868999362 -73.76362433676923
15 -29.33565241098404 -66.19170494681471
16 -38.48424731940031 -62.40117935748126
17 -38.09102689474821 -25.99016205153756
18 -33.10477687418461 -0.07804593106747401
19 -16.65046369098127 38.15087589300597
20 -19.583192117512226 43.68426122331964
21 -16.34591867774725 63.953351629706646
22 -11.203329086303711 64.53271247267233
23 -11.301537036895752 74.60390063743114
24 -14.009372487664223 81.25241929740568
25 -12.016953855752945 85.92277304180317
26 -8.926056060940027 92.69937930010626
27 -9.707495858892798 98.94672478897846
28 -8.142247498035431 99.706470840702
29 -8.689804207533598 103.1234668642953
30 -9.56459128856659 105.49846404467546
31 -11.445448406040668 106.70740032169537
32 -9.091758832335472 110.39507157450213
33 -11.660490818321705 110.82324768128066
34 -10.1090704575181 111.56177998492792
35 -4.614805817604065 112.21095617362457
36 -6.527922585606575 112.73015958534099
37 -1.6570787690579891 113.05192791883351
38 -6.960798362269998 117.07619652114646
39 -4.876044996082783 117.34027665325667
40 -1.3148587122559547 117.83642486880923
41 -4.76430769264698 119.20032127761252
42 -0.28450363129377365 119.43302815589446
43 -6.217554517090321 120.15025122129617
44 -4.427757650613785 122.63062574324714
45 1.0211626384407282 125.78956526488335
46 3.380259201338049 125.85283998079782
47 1.2568438425660133 126.58765053715479
train accuracy: 0.9910256410256411
validation accuracy: 0.9157801418439716
