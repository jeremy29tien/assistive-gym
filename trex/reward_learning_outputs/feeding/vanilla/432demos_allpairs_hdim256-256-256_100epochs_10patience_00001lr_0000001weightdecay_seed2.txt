demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 93096
num train_labels 93096
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:2
end of epoch 0: val_loss 0.06806237161494214, val_acc 0.9698581560283688
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.07055487082957747, val_acc 0.9680851063829787
trigger times: 1
end of epoch 2: val_loss 0.051571003220294696, val_acc 0.9813829787234043
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.059193377317341936, val_acc 0.9769503546099291
trigger times: 1
end of epoch 4: val_loss 0.040876012025220246, val_acc 0.9813829787234043
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.053999974002941084, val_acc 0.9778368794326241
trigger times: 1
end of epoch 6: val_loss 0.05008547459108703, val_acc 0.9804964539007093
trigger times: 2
end of epoch 7: val_loss 0.05574259873643465, val_acc 0.9778368794326241
trigger times: 3
end of epoch 8: val_loss 0.06579787184043583, val_acc 0.9796099290780141
trigger times: 4
end of epoch 9: val_loss 0.07874595060420157, val_acc 0.975177304964539
trigger times: 5
end of epoch 10: val_loss 0.09423321393050814, val_acc 0.976063829787234
trigger times: 6
end of epoch 11: val_loss 0.060011619174111484, val_acc 0.9804964539007093
trigger times: 7
end of epoch 12: val_loss 0.06996237266807796, val_acc 0.9769503546099291
trigger times: 8
end of epoch 13: val_loss 0.0712955235436743, val_acc 0.976063829787234
trigger times: 9
end of epoch 14: val_loss 0.056926505449847634, val_acc 0.9840425531914894
trigger times: 10
Early stopping.
0 -200.73882082104683 -181.59621720143082
1 -154.09252661466599 -151.71751520071632
2 -164.79206585884094 -147.47910081225547
3 -143.98032295703888 -133.76056380009578
4 -114.56614961382002 -117.07649082709086
5 -113.58062190562487 -111.99728318755238
6 -90.34491117106518 -96.08342159541479
7 -57.796642242057715 -67.56114110862306
8 -54.06395596038783 -64.68081889915388
9 -47.53920283098705 -61.275273342250266
10 -44.04450354113942 -55.44719879154653
11 -34.5478562945209 -47.635037654385464
12 -22.381513424625155 -42.93517995772881
13 -20.419962426181883 -34.935912968324274
14 32.245268658181885 1.293113865583746
15 78.86244898219593 37.008960077453615
16 129.71462808240904 77.62286246422622
17 135.86068210413214 79.3595955035673
18 153.40433913690504 94.19977594187397
19 158.15118027757853 94.60034688743727
20 152.44284407398663 94.84413775624779
21 153.8822785592638 95.93127096706885
22 156.3026660560281 97.31185243955532
23 155.73774846456945 97.75965500483397
24 160.13876504916698 99.706470840702
25 160.73898063856177 100.73191429849227
26 160.35163695694064 100.75917839256617
27 169.49936428753426 105.18568330899544
28 170.15945385568193 107.05778838409489
29 172.5432668697904 107.70203301945793
30 172.6264099541586 108.28588915868846
31 177.55135398946004 111.56177998492792
32 179.00683806219604 111.89996322956685
33 176.32193985258345 112.05237740392577
34 179.79109331319341 113.21987800793073
35 174.35733134916518 113.54624416393074
36 185.03571529098554 113.91500729297944
37 183.92395599366864 116.07692970774649
38 187.48080027091783 116.10590321293648
39 184.32576105219778 117.22159336730809
40 187.73862574552186 119.00744636937388
41 187.97552175377496 119.6581196203716
42 192.0301904269145 122.2662021465837
43 194.11489148973487 123.74388483204041
44 194.27445270685712 124.11350914107612
45 195.19916436608764 125.19854714485804
46 201.6148053832585 128.07431344844193
47 201.3236000528559 129.05523536383924
train accuracy: 0.9925023631520151
validation accuracy: 0.9840425531914894
