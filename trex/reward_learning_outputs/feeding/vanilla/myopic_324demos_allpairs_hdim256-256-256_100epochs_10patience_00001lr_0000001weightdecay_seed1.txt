demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:1
end of epoch 0: val_loss 0.22373726918327058, val_acc 0.9166666666666666
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.2819871348025354, val_acc 0.9069148936170213
trigger times: 1
end of epoch 2: val_loss 0.41110238540713523, val_acc 0.8900709219858156
trigger times: 2
end of epoch 3: val_loss 0.46763874206314054, val_acc 0.899822695035461
trigger times: 3
end of epoch 4: val_loss 0.5681268546747086, val_acc 0.8847517730496454
trigger times: 4
end of epoch 5: val_loss 0.5307942792677424, val_acc 0.8962765957446809
trigger times: 5
end of epoch 6: val_loss 0.5105933320044745, val_acc 0.8980496453900709
trigger times: 6
end of epoch 7: val_loss 0.5669307965597471, val_acc 0.8927304964539007
trigger times: 7
end of epoch 8: val_loss 0.5212794975122064, val_acc 0.8971631205673759
trigger times: 8
end of epoch 9: val_loss 0.5304843797902187, val_acc 0.899822695035461
trigger times: 9
end of epoch 10: val_loss 0.6538976962971915, val_acc 0.8900709219858156
trigger times: 10
Early stopping.
0 -60.20147461886518 -104.15236455725453
1 -27.74056367960293 -72.88057873599428
2 -26.932949751848355 -70.01853738663067
3 -18.789366067154333 -68.09863998902804
4 -39.32942568336148 -66.16919482425385
5 -6.388879617268685 -58.03865621204004
6 -10.547328288652352 -50.110592720448466
7 4.939219169842545 -42.344505534226904
8 -13.363842742459383 -39.119770934487114
9 0.8438811949745286 -37.530262653185694
10 -4.186306233750656 -34.65045736527935
11 2.381356274127029 -28.063290733583624
12 0.27179558284115046 -23.825239960800573
13 3.290198207483627 -18.759973466898963
14 6.993220999953337 -13.414526647052423
15 6.661283456676756 -9.713467352145916
16 8.530283317610156 -6.524706399157447
17 22.496416272824717 -1.5587361690857158
18 4.41841042478336 1.1295272782332058
19 31.20228777558077 1.3733664728469588
20 29.292555103689665 3.910840147015847
21 17.261705144308507 3.956864279956715
22 19.749349223216996 4.073945522417439
23 19.7507441425696 5.771561404278181
24 25.77122050325852 6.601839332596178
25 23.193985863890703 8.064956159563582
26 26.614433460723376 8.140144386082191
27 18.59375311524491 10.710369268899473
28 26.52951454014692 11.999567311575866
29 24.169395735603757 13.143941347541757
30 27.898728413390927 13.206037639420936
31 30.423438564990647 13.507771140823632
32 23.41595335182501 14.330527816265443
33 38.842691807309166 14.735494742760817
34 41.56267995813687 16.06397470914459
35 32.456865373504115 18.175857600465484
36 30.195438985305373 20.230967399184543
37 29.05202773027122 21.569402993580805
38 44.181601377087645 23.894455383781942
39 31.000817232386908 23.994788856620175
40 33.58314190688543 26.775312268993602
41 36.29903020855272 27.724609112624318
42 43.62199519749265 28.567087987508522
43 27.641997982558678 32.72555629328832
44 32.12768066616263 34.522162694388285
45 39.01599946198985 44.20142083216197
46 43.70375754177803 45.67190512037275
47 33.48778358724667 89.60722987949731
train accuracy: 0.9820356992699614
validation accuracy: 0.8900709219858156
