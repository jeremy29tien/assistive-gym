demos: (480, 200, 40)
demo_rewards: (480,)
sorted_train_rewards: [-238.95654684 -231.98610141 -227.53118916 -214.41770129 -204.41223781
 -202.98814303 -199.15784785 -194.89175614 -191.99381123 -189.66740242
 -189.30789369 -187.81566641 -186.07313034 -181.8612155  -181.79899293
 -181.71412786 -180.38100907 -178.40264539 -173.32301622 -173.14447233
 -172.4637427  -168.12866719 -167.81358357 -167.24712713 -165.91483205
 -164.52171291 -163.62780324 -159.92833715 -159.58559605 -158.91948622
 -158.75876908 -157.0327098  -156.13206541 -153.82504331 -153.47830902
 -153.21950984 -152.58553553 -151.7175152  -151.09962807 -149.73158479
 -148.41006071 -148.33843128 -147.49844234 -147.47910081 -145.49135313
 -144.22340051 -141.54946899 -141.44949148 -139.97173932 -139.20066403
 -138.6625699  -138.61924236 -135.77101015 -134.29983302 -129.471792
 -124.72954457 -123.82897183 -123.2975087  -122.42507947 -122.29313461
 -121.35361905 -118.93014437 -118.60870047 -117.07649083 -111.99728319
 -111.95416879 -111.82456558 -111.55938485 -108.07859913 -107.38796212
 -106.01366403 -104.75705754 -104.35997319 -102.05729731 -102.0525638
 -100.79888461 -100.69005607 -100.4999298   -99.86993771  -98.5192976
  -98.42391453  -97.56763316  -96.12840786  -96.0834216   -95.78629767
  -95.72791887  -94.51194096  -94.32183068  -93.8760127   -92.02798597
  -91.76959412  -91.32231658  -89.88535086  -89.11512657  -88.66738786
  -84.8047471   -84.23707176  -81.78273144  -81.16405315  -81.15486081
  -79.8622669   -79.61668855  -78.68748156  -78.65722865  -76.94529937
  -76.44940192  -73.15902797  -71.12319254  -71.09815897  -70.58579775
  -67.56114111  -67.46716696  -66.28614381  -64.6808189   -63.86052285
  -63.43050581  -62.57075967  -62.0661649   -61.57054741  -61.27527334
  -59.04296906  -59.02146236  -57.48973203  -55.44719879  -54.2864063
  -53.62138787  -52.51760365  -51.33455244  -49.67503573  -49.23936644
  -47.63503765  -42.93517996  -42.00460094  -36.77131616  -34.93591297
  -31.42868743  -30.45715645  -18.60085929  -12.69846948   -9.81079693
   -8.34444432   -2.5827935     1.29311387    6.73145762    8.88422653
   17.47400651   20.84127083   23.76776644   24.74176634   26.81939256
   28.59038505   28.74539544   29.134852     33.40103908   34.52388751
   34.63659789   36.56286209   37.00896008   38.85215533   41.73677653
   44.16899732   44.83371463   45.89140537   45.89757504   46.64228055
   47.85272731   48.47037657   50.67138421   52.66182944   58.62776652
   58.80512153   59.1605071    61.46047505   62.2276965    62.74203687
   64.71853682   68.10323921   74.28757855   75.9794571    77.18749523
   77.22391005   77.62286246   78.39646093   78.48236646   79.3595955
   80.59224254   80.75730686   80.86895054   82.97123402   83.75674978
   83.8185762    83.85070653   84.42166728   85.11659265   85.90627549
   85.9404124    86.12496906   86.94904961   87.16655718   87.39561451
   87.76959527   88.21510984   88.36493556   89.8320723    91.28103493
   92.06396174   92.17709635   92.20159701   92.54666017   92.91490936
   93.35905088   93.51419927   93.76842576   94.19977594   94.60034689
   94.66769797   94.84413776   95.22886022   95.81984721   95.93127097
   96.22657096   96.34488228   97.22752342   97.31185244   97.47398698
   97.57657135   97.759655     98.06349695   98.47049183   98.74913733
   99.03122063   99.26897218   99.34565222   99.44048172   99.83148701
  100.53051433  100.55714032  100.7319143   100.75917839  100.7977088
  100.8424906   101.38435155  101.53789448  101.60764068  101.6189997
  101.63310347  102.03454349  102.08829402  102.57644569  103.16890322
  103.25402632  103.31728348  103.40232019  103.4744081   103.4933436
  103.50798167  103.89263364  103.90973367  104.34759312  104.37734949
  104.52083972  104.55501087  104.61737038  104.61967789  104.62521378
  104.76951507  104.77847229  104.8937362   104.89456245  105.05849118
  105.18568331  105.33214185  105.65716252  105.7074157   105.8045272
  106.04955685  106.1552944   106.39425528  106.46206933  106.49396275
  106.54515087  106.74622735  107.03854266  107.05778838  107.16829123
  107.21504828  107.21554847  107.2881685   107.30236866  107.70203302
  107.87670194  107.96147703  108.02820541  108.28588916  108.28858048
  108.35312869  108.39772785  108.56292436  108.72088283  108.79935704
  109.02340032  109.16071209  109.39131335  109.47694977  109.54910496
  109.70911379  109.77726109  109.82650918  110.1003944   110.19789288
  110.3173157   110.33010371  110.49514575  110.52608422  110.54482596
  110.59225047  111.34321125  111.47773403  111.4953737   111.53285156
  111.63038806  111.89996323  111.92292566  111.93947388  112.02235057
  112.0523774   112.19312071  112.21535466  112.32257133  112.39899308
  112.42779759  112.6005029   112.70127527  113.02300081  113.04770713
  113.16421742  113.18709107  113.21987801  113.23481721  113.2991647
  113.34619285  113.4220841   113.4347277   113.46527872  113.49823135
  113.54624416  113.58961069  113.79846893  113.79907099  113.8202641
  113.82742831  113.88963525  113.91500729  113.95902096  113.98972603
  114.13587883  114.14258872  114.19910711  114.22253204  114.37721831
  114.5362235   114.78023701  114.80547636  115.09443674  115.10785836
  115.13846065  115.17650337  115.96446031  116.07692971  116.10590321
  116.12798659  116.21690546  116.32952487  116.41091851  116.47915487
  116.91082189  116.9302937   117.06664939  117.07877633  117.09472388
  117.10824748  117.17575755  117.20868444  117.22159337  117.26925374
  117.39102298  117.60763941  117.69961545  117.7200324   117.74026154
  117.74588406  117.83090322  117.84339654  117.91029171  118.13142264
  118.662252    118.81864792  118.99369545  119.00744637  119.29471869
  119.48536709  119.65811962  119.72511129  120.34331779  120.35314049
  120.36794816  120.72084447  121.25240045  121.58223675  121.80949928
  122.23562738  122.2386899   122.26620215  122.41528706  122.47519798
  122.54844906  122.73860873  123.64827134  123.74388483  124.11350914
  124.1812956   124.64400215  125.19854714  126.13114011  126.46991181
  126.9066037   126.93719164  126.97860983  127.20986504  127.30803269
  128.07431345  129.05523536]
sorted_val_rewards: [-1.99209121e+02 -1.81596217e+02 -1.64411510e+02 -1.61326055e+02
 -1.51563474e+02 -1.33760564e+02 -1.28900549e+02 -1.19902966e+02
 -1.19333782e+02 -1.14590354e+02 -1.08009223e+02 -8.90870546e+01
 -8.02539952e+01 -7.51748304e+01 -7.37636243e+01 -6.61917049e+01
 -6.24011794e+01 -2.59901621e+01 -7.80459311e-02  3.81508759e+01
  4.36842612e+01  6.39533516e+01  6.45327125e+01  7.46039006e+01
  8.12524193e+01  8.59227730e+01  9.26993793e+01  9.89467248e+01
  9.97064708e+01  1.03123467e+02  1.05498464e+02  1.06707400e+02
  1.10395072e+02  1.10823248e+02  1.11561780e+02  1.12210956e+02
  1.12730160e+02  1.13051928e+02  1.17076197e+02  1.17340277e+02
  1.17836425e+02  1.19200321e+02  1.19433028e+02  1.20150251e+02
  1.22630626e+02  1.25789565e+02  1.25852840e+02  1.26587651e+02]
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13568
Number of trainable paramters: 13568
device: cuda:0
end of epoch 0: val_loss 0.07154674019470544, val_acc 0.9804964539007093
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.05045140096709904, val_acc 0.9796099290780141
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.05380190246855858, val_acc 0.975177304964539
trigger times: 1
end of epoch 3: val_loss 0.07174707109070597, val_acc 0.9804964539007093
trigger times: 2
end of epoch 4: val_loss 0.06934526070474885, val_acc 0.9769503546099291
trigger times: 3
end of epoch 5: val_loss 0.0446267536891438, val_acc 0.9840425531914894
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.051906006057424535, val_acc 0.9831560283687943
trigger times: 1
end of epoch 7: val_loss 0.055347736964090934, val_acc 0.9822695035460993
trigger times: 2
end of epoch 8: val_loss 0.03707305848495468, val_acc 0.9840425531914894
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.05869240204029813, val_acc 0.9796099290780141
trigger times: 1
end of epoch 10: val_loss 0.06444335223765613, val_acc 0.9840425531914894
trigger times: 2
end of epoch 11: val_loss 0.051398606465678356, val_acc 0.9787234042553191
trigger times: 3
end of epoch 12: val_loss 0.05366014385329426, val_acc 0.9787234042553191
trigger times: 4
end of epoch 13: val_loss 0.034451642999477686, val_acc 0.9858156028368794
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.12166094283927745, val_acc 0.9769503546099291
trigger times: 1
end of epoch 15: val_loss 0.038856228635115696, val_acc 0.9849290780141844
trigger times: 2
end of epoch 16: val_loss 0.049000044419721375, val_acc 0.9858156028368794
trigger times: 3
end of epoch 17: val_loss 0.06618246547809219, val_acc 0.9822695035460993
trigger times: 4
end of epoch 18: val_loss 0.08163536280635597, val_acc 0.9813829787234043
trigger times: 5
end of epoch 19: val_loss 0.04483432194756438, val_acc 0.9875886524822695
trigger times: 6
end of epoch 20: val_loss 0.03376098509697941, val_acc 0.9858156028368794
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.03721786446095423, val_acc 0.9867021276595744
trigger times: 1
end of epoch 22: val_loss 0.04629882651246877, val_acc 0.9849290780141844
trigger times: 2
end of epoch 23: val_loss 0.03124772989260512, val_acc 0.9920212765957447
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.05729947307993816, val_acc 0.9875886524822695
trigger times: 1
end of epoch 25: val_loss 0.07972858064211737, val_acc 0.9787234042553191
trigger times: 2
end of epoch 26: val_loss 0.0858850897459394, val_acc 0.975177304964539
trigger times: 3
end of epoch 27: val_loss 0.04072659061127674, val_acc 0.9849290780141844
trigger times: 4
end of epoch 28: val_loss 0.033135643619848776, val_acc 0.9849290780141844
trigger times: 5
end of epoch 29: val_loss 0.047476961448685115, val_acc 0.9867021276595744
trigger times: 6
end of epoch 30: val_loss 0.052768746212662324, val_acc 0.9831560283687943
trigger times: 7
end of epoch 31: val_loss 0.04685568278287578, val_acc 0.9840425531914894
trigger times: 8
end of epoch 32: val_loss 0.08436684334751886, val_acc 0.9840425531914894
trigger times: 9
end of epoch 33: val_loss 0.054492972393170705, val_acc 0.9840425531914894
trigger times: 10
Early stopping.
0 -218.70768237113953 -199.20912126544414
1 -174.45733416080475 -181.59621720143082
2 -152.3377326130867 -164.41151030608535
3 -149.78396854922175 -161.32605513368566
4 -129.81683623977005 -151.5634741258087
5 -137.64160206913948 -133.76056380009578
6 -111.6205594111234 -128.90054905506827
7 -110.98930262611248 -119.90296557087576
8 -114.12233383208513 -119.33378180391257
9 -102.52765236760024 -114.59035433631308
10 -84.55465187464142 -108.00922275200763
11 -61.030350891989656 -89.08705457223661
12 -61.323642898350954 -80.25399517559948
13 -47.56065880478127 -75.17483043591994
14 -47.85991410800489 -73.76362433676923
15 -26.83329928200692 -66.19170494681471
16 -35.50935470392869 -62.40117935748126
17 -19.632124940864742 -25.99016205153756
18 23.25059982505627 -0.07804593106747401
19 85.31181984080467 38.15087589300597
20 78.10681101959199 43.68426122331964
21 104.25918683409691 63.953351629706646
22 110.91348705848213 64.53271247267233
23 116.68860855605453 74.60390063743114
24 123.5428486475721 81.25241929740568
25 132.8193875029683 85.92277304180317
26 138.89025569427758 92.69937930010626
27 146.28168184345122 98.94672478897846
28 146.01709754776675 99.706470840702
29 154.7619874048396 103.1234668642953
30 154.13716686703265 105.49846404467546
31 149.28888669364096 106.70740032169537
32 159.75993040401954 110.39507157450213
33 160.99803563326714 110.82324768128066
34 160.6890117651783 111.56177998492792
35 159.6017498295987 112.21095617362457
36 160.88472859974718 112.73015958534099
37 165.25333728897385 113.05192791883351
38 166.4580390988267 117.07619652114646
39 167.76446338836104 117.34027665325667
40 169.2306019780226 117.83642486880923
41 170.43043208948802 119.20032127761252
42 174.69177066534758 119.43302815589446
43 171.2224661426153 120.15025122129617
44 175.1929498630343 122.63062574324714
45 182.87872537161456 125.78956526488335
46 178.59730814793147 125.85283998079782
47 184.70542059492436 126.58765053715479
train accuracy: 0.9905018537629476
validation accuracy: 0.9840425531914894
