demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 780
num train_labels 780
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:0
end of epoch 0: val_loss 0.4424912848348555, val_acc 0.8297872340425532
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.6931576625963699, val_acc 0.8058510638297872
trigger times: 1
end of epoch 2: val_loss 0.6974458398773005, val_acc 0.8182624113475178
trigger times: 2
end of epoch 3: val_loss 0.4734636140347419, val_acc 0.8572695035460993
trigger times: 3
end of epoch 4: val_loss 0.9595002729430375, val_acc 0.8040780141843972
trigger times: 4
end of epoch 5: val_loss 0.4379438777254021, val_acc 0.8652482269503546
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.66408405037463, val_acc 0.8528368794326241
trigger times: 1
end of epoch 7: val_loss 0.41824107890338513, val_acc 0.8882978723404256
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.3744564735191217, val_acc 0.8812056737588653
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.6940279134163966, val_acc 0.8617021276595744
trigger times: 1
end of epoch 10: val_loss 0.36854208843987163, val_acc 0.8927304964539007
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.5820681639082257, val_acc 0.8829787234042553
trigger times: 1
end of epoch 12: val_loss 0.8919975965921196, val_acc 0.8812056737588653
trigger times: 2
end of epoch 13: val_loss 0.7148808862010514, val_acc 0.8590425531914894
trigger times: 3
end of epoch 14: val_loss 0.6643448307205795, val_acc 0.8652482269503546
trigger times: 4
end of epoch 15: val_loss 0.33379586234752867, val_acc 0.9095744680851063
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.5209200799001501, val_acc 0.8936170212765957
trigger times: 1
end of epoch 17: val_loss 0.3794887425462576, val_acc 0.898936170212766
trigger times: 2
end of epoch 18: val_loss 0.5435250138127605, val_acc 0.900709219858156
trigger times: 3
end of epoch 19: val_loss 0.4328712079290693, val_acc 0.901595744680851
trigger times: 4
end of epoch 20: val_loss 0.9440477036295042, val_acc 0.8723404255319149
trigger times: 5
end of epoch 21: val_loss 0.26962952567740517, val_acc 0.9086879432624113
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.18836584826208091, val_acc 0.9131205673758865
trigger times: 0
saving model weights...
end of epoch 23: val_loss 0.30722086299685003, val_acc 0.9086879432624113
trigger times: 1
end of epoch 24: val_loss 0.41199857999493017, val_acc 0.8953900709219859
trigger times: 2
end of epoch 25: val_loss 0.5635442770169966, val_acc 0.8679078014184397
trigger times: 3
end of epoch 26: val_loss 0.40412553193982553, val_acc 0.8953900709219859
trigger times: 4
end of epoch 27: val_loss 0.9408209029372806, val_acc 0.875
trigger times: 5
end of epoch 28: val_loss 0.5661248415008181, val_acc 0.8918439716312057
trigger times: 6
end of epoch 29: val_loss 0.622466710653524, val_acc 0.8900709219858156
trigger times: 7
end of epoch 30: val_loss 1.0460720663333714, val_acc 0.8723404255319149
trigger times: 8
end of epoch 31: val_loss 0.75519347076707, val_acc 0.87322695035461
trigger times: 9
end of epoch 32: val_loss 5.1450124794287015, val_acc 0.8342198581560284
trigger times: 10
Early stopping.
0 28.44101645052433 -181.59621720143082
1 78.33834859728813 -151.71751520071632
2 55.5948628783226 -147.47910081225547
3 64.5475394576788 -133.76056380009578
4 140.45336888730526 -117.07649082709086
5 122.73995848745108 -111.99728318755238
6 144.86558113247156 -96.08342159541479
7 175.7194223627448 -67.56114110862306
8 253.9429930150509 -64.68081889915388
9 172.53330990672112 -61.275273342250266
10 211.95206901431084 -55.44719879154653
11 255.63120938837528 -47.635037654385464
12 157.33626294136047 -42.93517995772881
13 206.6813693791628 -34.935912968324274
14 168.67299748957157 1.293113865583746
15 210.04889205098152 37.008960077453615
16 217.0328510850668 77.62286246422622
17 236.1271279901266 79.3595955035673
18 189.75949361920357 94.19977594187397
19 293.1576226055622 94.60034688743727
20 274.6055745780468 94.84413775624779
21 255.05908897519112 95.93127096706885
22 226.6593468785286 97.31185243955532
23 289.21626932919025 97.75965500483397
24 243.76353815197945 99.706470840702
25 235.83595618605614 100.73191429849227
26 276.1412414908409 100.75917839256617
27 234.52320609986782 105.18568330899544
28 298.9774677455425 107.05778838409489
29 189.76320387423038 107.70203301945793
30 273.0707833021879 108.28588915868846
31 306.736359834671 111.56177998492792
32 269.2992245256901 111.89996322956685
33 266.072314620018 112.05237740392577
34 288.8760753571987 113.21987800793073
35 266.3070914745331 113.54624416393074
36 317.09205654263496 113.91500729297944
37 290.33196434378624 116.07692970774649
38 332.5398568511009 116.10590321293648
39 233.4623686671257 117.22159336730809
40 295.0213889181614 119.00744636937388
41 260.71099354326725 119.6581196203716
42 309.489595413208 122.2662021465837
43 324.14407846331596 123.74388483204041
44 335.7415321469307 124.11350914107612
45 264.78209160268307 125.19854714485804
46 333.9931697845459 128.07431344844193
47 359.26834493875504 129.05523536383924
train accuracy: 0.8961538461538462
validation accuracy: 0.8342198581560284
