demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=256, bias=True)
  (4): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 208128
Number of trainable paramters: 208128
device: cuda:1
end of epoch 0: val_loss 0.08794405432288556, val_acc 0.9654255319148937
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.07020065499808198, val_acc 0.9698581560283688
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.11606722032287373, val_acc 0.9654255319148937
trigger times: 1
end of epoch 3: val_loss 0.06620168332046905, val_acc 0.9787234042553191
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.04578048174494351, val_acc 0.9813829787234043
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.06890651982592072, val_acc 0.9787234042553191
trigger times: 1
end of epoch 6: val_loss 0.06116893907913863, val_acc 0.974290780141844
trigger times: 2
end of epoch 7: val_loss 0.0647839111266295, val_acc 0.9787234042553191
trigger times: 3
end of epoch 8: val_loss 0.06757170006097303, val_acc 0.974290780141844
trigger times: 4
end of epoch 9: val_loss 0.0494506607456041, val_acc 0.9831560283687943
trigger times: 5
end of epoch 10: val_loss 0.0698045887979928, val_acc 0.9778368794326241
trigger times: 6
end of epoch 11: val_loss 0.10213360266545911, val_acc 0.9778368794326241
trigger times: 7
end of epoch 12: val_loss 0.07155945733977416, val_acc 0.973404255319149
trigger times: 8
end of epoch 13: val_loss 0.061350020740576834, val_acc 0.9787234042553191
trigger times: 9
end of epoch 14: val_loss 0.07709245832764393, val_acc 0.9787234042553191
trigger times: 10
Early stopping.
0 -214.5996269285679 -231.98610141016846
1 -144.07531169056892 -180.38100907229617
2 -128.97337336838245 -159.58559605250935
3 -134.8799150288105 -158.91948622182267
4 -112.74751210957766 -144.22340051424408
5 -91.21077193319798 -119.33378180391257
6 -87.35999385965988 -118.93014436813327
7 -80.13802587217651 -111.99728318755238
8 -45.56729726027697 -102.05729730987049
9 -42.0263487175107 -81.78273143585434
10 -47.88484289729968 -81.15486081250215
11 -23.113835517782718 -62.06616489925748
12 -20.12629496178124 -59.021462360023776
13 -11.798507089726627 -57.48973202587643
14 -15.035660916939378 -54.2864062956764
15 -10.275127714674454 -25.99016205153756
16 16.550852321553975 -18.600859293147124
17 29.435261537873885 -9.810796929333884
18 35.851420120336115 -2.58279350160679
19 63.863122703973204 34.523887505208855
20 74.18834797851741 38.85215532611078
21 77.28437999100424 44.83371463291592
22 109.25686733273324 64.53271247267233
23 109.0356288831681 78.39646093135971
24 125.7628637017915 85.11659264541795
25 135.0778203890659 94.60034688743727
26 126.71681755175814 97.31185243955532
27 139.05976539140102 97.75965500483397
28 131.5508262391486 98.47049183311258
29 133.4454035437666 100.53051433426002
30 142.0473058715579 102.03454348571637
31 137.15646059048595 103.89263364013634
32 138.60009940354212 104.89456244575071
33 141.39192381489556 105.33214185027923
34 139.1844494449033 107.03854266018475
35 145.0801991459448 108.39772785012178
36 142.48427665629424 109.54910495619963
37 146.31068201363087 111.56177998492792
38 152.40694378712215 111.92292565511607
39 148.4848575410433 112.02235057401518
40 147.5113246854453 112.32257133288935
41 148.63845530478284 113.2348172145928
42 151.93116775341332 113.8274283128223
43 154.46376151536242 116.41091851337076
44 157.9906932641752 117.06664939486333
45 157.94453138671815 117.10824747619725
46 164.12128668790683 122.23868990266085
47 167.25251291051973 127.2098650437575
train accuracy: 0.9914765126323434
validation accuracy: 0.9787234042553191
