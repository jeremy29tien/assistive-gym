demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 93096
num train_labels 93096
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:2
end of epoch 0: val_loss 0.029619938544625874, val_acc 0.9858156028368794
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.027335942072144872, val_acc 0.9911347517730497
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.0526975950929301, val_acc 0.9804964539007093
trigger times: 1
end of epoch 3: val_loss 0.041714551062428926, val_acc 0.9858156028368794
trigger times: 2
end of epoch 4: val_loss 0.03264944927924757, val_acc 0.9902482269503546
trigger times: 3
end of epoch 5: val_loss 0.03824935039424128, val_acc 0.9902482269503546
trigger times: 4
end of epoch 6: val_loss 0.03519834463121805, val_acc 0.9875886524822695
trigger times: 5
end of epoch 7: val_loss 0.05491727442449606, val_acc 0.9867021276595744
trigger times: 6
end of epoch 8: val_loss 0.04195050359928106, val_acc 0.9875886524822695
trigger times: 7
end of epoch 9: val_loss 0.025323280538538353, val_acc 0.9929078014184397
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.026589943356306465, val_acc 0.9911347517730497
trigger times: 1
end of epoch 11: val_loss 0.03500269845908244, val_acc 0.9875886524822695
trigger times: 2
end of epoch 12: val_loss 0.0284591576068899, val_acc 0.9911347517730497
trigger times: 3
end of epoch 13: val_loss 0.028405236708767004, val_acc 0.9929078014184397
trigger times: 4
end of epoch 14: val_loss 0.03469519239287029, val_acc 0.9911347517730497
trigger times: 5
end of epoch 15: val_loss 0.040707225781623786, val_acc 0.9884751773049646
trigger times: 6
end of epoch 16: val_loss 0.022153102526666385, val_acc 0.9911347517730497
trigger times: 0
saving model weights...
end of epoch 17: val_loss 0.02746194157515833, val_acc 0.9902482269503546
trigger times: 1
end of epoch 18: val_loss 0.033929720459982576, val_acc 0.9929078014184397
trigger times: 2
end of epoch 19: val_loss 0.03150788141269961, val_acc 0.9920212765957447
trigger times: 3
end of epoch 20: val_loss 0.03177794512102144, val_acc 0.9920212765957447
trigger times: 4
end of epoch 21: val_loss 0.030471946662137716, val_acc 0.9884751773049646
trigger times: 5
end of epoch 22: val_loss 0.03762187234757741, val_acc 0.9884751773049646
trigger times: 6
end of epoch 23: val_loss 0.03489460045100907, val_acc 0.9893617021276596
trigger times: 7
end of epoch 24: val_loss 0.029847424105927917, val_acc 0.9902482269503546
trigger times: 8
end of epoch 25: val_loss 0.03790213215059207, val_acc 0.9884751773049646
trigger times: 9
end of epoch 26: val_loss 0.041091186621791884, val_acc 0.9902482269503546
trigger times: 10
Early stopping.
0 -315.7068719267845 -199.20912126544414
1 -275.2813166975975 -181.59621720143082
2 -252.80793976783752 -164.41151030608535
3 -245.26186364889145 -161.32605513368566
4 -213.11820782721043 -151.5634741258087
5 -202.71927413344383 -133.76056380009578
6 -181.46798011567444 -128.90054905506827
7 -164.67901770999015 -119.90296557087576
8 -160.95059452950954 -119.33378180391257
9 -151.97853315569228 -114.59035433631308
10 -145.4204612225294 -108.00922275200763
11 -96.57256726332707 -89.08705457223661
12 -87.65128595288843 -80.25399517559948
13 -66.67529223818565 -75.17483043591994
14 -67.59820471482817 -73.76362433676923
15 -65.80230455455603 -66.19170494681471
16 -52.52744659243035 -62.40117935748126
17 -29.983704883954488 -25.99016205153756
18 49.56545172352344 -0.07804593106747401
19 125.17227014584932 38.15087589300597
20 121.93636285932735 43.68426122331964
21 159.48661519191228 63.953351629706646
22 170.58592499792576 64.53271247267233
23 178.3622332427767 74.60390063743114
24 189.25994397257455 81.25241929740568
25 201.13547744217794 85.92277304180317
26 210.69326488720253 92.69937930010626
27 220.6825479932595 98.94672478897846
28 223.90497373812832 99.706470840702
29 230.4609953109175 103.1234668642953
30 236.94325446058065 105.49846404467546
31 230.9391014977591 106.70740032169537
32 242.31174886715598 110.39507157450213
33 245.35586356022395 110.82324768128066
34 243.1591345871566 111.56177998492792
35 245.798941696994 112.21095617362457
36 249.25206492014695 112.73015958534099
37 248.09118485823274 113.05192791883351
38 256.7868344495073 117.07619652114646
39 257.89416379807517 117.34027665325667
40 256.0027572317049 117.83642486880923
41 260.0877455342561 119.20032127761252
42 267.39267611852847 119.43302815589446
43 261.77507649350446 120.15025122129617
44 263.1698524800013 122.63062574324714
45 279.83228871709434 125.78956526488335
46 268.28046012314735 125.85283998079782
47 275.9364963377302 126.58765053715479
train accuracy: 0.9946614247658331
validation accuracy: 0.9902482269503546
