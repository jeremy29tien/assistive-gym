demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142336
Number of trainable paramters: 142336
device: cuda:3
end of epoch 0: val_loss 0.3196760898015085, val_acc 0.9042553191489362
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.33419241854163045, val_acc 0.8927304964539007
trigger times: 1
end of epoch 2: val_loss 0.3270290320444396, val_acc 0.8847517730496454
trigger times: 2
end of epoch 3: val_loss 0.3512873684286457, val_acc 0.8829787234042553
trigger times: 3
end of epoch 4: val_loss 0.3087627743026126, val_acc 0.901595744680851
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.2884202998211807, val_acc 0.9086879432624113
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.3263433415856351, val_acc 0.8900709219858156
trigger times: 1
end of epoch 7: val_loss 0.33373926787353153, val_acc 0.8785460992907801
trigger times: 2
end of epoch 8: val_loss 0.31000878017706174, val_acc 0.8847517730496454
trigger times: 3
end of epoch 9: val_loss 0.2931691948790929, val_acc 0.8882978723404256
trigger times: 4
end of epoch 10: val_loss 0.3161768771605632, val_acc 0.8909574468085106
trigger times: 5
end of epoch 11: val_loss 0.3099446717574565, val_acc 0.8980496453900709
trigger times: 6
end of epoch 12: val_loss 0.30178079065137725, val_acc 0.8980496453900709
trigger times: 7
end of epoch 13: val_loss 0.3109303201682509, val_acc 0.900709219858156
trigger times: 8
end of epoch 14: val_loss 0.2957865092980779, val_acc 0.8962765957446809
trigger times: 9
end of epoch 15: val_loss 0.3063762064483215, val_acc 0.8847517730496454
trigger times: 10
Early stopping.
0 -2.969208477763459 -181.59621720143082
1 -2.5045237346785143 -151.71751520071632
2 -2.6178595624896843 -147.47910081225547
3 -2.39531405170419 -133.76056380009578
4 -2.04023509790386 -117.07649082709086
5 -2.172689838334918 -111.99728318755238
6 -2.120520000884426 -96.08342159541479
7 -1.5572115598279197 -67.56114110862306
8 -1.6394577541504987 -64.68081889915388
9 -1.4839552588400693 -61.275273342250266
10 -1.6435570190842554 -55.44719879154653
11 -1.2531113076402107 -47.635037654385464
12 -1.2539495305391029 -42.93517995772881
13 -1.8459963474488177 -34.935912968324274
14 -0.7848963592314249 1.293113865583746
15 -0.24043756184801168 37.008960077453615
16 -0.11769926237684558 77.62286246422622
17 0.21726920372566383 79.3595955035673
18 0.5530525303038303 94.19977594187397
19 0.6812865866027096 94.60034688743727
20 0.07340736605692655 94.84413775624779
21 0.2124792034210259 95.93127096706885
22 -0.18792283174116164 97.31185243955532
23 0.8614900662214495 97.75965500483397
24 0.4549647981330054 99.706470840702
25 0.5027582468173932 100.73191429849227
26 -0.03986623972377856 100.75917839256617
27 0.4039236163953319 105.18568330899544
28 1.4896505215729121 107.05778838409489
29 1.3455725340099889 107.70203301945793
30 0.9620755055511836 108.28588915868846
31 0.5357163881417364 111.56177998492792
32 0.4225205982893385 111.89996322956685
33 0.7952364009579469 112.05237740392577
34 1.1422497455132543 113.21987800793073
35 0.1317031180660706 113.54624416393074
36 1.3508428846107563 113.91500729297944
37 1.4639673563960969 116.07692970774649
38 1.3635391541120043 116.10590321293648
39 1.457440379585023 117.22159336730809
40 1.7365641171454627 119.00744636937388
41 1.213524865051113 119.6581196203716
42 1.1071419608815631 122.2662021465837
43 1.3319252300279913 123.74388483204041
44 1.1928637863929907 124.11350914107612
45 1.2861935373221058 125.19854714485804
46 2.0067216569077573 128.07431344844193
47 2.437451461009914 129.05523536383924
train accuracy: 0.8766960975423308
validation accuracy: 0.8847517730496454
