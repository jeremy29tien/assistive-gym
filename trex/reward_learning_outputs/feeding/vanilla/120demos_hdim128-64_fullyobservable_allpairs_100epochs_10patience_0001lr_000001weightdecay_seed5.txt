demos: (480, 200, 40)
demo_rewards: (480,)
sorted_train_rewards: [-2.38956547e+02 -2.31986101e+02 -2.27531189e+02 -2.04412238e+02
 -2.02988143e+02 -1.91993811e+02 -1.89667402e+02 -1.89307894e+02
 -1.87815666e+02 -1.86073130e+02 -1.81861215e+02 -1.81798993e+02
 -1.81714128e+02 -1.81596217e+02 -1.80381009e+02 -1.78402645e+02
 -1.73323016e+02 -1.73144472e+02 -1.72463743e+02 -1.68128667e+02
 -1.67813584e+02 -1.64521713e+02 -1.64411510e+02 -1.63627803e+02
 -1.61326055e+02 -1.59928337e+02 -1.59585596e+02 -1.58919486e+02
 -1.58758769e+02 -1.57032710e+02 -1.56132065e+02 -1.53478309e+02
 -1.53219510e+02 -1.51717515e+02 -1.51563474e+02 -1.49731585e+02
 -1.48410061e+02 -1.48338431e+02 -1.47498442e+02 -1.47479101e+02
 -1.45491353e+02 -1.44223401e+02 -1.41549469e+02 -1.41449491e+02
 -1.39971739e+02 -1.39200664e+02 -1.38662570e+02 -1.35771010e+02
 -1.34299833e+02 -1.33760564e+02 -1.29471792e+02 -1.28900549e+02
 -1.24729545e+02 -1.23828972e+02 -1.22425079e+02 -1.22293135e+02
 -1.21353619e+02 -1.19902966e+02 -1.19333782e+02 -1.18930144e+02
 -1.18608700e+02 -1.17076491e+02 -1.14590354e+02 -1.11997283e+02
 -1.11954169e+02 -1.11824566e+02 -1.11559385e+02 -1.08078599e+02
 -1.08009223e+02 -1.07387962e+02 -1.06013664e+02 -1.04757058e+02
 -1.04359973e+02 -1.02057297e+02 -1.02052564e+02 -1.00798885e+02
 -1.00690056e+02 -1.00499930e+02 -9.98699377e+01 -9.85192976e+01
 -9.84239145e+01 -9.75676332e+01 -9.60834216e+01 -9.57862977e+01
 -9.57279189e+01 -9.45119410e+01 -9.43218307e+01 -9.38760127e+01
 -9.20279860e+01 -9.17695941e+01 -9.13223166e+01 -8.98853509e+01
 -8.91151266e+01 -8.90870546e+01 -8.86673879e+01 -8.48047471e+01
 -8.42370718e+01 -8.17827314e+01 -8.11640532e+01 -8.11548608e+01
 -8.02539952e+01 -7.98622669e+01 -7.86874816e+01 -7.86572287e+01
 -7.64494019e+01 -7.37636243e+01 -7.31590280e+01 -7.11231925e+01
 -7.10981590e+01 -7.05857977e+01 -6.75611411e+01 -6.74671670e+01
 -6.62861438e+01 -6.61917049e+01 -6.46808189e+01 -6.38605228e+01
 -6.25707597e+01 -6.24011794e+01 -6.20661649e+01 -6.15705474e+01
 -6.12752733e+01 -5.90429691e+01 -5.90214624e+01 -5.74897320e+01
 -5.54471988e+01 -5.42864063e+01 -5.36213879e+01 -5.25176037e+01
 -5.13345524e+01 -4.96750357e+01 -4.92393664e+01 -4.76350377e+01
 -4.29351800e+01 -4.20046009e+01 -3.67713162e+01 -3.49359130e+01
 -3.14286874e+01 -3.04571565e+01 -2.59901621e+01 -1.86008593e+01
 -1.26984695e+01 -9.81079693e+00 -8.34444432e+00 -2.58279350e+00
 -7.80459311e-02  1.29311387e+00  6.73145762e+00  8.88422653e+00
  1.74740065e+01  2.08412708e+01  2.37677664e+01  2.47417663e+01
  2.68193926e+01  2.85903851e+01  2.87453954e+01  2.91348520e+01
  3.46365979e+01  3.65628621e+01  3.70089601e+01  3.88521553e+01
  4.17367765e+01  4.36842612e+01  4.41689973e+01  4.48337146e+01
  4.58914054e+01  4.58975750e+01  4.66422805e+01  4.78527273e+01
  4.84703766e+01  5.06713842e+01  5.26618294e+01  5.88051215e+01
  5.91605071e+01  6.14604751e+01  6.22276965e+01  6.27420369e+01
  6.47185368e+01  6.81032392e+01  7.42875786e+01  7.46039006e+01
  7.59794571e+01  7.71874952e+01  7.72239101e+01  7.76228625e+01
  7.83964609e+01  7.93595955e+01  8.05922425e+01  8.07573069e+01
  8.08689505e+01  8.12524193e+01  8.29712340e+01  8.37567498e+01
  8.38185762e+01  8.38507065e+01  8.44216673e+01  8.51165926e+01
  8.59062755e+01  8.59227730e+01  8.59404124e+01  8.61249691e+01
  8.69490496e+01  8.71665572e+01  8.73956145e+01  8.77695953e+01
  8.98320723e+01  9.12810349e+01  9.20639617e+01  9.21770964e+01
  9.22015970e+01  9.26993793e+01  9.29149094e+01  9.33590509e+01
  9.35141993e+01  9.37684258e+01  9.41997759e+01  9.46003469e+01
  9.46676980e+01  9.52288602e+01  9.58198472e+01  9.59312710e+01
  9.62265710e+01  9.63448823e+01  9.72275234e+01  9.74739870e+01
  9.75765713e+01  9.77596550e+01  9.80634969e+01  9.84704918e+01
  9.87491373e+01  9.89467248e+01  9.90312206e+01  9.92689722e+01
  9.93456522e+01  9.94404817e+01  9.97064708e+01  9.98314870e+01
  1.00530514e+02  1.00557140e+02  1.00731914e+02  1.00759178e+02
  1.00842491e+02  1.01384352e+02  1.01537894e+02  1.01607641e+02
  1.01619000e+02  1.01633103e+02  1.02034543e+02  1.02088294e+02
  1.02576446e+02  1.03123467e+02  1.03168903e+02  1.03254026e+02
  1.03317283e+02  1.03402320e+02  1.03474408e+02  1.03493344e+02
  1.03507982e+02  1.03892634e+02  1.04347593e+02  1.04520840e+02
  1.04555011e+02  1.04617370e+02  1.04619678e+02  1.04769515e+02
  1.04778472e+02  1.04893736e+02  1.04894562e+02  1.05185683e+02
  1.05332142e+02  1.05498464e+02  1.05657163e+02  1.05707416e+02
  1.05804527e+02  1.06049557e+02  1.06394255e+02  1.06462069e+02
  1.06493963e+02  1.06545151e+02  1.06707400e+02  1.07038543e+02
  1.07057788e+02  1.07168291e+02  1.07215048e+02  1.07215548e+02
  1.07288168e+02  1.07302369e+02  1.07702033e+02  1.07876702e+02
  1.07961477e+02  1.08028205e+02  1.08285889e+02  1.08288580e+02
  1.08353129e+02  1.08397728e+02  1.08720883e+02  1.09023400e+02
  1.09160712e+02  1.09391313e+02  1.09476950e+02  1.09709114e+02
  1.09777261e+02  1.09826509e+02  1.10100394e+02  1.10197893e+02
  1.10317316e+02  1.10330104e+02  1.10395072e+02  1.10495146e+02
  1.10526084e+02  1.10544826e+02  1.10592250e+02  1.10823248e+02
  1.11343211e+02  1.11495374e+02  1.11532852e+02  1.11561780e+02
  1.11630388e+02  1.11899963e+02  1.11922926e+02  1.11939474e+02
  1.12022351e+02  1.12052377e+02  1.12193121e+02  1.12210956e+02
  1.12215355e+02  1.12322571e+02  1.12398993e+02  1.12427798e+02
  1.12701275e+02  1.12730160e+02  1.13023001e+02  1.13047707e+02
  1.13051928e+02  1.13164217e+02  1.13187091e+02  1.13219878e+02
  1.13234817e+02  1.13299165e+02  1.13346193e+02  1.13434728e+02
  1.13465279e+02  1.13498231e+02  1.13546244e+02  1.13589611e+02
  1.13798469e+02  1.13799071e+02  1.13820264e+02  1.13827428e+02
  1.13889635e+02  1.13915007e+02  1.13959021e+02  1.13989726e+02
  1.14142589e+02  1.14199107e+02  1.14222532e+02  1.14377218e+02
  1.14536223e+02  1.14780237e+02  1.14805476e+02  1.15094437e+02
  1.15107858e+02  1.15138461e+02  1.15176503e+02  1.15964460e+02
  1.16076930e+02  1.16105903e+02  1.16127987e+02  1.16216905e+02
  1.16329525e+02  1.16410919e+02  1.16479155e+02  1.16910822e+02
  1.16930294e+02  1.17066649e+02  1.17076197e+02  1.17078776e+02
  1.17094724e+02  1.17108247e+02  1.17175758e+02  1.17208684e+02
  1.17221593e+02  1.17269254e+02  1.17340277e+02  1.17607639e+02
  1.17699615e+02  1.17720032e+02  1.17740262e+02  1.17745884e+02
  1.17830903e+02  1.17843397e+02  1.18131423e+02  1.18662252e+02
  1.18818648e+02  1.18993695e+02  1.19007446e+02  1.19200321e+02
  1.19294719e+02  1.19433028e+02  1.19485367e+02  1.19658120e+02
  1.20150251e+02  1.20343318e+02  1.20353140e+02  1.20367948e+02
  1.20720844e+02  1.21252400e+02  1.21582237e+02  1.21809499e+02
  1.22235627e+02  1.22238690e+02  1.22266202e+02  1.22415287e+02
  1.22475198e+02  1.22548449e+02  1.22630626e+02  1.22738609e+02
  1.23648271e+02  1.23743885e+02  1.24181296e+02  1.24644002e+02
  1.25198547e+02  1.25789565e+02  1.25852840e+02  1.26131140e+02
  1.26469912e+02  1.26587651e+02  1.26906604e+02  1.26937192e+02
  1.26978610e+02  1.27209865e+02  1.28074313e+02  1.29055235e+02]
sorted_val_rewards: [-214.41770129 -199.20912127 -199.15784785 -194.89175614 -167.24712713
 -165.91483205 -153.82504331 -152.58553553 -151.09962807 -138.61924236
 -123.2975087   -96.12840786  -79.61668855  -76.94529937  -75.17483044
  -63.43050581   33.40103908   34.52388751   38.15087589   58.62776652
   63.95335163   64.53271247   78.48236646   88.21510984   88.36493556
   92.54666017   94.84413776   97.31185244  100.7977088   103.90973367
  104.37734949  104.62521378  105.05849118  106.1552944   106.74622735
  108.56292436  108.79935704  109.54910496  111.47773403  112.6005029
  113.4220841   114.13587883  117.39102298  117.83642487  117.91029171
  119.72511129  124.11350914  127.30803269]
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13568
Number of trainable paramters: 13568
device: cuda:3
end of epoch 0: val_loss 0.215364306161452, val_acc 0.925531914893617
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.189413962434286, val_acc 0.9388297872340425
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.10300397725171283, val_acc 0.9556737588652482
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.14586171289261382, val_acc 0.9609929078014184
trigger times: 1
end of epoch 4: val_loss 0.10923924269416914, val_acc 0.9671985815602837
trigger times: 2
end of epoch 5: val_loss 0.19699516822581994, val_acc 0.9530141843971631
trigger times: 3
end of epoch 6: val_loss 0.15100092998103273, val_acc 0.9601063829787234
trigger times: 4
end of epoch 7: val_loss 0.1418596421401576, val_acc 0.9627659574468085
trigger times: 5
end of epoch 8: val_loss 0.11557772088975252, val_acc 0.9725177304964538
trigger times: 6
end of epoch 9: val_loss 0.127231232216649, val_acc 0.9689716312056738
trigger times: 7
end of epoch 10: val_loss 0.1573247868371016, val_acc 0.9654255319148937
trigger times: 8
end of epoch 11: val_loss 0.1536143228892276, val_acc 0.9663120567375887
trigger times: 9
end of epoch 12: val_loss 0.20594657282647844, val_acc 0.973404255319149
trigger times: 10
Early stopping.
0 -103.29222577810287 -214.417701286285
1 -126.75125622749329 -199.20912126544414
2 -118.23708961904049 -199.15784784598026
3 -117.25519692897797 -194.8917561390424
4 -75.37196467071772 -167.2471271326611
5 -63.935163776390254 -165.91483205068334
6 -65.86247565876693 -153.8250433077314
7 -82.51878674328327 -152.58553553277824
8 -86.28534147143364 -151.09962806671584
9 -70.5476816034352 -138.61924235846436
10 -71.66272892057896 -123.29750870197792
11 -35.72153272363357 -96.12840786180749
12 -30.70295350451488 -79.61668855255952
13 -22.785955674087745 -76.94529937266098
14 -32.48467525016167 -75.17483043591994
15 -21.70073012521607 -63.43050580727678
16 43.70309953231481 33.401039078259274
17 32.72686477597745 34.523887505208855
18 47.490307831176324 38.15087589300597
19 55.654552514723036 58.627766523049665
20 60.115864536906884 63.953351629706646
21 66.11091121871141 64.53271247267233
22 70.10686191439163 78.48236646312385
23 75.65509456989821 88.2151098413278
24 75.97731593623757 88.36493556294752
25 77.09878209116869 92.54666016860398
26 80.76087882206775 94.84413775624779
27 82.2338529068511 97.31185243955532
28 85.34611396165565 100.7977087983452
29 86.18436285748612 103.90973367320053
30 87.62275265483186 104.37734948926527
31 86.08651769789867 104.62521378176316
32 90.85311148199253 105.0584911836046
33 92.58994976611575 106.15529439510486
34 91.99208719248418 106.74622734614397
35 92.98583028960275 108.56292436467679
36 94.96150081418455 108.79935704378586
37 93.07796080203116 109.54910495619963
38 91.99575647249003 111.47773402735328
39 96.15192196174394 112.60050290438329
40 94.2947405106388 113.42208410212889
41 95.62965858890675 114.13587883143343
42 97.65594242488442 117.39102298242703
43 98.05206505069509 117.83642486880923
44 99.54039761517197 117.9102917094035
45 99.62251525744796 119.72511129460356
46 104.32907865429297 124.11350914107612
47 105.19901896045485 127.30803269453892
train accuracy: 0.9830532212885154
validation accuracy: 0.973404255319149
