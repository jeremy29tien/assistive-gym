demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=256, bias=True)
  (4): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 208128
Number of trainable paramters: 208128
device: cuda:0
end of epoch 0: val_loss 0.08292865440262735, val_acc 0.9663120567375887
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.07903982263948918, val_acc 0.9654255319148937
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.1139821158046327, val_acc 0.9609929078014184
trigger times: 1
end of epoch 3: val_loss 0.08755652714331758, val_acc 0.9689716312056738
trigger times: 2
end of epoch 4: val_loss 0.11692545728072462, val_acc 0.9645390070921985
trigger times: 3
end of epoch 5: val_loss 0.07460823719456351, val_acc 0.9716312056737588
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.09118935710311221, val_acc 0.9689716312056738
trigger times: 1
end of epoch 7: val_loss 0.086218536502471, val_acc 0.9716312056737588
trigger times: 2
end of epoch 8: val_loss 0.07350230697039646, val_acc 0.9769503546099291
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.09062260332532425, val_acc 0.9689716312056738
trigger times: 1
end of epoch 10: val_loss 0.08307978761470887, val_acc 0.9716312056737588
trigger times: 2
end of epoch 11: val_loss 0.13634940040356275, val_acc 0.9671985815602837
trigger times: 3
end of epoch 12: val_loss 0.1287009058911327, val_acc 0.9636524822695035
trigger times: 4
end of epoch 13: val_loss 0.13713317162151933, val_acc 0.9707446808510638
trigger times: 5
end of epoch 14: val_loss 0.18928350147654222, val_acc 0.9645390070921985
trigger times: 6
end of epoch 15: val_loss 0.0894662388466251, val_acc 0.9725177304964538
trigger times: 7
end of epoch 16: val_loss 0.12644345643757507, val_acc 0.9698581560283688
trigger times: 8
end of epoch 17: val_loss 0.07041538917128952, val_acc 0.9698581560283688
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.16564260719312335, val_acc 0.9680851063829787
trigger times: 1
end of epoch 19: val_loss 0.07007193430700204, val_acc 0.975177304964539
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.1633460193774142, val_acc 0.9698581560283688
trigger times: 1
end of epoch 21: val_loss 0.09574804100678122, val_acc 0.9716312056737588
trigger times: 2
end of epoch 22: val_loss 0.10227401481818205, val_acc 0.974290780141844
trigger times: 3
end of epoch 23: val_loss 0.1871742841846266, val_acc 0.9663120567375887
trigger times: 4
end of epoch 24: val_loss 0.1084009840446349, val_acc 0.974290780141844
trigger times: 5
end of epoch 25: val_loss 0.12190717074497576, val_acc 0.9671985815602837
trigger times: 6
end of epoch 26: val_loss 0.11575300513682374, val_acc 0.9725177304964538
trigger times: 7
end of epoch 27: val_loss 0.08392511765840778, val_acc 0.9787234042553191
trigger times: 8
end of epoch 28: val_loss 0.10473353158600485, val_acc 0.9725177304964538
trigger times: 9
end of epoch 29: val_loss 0.23269788740524477, val_acc 0.9698581560283688
trigger times: 10
Early stopping.
0 -280.76528465747833 -181.59621720143082
1 -237.82286566495895 -151.71751520071632
2 -236.86518359184265 -147.47910081225547
3 -212.82428812980652 -133.76056380009578
4 -159.0664659049362 -117.07649082709086
5 -165.47705044597387 -111.99728318755238
6 -137.81412547826767 -96.08342159541479
7 -97.87214525975287 -67.56114110862306
8 -89.78621123824269 -64.68081889915388
9 -84.33559868764132 -61.275273342250266
10 -78.06593216024339 -55.44719879154653
11 -63.08586870320141 -47.635037654385464
12 -70.11327158659697 -42.93517995772881
13 -67.42322006076574 -34.935912968324274
14 -18.778890639310703 1.293113865583746
15 16.31920176371932 37.008960077453615
16 74.80256094038486 77.62286246422622
17 76.91157236881554 79.3595955035673
18 89.43653218448162 94.19977594187397
19 113.60750274546444 94.60034688743727
20 90.39541684836149 94.84413775624779
21 92.94475062191486 95.93127096706885
22 93.42259525880218 97.31185243955532
23 109.3990475051105 97.75965500483397
24 99.60920031741261 99.706470840702
25 98.26533000357449 100.73191429849227
26 100.2987374374643 100.75917839256617
27 110.12346251774579 105.18568330899544
28 127.532315406017 107.05778838409489
29 114.65666356496513 107.70203301945793
30 118.14959608018398 108.28588915868846
31 118.52984483446926 111.56177998492792
32 120.8714574277401 111.89996322956685
33 123.59699867106974 112.05237740392577
34 123.4112386573106 113.21987800793073
35 118.34884150512516 113.54624416393074
36 130.96665224060416 113.91500729297944
37 128.9595498405397 116.07692970774649
38 131.43208680860698 116.10590321293648
39 127.86595416767523 117.22159336730809
40 137.04053489211947 119.00744636937388
41 137.25576640293002 119.6581196203716
42 139.9610730241984 122.2662021465837
43 142.13826161436737 123.74388483204041
44 145.79671174753457 124.11350914107612
45 140.28882214892656 125.19854714485804
46 153.31849298160523 128.07431344844193
47 149.3850325345993 129.05523536383924
train accuracy: 0.9913618468830027
validation accuracy: 0.9698581560283688
