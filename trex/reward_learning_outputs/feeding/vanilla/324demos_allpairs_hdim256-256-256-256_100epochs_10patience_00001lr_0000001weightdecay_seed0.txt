demos: (480, 200, 40)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=40, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=256, bias=True)
  (4): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 208128
Number of trainable paramters: 208128
device: cuda:1
end of epoch 0: val_loss 0.04919147770251936, val_acc 0.9796099290780141
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.030520497516575092, val_acc 0.9902482269503546
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.042540934046025256, val_acc 0.9804964539007093
trigger times: 1
end of epoch 3: val_loss 0.05291103362165638, val_acc 0.9822695035460993
trigger times: 2
end of epoch 4: val_loss 0.029279805239183283, val_acc 0.9858156028368794
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.029425630911628475, val_acc 0.9875886524822695
trigger times: 1
end of epoch 6: val_loss 0.041223838341759, val_acc 0.9858156028368794
trigger times: 2
end of epoch 7: val_loss 0.0396252876214482, val_acc 0.9849290780141844
trigger times: 3
end of epoch 8: val_loss 0.03560782953597592, val_acc 0.9867021276595744
trigger times: 4
end of epoch 9: val_loss 0.046150073160820385, val_acc 0.9840425531914894
trigger times: 5
end of epoch 10: val_loss 0.03475743603047865, val_acc 0.9840425531914894
trigger times: 6
end of epoch 11: val_loss 0.039295881498005836, val_acc 0.9858156028368794
trigger times: 7
end of epoch 12: val_loss 0.029170309302183315, val_acc 0.9893617021276596
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.035116518359453466, val_acc 0.9902482269503546
trigger times: 1
end of epoch 14: val_loss 0.057315554833386254, val_acc 0.9831560283687943
trigger times: 2
end of epoch 15: val_loss 0.023125776926853685, val_acc 0.9893617021276596
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.023200995377336883, val_acc 0.9902482269503546
trigger times: 1
end of epoch 17: val_loss 0.03511919241445132, val_acc 0.9875886524822695
trigger times: 2
end of epoch 18: val_loss 0.038262942162139886, val_acc 0.9867021276595744
trigger times: 3
end of epoch 19: val_loss 0.027577626212638196, val_acc 0.9893617021276596
trigger times: 4
end of epoch 20: val_loss 0.02446617966257195, val_acc 0.9893617021276596
trigger times: 5
end of epoch 21: val_loss 0.028288934239979583, val_acc 0.9911347517730497
trigger times: 6
end of epoch 22: val_loss 0.03286780388271969, val_acc 0.9884751773049646
trigger times: 7
end of epoch 23: val_loss 0.034404185891606855, val_acc 0.9858156028368794
trigger times: 8
end of epoch 24: val_loss 0.028711752081353667, val_acc 0.9902482269503546
trigger times: 9
end of epoch 25: val_loss 0.03321729005116789, val_acc 0.9867021276595744
trigger times: 10
Early stopping.
0 -222.22486498951912 -199.20912126544414
1 -191.51041665673256 -181.59621720143082
2 -164.66745829582214 -164.41151030608535
3 -164.31762981414795 -161.32605513368566
4 -144.18235682323575 -151.5634741258087
5 -139.95453849434853 -133.76056380009578
6 -120.93010933697224 -128.90054905506827
7 -116.94212295906618 -119.90296557087576
8 -118.29820181429386 -119.33378180391257
9 -111.35668887663633 -114.59035433631308
10 -92.59766480373219 -108.00922275200763
11 -63.06610405325773 -89.08705457223661
12 -60.90627864864655 -80.25399517559948
13 -50.002213838044554 -75.17483043591994
14 -49.76487823680509 -73.76362433676923
15 -34.33493638690561 -66.19170494681471
16 -35.47665856144158 -62.40117935748126
17 -14.717440846608952 -25.99016205153756
18 37.99154718508362 -0.07804593106747401
19 111.38394416077062 38.15087589300597
20 107.88316850416595 43.68426122331964
21 144.32937302580103 63.953351629706646
22 149.05530029415968 64.53271247267233
23 151.08366318797925 74.60390063743114
24 164.65174042922445 81.25241929740568
25 173.18395497463644 85.92277304180317
26 183.1508758754935 92.69937930010626
27 190.9003362711519 98.94672478897846
28 193.6179786294233 99.706470840702
29 197.25869554386009 103.1234668642953
30 200.41327183239628 105.49846404467546
31 198.09584362257738 106.70740032169537
32 205.52904434187803 110.39507157450213
33 210.7536656757875 110.82324768128066
34 205.95273762365105 111.56177998492792
35 209.54321571136825 112.21095617362457
36 209.42558172979625 112.73015958534099
37 213.0644324880559 113.05192791883351
38 216.61920543230372 117.07619652114646
39 217.19850934261922 117.34027665325667
40 215.5044035965111 117.83642486880923
41 221.87143548135646 119.20032127761252
42 223.5107576995797 119.43302815589446
43 221.27157499664463 120.15025122129617
44 222.51500524918083 122.63062574324714
45 231.47173965512775 125.78956526488335
46 226.92211809684522 125.85283998079782
47 228.5914862388745 126.58765053715479
train accuracy: 0.991075182509651
validation accuracy: 0.9867021276595744
