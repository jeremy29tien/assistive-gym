demos: (480, 200, 19)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 93096
num train_labels 93096
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 136960
Number of trainable paramters: 136960
device: cuda:0
end of epoch 0: val_loss 0.2181075466005956, val_acc 0.9069148936170213
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.2390939193209015, val_acc 0.9148936170212766
trigger times: 1
end of epoch 2: val_loss 0.21835800581178288, val_acc 0.9078014184397163
trigger times: 2
end of epoch 3: val_loss 0.22113174303104552, val_acc 0.9060283687943262
trigger times: 3
end of epoch 4: val_loss 0.20863907024772305, val_acc 0.9202127659574468
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.213546912305061, val_acc 0.9148936170212766
trigger times: 1
end of epoch 6: val_loss 0.23033853946234528, val_acc 0.9095744680851063
trigger times: 2
end of epoch 7: val_loss 0.23315543991594204, val_acc 0.9078014184397163
trigger times: 3
end of epoch 8: val_loss 0.23320672713580373, val_acc 0.9131205673758865
trigger times: 4
end of epoch 9: val_loss 0.24422484197403346, val_acc 0.9131205673758865
trigger times: 5
end of epoch 10: val_loss 0.22355708785454084, val_acc 0.9193262411347518
trigger times: 6
end of epoch 11: val_loss 0.2382157092393882, val_acc 0.9140070921985816
trigger times: 7
end of epoch 12: val_loss 0.2102658317496639, val_acc 0.9148936170212766
trigger times: 8
end of epoch 13: val_loss 0.22358930166656849, val_acc 0.9175531914893617
trigger times: 9
end of epoch 14: val_loss 0.22274408614203225, val_acc 0.9131205673758865
trigger times: 10
Early stopping.
0 -45.795070596039295 -103.1521388660611
1 -42.97420707345009 -87.66500551980913
2 -38.53378088772297 -84.84678494475872
3 -40.38785420358181 -81.84756319767409
4 -37.52693570405245 -81.40890616600856
5 -42.36182700097561 -80.85477622402837
6 -40.5697403550148 -80.48391381073206
7 -39.81693532690406 -80.25156008912637
8 -28.058423951501027 -33.34809776864483
9 -27.84712958568707 -20.418009315237114
10 -28.21045910124667 -20.131276401477507
11 -27.857605430297554 -15.362938437720853
12 -29.155806988943368 -1.8963398599782397
13 -29.192894470645115 -0.5728421252635456
14 -25.111696060746908 4.383180415256293
15 -26.038557858439162 6.8392346078963
16 -28.26855620718561 6.841662837067183
17 -25.03965986194089 27.01133577467053
18 -23.82115539908409 33.68434584390655
19 -26.180548103991896 35.05308871945419
20 -24.9864723905921 50.52841485696464
21 -23.336533601628616 56.635725849291575
22 -24.305383834056556 57.76744549995627
23 -23.55844572163187 69.90236244353298
24 -23.730056384578347 81.30023532085879
25 -19.200924600940198 92.20740196031777
26 -22.348551786970347 92.26562045392546
27 -22.352732258383185 101.50661112827956
28 -24.863026656676084 125.53777532052302
29 -20.783648228971288 157.73723206248386
30 -21.42322958749719 158.4543688649787
31 -21.245533735724166 169.19923674906562
32 -18.12855192250572 185.35424212127486
33 -23.11337892129086 204.5598807600962
34 -19.114001313457265 205.26719391532652
35 -11.575780549785122 214.6263040548001
36 -19.595156321069226 233.63745376530167
37 -15.516675704624504 266.3586992205008
38 -14.477110178908333 284.9323058364518
39 -17.29767437116243 291.32952200448
40 -16.011151796672493 320.0946312511725
41 -12.647912403568625 324.0197976082746
42 -16.048477393575013 333.0407029156643
43 -13.80145347584039 361.84626739841684
44 -9.634517430327833 390.7517184882046
45 -11.70765539095737 400.0413426933392
46 -11.928850296884775 403.13796958654353
47 -17.42594479979016 423.1355871564197
train accuracy: 0.9655946549798058
validation accuracy: 0.9131205673758865
