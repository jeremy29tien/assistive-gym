demos: (480, 200, 19)
demo_rewards: (480,)
sorted_train_rewards: [-1.68281407e+02 -1.45727567e+02 -1.39123043e+02 -1.25906495e+02
 -1.23737226e+02 -1.18441257e+02 -1.15454518e+02 -1.15278652e+02
 -1.14763156e+02 -1.14085096e+02 -1.12885489e+02 -1.11179560e+02
 -1.10579609e+02 -1.10248073e+02 -1.06969949e+02 -1.05880618e+02
 -1.03152139e+02 -1.01565908e+02 -1.00495946e+02 -1.00348353e+02
 -9.79572956e+01 -9.72257679e+01 -9.60581398e+01 -9.47527923e+01
 -9.42721572e+01 -9.39936009e+01 -9.08470847e+01 -8.99058909e+01
 -8.98050349e+01 -8.92193126e+01 -8.88554189e+01 -8.76650055e+01
 -8.71119989e+01 -8.58361739e+01 -8.48467849e+01 -8.42859587e+01
 -8.40914856e+01 -8.33560899e+01 -8.18475632e+01 -8.14466377e+01
 -8.14089062e+01 -8.10067480e+01 -8.08547762e+01 -8.04839138e+01
 -8.02515601e+01 -7.86319117e+01 -7.84173816e+01 -7.80515779e+01
 -7.63592934e+01 -7.50296954e+01 -7.48307224e+01 -7.41288702e+01
 -7.35268215e+01 -7.30055356e+01 -7.26136252e+01 -7.23199016e+01
 -7.10097190e+01 -7.02497554e+01 -6.98788242e+01 -6.90105689e+01
 -6.88032812e+01 -6.82574483e+01 -6.81709241e+01 -6.78589144e+01
 -6.77543594e+01 -6.77033281e+01 -6.58589998e+01 -6.53374190e+01
 -6.49762184e+01 -5.92349209e+01 -5.78572803e+01 -5.71589430e+01
 -5.50587300e+01 -5.45877794e+01 -5.38140519e+01 -5.23998422e+01
 -5.20708438e+01 -5.03190683e+01 -4.94482556e+01 -4.87197115e+01
 -4.55497329e+01 -4.52135310e+01 -4.44901949e+01 -4.39133519e+01
 -4.35323363e+01 -4.26845918e+01 -4.13106519e+01 -3.93647734e+01
 -3.84347912e+01 -3.72825558e+01 -3.64541601e+01 -3.63795012e+01
 -3.56366006e+01 -3.51896507e+01 -3.36956904e+01 -3.33480978e+01
 -2.77930868e+01 -2.71949370e+01 -2.63456513e+01 -2.26604777e+01
 -2.17096738e+01 -2.11615803e+01 -2.04180093e+01 -2.01425951e+01
 -2.01312764e+01 -2.00548875e+01 -1.87912468e+01 -1.74207898e+01
 -1.68901975e+01 -1.61143764e+01 -1.53629384e+01 -1.44711839e+01
 -1.39267768e+01 -1.12508710e+01 -7.28306957e+00 -6.03798033e+00
 -4.48405644e+00 -1.63608121e+00 -1.43319012e+00 -1.25215708e+00
 -5.72842125e-01  2.69474090e-01  8.48755488e-01  9.71479122e-01
  3.13137341e+00  4.09388103e+00  4.38318042e+00  6.23806049e+00
  6.84166284e+00  8.10954932e+00  8.62917691e+00  1.29952266e+01
  1.34336148e+01  1.51800256e+01  1.58979399e+01  1.65923688e+01
  1.89453788e+01  1.94910199e+01  1.94916920e+01  2.05305881e+01
  2.15104917e+01  2.41170029e+01  2.43710703e+01  2.58660332e+01
  2.60724768e+01  2.74672520e+01  2.75751580e+01  2.85285750e+01
  3.05892332e+01  3.27442577e+01  3.36843458e+01  3.37899159e+01
  3.41611587e+01  3.42153602e+01  3.48663961e+01  3.50530887e+01
  3.72456187e+01  3.99267393e+01  3.99602846e+01  4.23875378e+01
  4.39159825e+01  4.42327291e+01  4.48500197e+01  4.83776686e+01
  4.88736658e+01  5.05284149e+01  5.53688861e+01  5.56692378e+01
  5.66357258e+01  5.77674455e+01  5.78923918e+01  5.86309323e+01
  5.93436812e+01  6.02906417e+01  6.47458201e+01  6.49603340e+01
  6.68749790e+01  6.76039372e+01  6.78694515e+01  6.85395993e+01
  6.85552576e+01  6.99023624e+01  7.00722782e+01  7.10566011e+01
  7.18958063e+01  7.42631002e+01  7.64437617e+01  7.80250544e+01
  8.00743291e+01  8.04122550e+01  8.13002353e+01  8.18901469e+01
  8.32565101e+01  8.33459185e+01  8.87279953e+01  9.04900629e+01
  9.13388441e+01  9.22074020e+01  9.22656205e+01  9.30086833e+01
  9.30344412e+01  9.73584524e+01  9.78474792e+01  9.87173322e+01
  9.92848455e+01  1.01506611e+02  1.02399087e+02  1.03162159e+02
  1.03450957e+02  1.03768508e+02  1.04406383e+02  1.07092483e+02
  1.07710068e+02  1.11625387e+02  1.11924895e+02  1.13547690e+02
  1.14761007e+02  1.16179984e+02  1.16837433e+02  1.21326132e+02
  1.25537775e+02  1.33048584e+02  1.33573303e+02  1.36511921e+02
  1.37758285e+02  1.39131419e+02  1.41638120e+02  1.41844812e+02
  1.42424512e+02  1.43496609e+02  1.43510881e+02  1.46199060e+02
  1.46556236e+02  1.48407567e+02  1.48912536e+02  1.49165606e+02
  1.50802776e+02  1.51584630e+02  1.57633889e+02  1.57737232e+02
  1.58454369e+02  1.58956502e+02  1.63267115e+02  1.64188964e+02
  1.64960890e+02  1.66294281e+02  1.68205106e+02  1.68399273e+02
  1.69199237e+02  1.70621405e+02  1.71268575e+02  1.71509314e+02
  1.72966312e+02  1.74644664e+02  1.74710408e+02  1.79019423e+02
  1.81095791e+02  1.82868270e+02  1.82965600e+02  1.85354242e+02
  1.89562752e+02  1.89574834e+02  1.89672310e+02  1.91027077e+02
  2.01784542e+02  2.04149936e+02  2.04559881e+02  2.05267194e+02
  2.06118964e+02  2.08133270e+02  2.09294404e+02  2.09568207e+02
  2.10601509e+02  2.13399135e+02  2.14626304e+02  2.15422119e+02
  2.21445846e+02  2.22947849e+02  2.25145886e+02  2.26628167e+02
  2.28189327e+02  2.28893032e+02  2.33637454e+02  2.35044073e+02
  2.36284670e+02  2.36684500e+02  2.37351876e+02  2.37904515e+02
  2.39673843e+02  2.40087383e+02  2.40746217e+02  2.44455371e+02
  2.45745089e+02  2.47396543e+02  2.49133124e+02  2.50199516e+02
  2.50956709e+02  2.51005724e+02  2.52138755e+02  2.56224103e+02
  2.56233725e+02  2.57266362e+02  2.58179264e+02  2.60083093e+02
  2.60765499e+02  2.62492064e+02  2.63648761e+02  2.65056624e+02
  2.66105443e+02  2.66165893e+02  2.66358699e+02  2.66927189e+02
  2.73760950e+02  2.75046082e+02  2.76765168e+02  2.77043285e+02
  2.78210634e+02  2.78449570e+02  2.82206256e+02  2.82325028e+02
  2.84101176e+02  2.84932306e+02  2.86464187e+02  2.86772706e+02
  2.86931290e+02  2.87806515e+02  2.88479049e+02  2.88826115e+02
  2.89886738e+02  2.90633014e+02  2.90673531e+02  2.91437334e+02
  2.91755546e+02  2.92387265e+02  2.93628281e+02  2.94666653e+02
  2.94932601e+02  2.96188315e+02  2.96502288e+02  2.96822454e+02
  2.97975474e+02  2.98212430e+02  3.00668612e+02  3.01696582e+02
  3.01876822e+02  3.02112561e+02  3.03572646e+02  3.04409455e+02
  3.04449763e+02  3.06727103e+02  3.07812719e+02  3.08096964e+02
  3.10210664e+02  3.11984654e+02  3.12755351e+02  3.14574421e+02
  3.17861441e+02  3.19020009e+02  3.20094631e+02  3.20899374e+02
  3.21383177e+02  3.23073902e+02  3.23170946e+02  3.23359694e+02
  3.23678838e+02  3.24019798e+02  3.26023906e+02  3.26326863e+02
  3.27089227e+02  3.27093677e+02  3.29794865e+02  3.33040703e+02
  3.34824512e+02  3.34942982e+02  3.38064250e+02  3.41586114e+02
  3.42427604e+02  3.43392884e+02  3.45169167e+02  3.45467793e+02
  3.50754649e+02  3.51098770e+02  3.52589335e+02  3.52938639e+02
  3.53122732e+02  3.57327858e+02  3.57432774e+02  3.60404347e+02
  3.60494199e+02  3.61846267e+02  3.63639297e+02  3.65528706e+02
  3.66902709e+02  3.67026251e+02  3.67221496e+02  3.70665008e+02
  3.71474657e+02  3.72391756e+02  3.74044271e+02  3.75975458e+02
  3.76103509e+02  3.78576962e+02  3.79102861e+02  3.81301733e+02
  3.83305336e+02  3.84246014e+02  3.89847138e+02  3.89945653e+02
  3.90751718e+02  3.94941529e+02  3.96037631e+02  4.00041343e+02
  4.03137970e+02  4.06941403e+02  4.07718412e+02  4.13881992e+02
  4.14385640e+02  4.14433011e+02  4.15231967e+02  4.18896984e+02
  4.19530524e+02  4.20454152e+02  4.20949962e+02  4.23135587e+02
  4.29050163e+02  4.31102150e+02  4.34196834e+02  4.34808157e+02
  4.44554017e+02  4.45936030e+02  4.67300177e+02  4.73149208e+02]
sorted_val_rewards: [-133.97354737 -106.29807636 -106.26498585  -97.24485765  -78.93406316
  -65.00607092  -64.31517893  -63.24633649  -63.1014303   -57.66105273
  -56.30990465  -36.57284828  -34.92774699  -34.48765021  -29.59331475
  -21.77695065   -1.89633986    0.53212097    4.23223568    6.83923461
   19.62253484   27.01133577   55.12239522   55.92026357   65.16806407
   87.34870755  105.72097092  148.38223634  158.93814401  194.95027356
  199.51892684  209.04733791  224.80518376  242.29822404  247.42473325
  278.92635304  290.00250527  291.329522    306.38417501  317.59854966
  330.08344617  331.71313838  336.26436621  360.23062633  368.30963475
  368.8718416   370.49736551  373.90298986]
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:1
end of epoch 0: val_loss 0.20511348995555023, val_acc 0.9095744680851063
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.15702337032773406, val_acc 0.9237588652482269
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.1554928624468451, val_acc 0.925531914893617
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.15023577319643758, val_acc 0.9308510638297872
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.16874329355960319, val_acc 0.9281914893617021
trigger times: 1
end of epoch 5: val_loss 0.1616089691903652, val_acc 0.9326241134751773
trigger times: 2
end of epoch 6: val_loss 0.15716388656441457, val_acc 0.9308510638297872
trigger times: 3
end of epoch 7: val_loss 0.1493409320913315, val_acc 0.9352836879432624
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.15307013276104817, val_acc 0.9335106382978723
trigger times: 1
end of epoch 9: val_loss 0.15991338798742183, val_acc 0.9379432624113475
trigger times: 2
end of epoch 10: val_loss 0.15937731977699018, val_acc 0.9335106382978723
trigger times: 3
end of epoch 11: val_loss 0.16133933562879973, val_acc 0.9343971631205674
trigger times: 4
end of epoch 12: val_loss 0.15939338298860392, val_acc 0.9361702127659575
trigger times: 5
end of epoch 13: val_loss 0.15746444540088483, val_acc 0.9352836879432624
trigger times: 6
end of epoch 14: val_loss 0.15382670112972055, val_acc 0.9414893617021277
trigger times: 7
end of epoch 15: val_loss 0.16010923800344462, val_acc 0.9397163120567376
trigger times: 8
end of epoch 16: val_loss 0.1633781448441852, val_acc 0.9432624113475178
trigger times: 9
end of epoch 17: val_loss 0.154400446009149, val_acc 0.9432624113475178
trigger times: 10
Early stopping.
0 -61.901967983692884 -133.973547371322
1 -51.07675499096513 -106.29807635708414
2 -39.82127503864467 -106.26498585222654
3 -46.28163554519415 -97.24485765376211
4 -44.224017802625895 -78.93406316283158
5 -38.78716791793704 -65.00607091797544
6 -38.98710383661091 -64.31517892724126
7 -39.61330706253648 -63.24633649146097
8 -36.602788865566254 -63.101430300225374
9 -37.83316634176299 -57.66105272983283
10 -35.529335394501686 -56.30990465208505
11 -27.743619577027857 -36.57284828127824
12 -24.165458951843902 -34.92774699423874
13 -30.252521871298086 -34.487650213835884
14 -24.086321313865483 -29.593314750499733
15 -27.345739421551116 -21.776950647506872
16 -27.733893661061302 -1.8963398599782397
17 -23.853183092782274 0.5321209736499255
18 -26.289380124071613 4.232235681838149
19 -22.572116635739803 6.8392346078963
20 -22.470425592036918 19.622534836185636
21 -24.345602251240052 27.01133577467053
22 -22.122221841476858 55.12239522211616
23 -19.677159453276545 55.92026357237822
24 -20.759391960222274 65.1680640695934
25 -16.459708833601326 87.3487075539824
26 -18.32618169928901 105.72097091834365
27 -16.696788957808167 148.38223634397093
28 -15.052951413148548 158.9381440127044
29 -13.424718664376996 194.9502735624309
30 -14.60960621025879 199.51892684347123
31 -12.184033762314357 209.04733790839154
32 -13.783403371926397 224.80518375748704
33 -11.194112272991333 242.29822403524432
34 -13.644790665595792 247.4247332535776
35 -8.811907489667647 278.9263530439534
36 -6.1094629967119545 290.0025052732212
37 -13.188020263565704 291.32952200448
38 -8.426794869825244 306.3841750088036
39 -10.359995240694843 317.59854966426605
40 -4.216223005671054 330.083446167866
41 -6.4005586333805695 331.71313838439016
42 -7.469955020525958 336.2643662087101
43 -5.641515880008228 360.2306263300218
44 -5.71074153855443 368.3096347545002
45 -9.471218718637829 368.87184160352797
46 -7.206883927457966 370.4973655070732
47 -7.190970354364254 373.9029898572837
train accuracy: 0.976436188510492
validation accuracy: 0.9432624113475178
