demos: (480, 200, 19)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 93096
num train_labels 93096
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 136960
Number of trainable paramters: 136960
device: cuda:0
end of epoch 0: val_loss 0.1572555182423586, val_acc 0.9308510638297872
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.13215386897560577, val_acc 0.9379432624113475
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.15913590000790664, val_acc 0.9290780141843972
trigger times: 1
end of epoch 3: val_loss 0.14903326545303358, val_acc 0.9335106382978723
trigger times: 2
end of epoch 4: val_loss 0.14895175595161791, val_acc 0.9317375886524822
trigger times: 3
end of epoch 5: val_loss 0.15337538903787168, val_acc 0.9361702127659575
trigger times: 4
end of epoch 6: val_loss 0.15115357775068514, val_acc 0.9317375886524822
trigger times: 5
end of epoch 7: val_loss 0.13910118014465717, val_acc 0.9361702127659575
trigger times: 6
end of epoch 8: val_loss 0.14092275695300216, val_acc 0.9361702127659575
trigger times: 7
end of epoch 9: val_loss 0.1568955365665197, val_acc 0.9361702127659575
trigger times: 8
end of epoch 10: val_loss 0.13747056464668184, val_acc 0.9370567375886525
trigger times: 9
end of epoch 11: val_loss 0.1331441929107718, val_acc 0.9414893617021277
trigger times: 10
Early stopping.
0 -42.50110375881195 -133.973547371322
1 -36.71370830386877 -106.29807635708414
2 -31.238522365689278 -106.26498585222654
3 -33.712688095867634 -97.24485765376211
4 -31.28196831792593 -78.93406316283158
5 -27.339091550558805 -65.00607091797544
6 -27.990696507506073 -64.31517892724126
7 -27.414399098604918 -63.24633649146097
8 -24.502369585447013 -63.101430300225374
9 -26.536516387946904 -57.66105272983283
10 -24.320742331445217 -56.30990465208505
11 -18.95896205189638 -36.57284828127824
12 -15.717426109593362 -34.92774699423874
13 -18.806061501963995 -34.487650213835884
14 -15.446286154794507 -29.593314750499733
15 -16.99842752830591 -21.776950647506872
16 -17.474949474679306 -1.8963398599782397
17 -15.979900566395372 0.5321209736499255
18 -16.57423060189467 4.232235681838149
19 -14.643484760541469 6.8392346078963
20 -15.382653886219487 19.622534836185636
21 -15.783284297445789 27.01133577467053
22 -14.467645964003168 55.12239522211616
23 -12.245632771868259 55.92026357237822
24 -14.152389183524065 65.1680640695934
25 -10.661969494190998 87.3487075539824
26 -11.330146713531576 105.72097091834365
27 -9.865601957892068 148.38223634397093
28 -9.613317691488191 158.9381440127044
29 -8.375236892374232 194.9502735624309
30 -8.130816552322358 199.51892684347123
31 -6.836323234136216 209.04733790839154
32 -9.48782165022567 224.80518375748704
33 -5.478027815348469 242.29822403524432
34 -8.76967799931299 247.4247332535776
35 -4.867148243705742 278.9263530439534
36 -2.9273041824344546 290.0025052732212
37 -7.845007263007574 291.32952200448
38 -2.9224253012798727 306.3841750088036
39 -5.879800063208677 317.59854966426605
40 -1.9179889606311917 330.083446167866
41 -1.869336968054995 331.71313838439016
42 -3.255554390605539 336.2643662087101
43 -2.7930889257695526 360.2306263300218
44 -1.4106563065433875 368.3096347545002
45 -5.244758702348918 368.87184160352797
46 -3.174476114450954 370.4973655070732
47 -3.416872755740769 373.9029898572837
train accuracy: 0.959063762138008
validation accuracy: 0.9414893617021277
