demos: (480, 200, 20)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=20, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 11008
Number of trainable paramters: 11008
device: cuda:2
end of epoch 0: val_loss 0.18702624057616696, val_acc 0.9264184397163121
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.13982887421455642, val_acc 0.9450354609929078
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.14066343760179126, val_acc 0.9450354609929078
trigger times: 1
end of epoch 3: val_loss 0.11812095601315906, val_acc 0.9547872340425532
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.10444858243188367, val_acc 0.9547872340425532
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.10541325745399874, val_acc 0.9592198581560284
trigger times: 1
end of epoch 6: val_loss 0.10015690655465301, val_acc 0.9592198581560284
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.0914255800324258, val_acc 0.9609929078014184
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.09814767950482724, val_acc 0.9583333333333334
trigger times: 1
end of epoch 9: val_loss 0.0880577728898748, val_acc 0.9636524822695035
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.08710710758453506, val_acc 0.9645390070921985
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.08983810818208451, val_acc 0.9636524822695035
trigger times: 1
end of epoch 12: val_loss 0.09288121820251453, val_acc 0.9609929078014184
trigger times: 2
end of epoch 13: val_loss 0.0879248122730038, val_acc 0.9680851063829787
trigger times: 3
end of epoch 14: val_loss 0.08886094166810445, val_acc 0.9716312056737588
trigger times: 4
end of epoch 15: val_loss 0.08139517856596343, val_acc 0.9689716312056738
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.08047382054570362, val_acc 0.9689716312056738
trigger times: 0
saving model weights...
end of epoch 17: val_loss 0.07489038780615351, val_acc 0.9716312056737588
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.07531613672949454, val_acc 0.9716312056737588
trigger times: 1
end of epoch 19: val_loss 0.08050104651637148, val_acc 0.9680851063829787
trigger times: 2
end of epoch 20: val_loss 0.07408460537705139, val_acc 0.9698581560283688
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.08021050462188094, val_acc 0.9707446808510638
trigger times: 1
end of epoch 22: val_loss 0.07344044886787614, val_acc 0.975177304964539
trigger times: 0
saving model weights...
end of epoch 23: val_loss 0.08499256569535583, val_acc 0.9725177304964538
trigger times: 1
end of epoch 24: val_loss 0.0708467935615976, val_acc 0.973404255319149
trigger times: 0
saving model weights...
end of epoch 25: val_loss 0.07539699326597891, val_acc 0.9716312056737588
trigger times: 1
end of epoch 26: val_loss 0.07433641499918184, val_acc 0.9716312056737588
trigger times: 2
end of epoch 27: val_loss 0.08605397809222975, val_acc 0.9716312056737588
trigger times: 3
end of epoch 28: val_loss 0.07050448169379127, val_acc 0.974290780141844
trigger times: 0
saving model weights...
end of epoch 29: val_loss 0.07273614597823949, val_acc 0.976063829787234
trigger times: 1
end of epoch 30: val_loss 0.07211556452550526, val_acc 0.9787234042553191
trigger times: 2
end of epoch 31: val_loss 0.06979651554648925, val_acc 0.9787234042553191
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.07358330360496078, val_acc 0.974290780141844
trigger times: 1
end of epoch 33: val_loss 0.07505573687514194, val_acc 0.9778368794326241
trigger times: 2
end of epoch 34: val_loss 0.0736329290253104, val_acc 0.9716312056737588
trigger times: 3
end of epoch 35: val_loss 0.07696976170256875, val_acc 0.974290780141844
trigger times: 4
end of epoch 36: val_loss 0.07561813013880514, val_acc 0.973404255319149
trigger times: 5
end of epoch 37: val_loss 0.07657814541570594, val_acc 0.9787234042553191
trigger times: 6
end of epoch 38: val_loss 0.07505652982028443, val_acc 0.9804964539007093
trigger times: 7
end of epoch 39: val_loss 0.07759769244427582, val_acc 0.9787234042553191
trigger times: 8
end of epoch 40: val_loss 0.0777666714677358, val_acc 0.974290780141844
trigger times: 9
end of epoch 41: val_loss 0.08113571333751003, val_acc 0.9769503546099291
trigger times: 10
Early stopping.
0 -40.41119560971856 -168.2213375129506
1 -21.395919786999002 -110.68148617269557
2 -27.31153088621795 -105.98216711742712
3 -24.408241467550397 -95.2328412433046
4 -22.344470838899724 -88.00251735515506
5 -19.535171745577827 -85.65708032322048
6 -7.457532875123434 -71.91238572832597
7 -12.257912019267678 -67.75529879555899
8 -17.954392884043045 -60.99752046863584
9 -7.694236662238836 -48.67811963208713
10 -13.817867545469198 -45.84919271975732
11 -7.577584910264704 -44.91517520770373
12 -8.605422805412672 -39.63316561473836
13 -6.3602628405205905 -37.80207708518889
14 -7.158233158872463 -35.46378324392415
15 -9.421920693479478 -28.936245477092292
16 -3.6620373447658494 -3.886671341184772
17 3.3687002575024962 22.340428764253097
18 4.247546521015465 23.11531098032165
19 6.569925376912579 27.626233098474128
20 8.328968942165375 34.206289262552914
21 9.443041675840504 40.855643842278326
22 14.010310503537767 66.34646031672955
23 15.179233266157098 71.97667495908537
24 19.54524555413809 87.99707248149413
25 24.783993332501268 107.67434762982126
26 25.011430634884164 116.49385113358302
27 25.99571804580046 121.59296777968088
28 35.4329737010994 153.90033934252173
29 39.68882710277103 178.96626160807773
30 47.47829365218058 199.51892684347123
31 49.54935620166361 210.7972822026549
32 47.736919584916905 223.43616012483506
33 49.55260286666453 232.69970431509427
34 59.567151886352804 267.7439522684791
35 64.45577635546215 284.59693330368526
36 63.557969553628936 287.6950789987769
37 67.2436736789532 289.6006220027607
38 67.3811166700907 302.440738419582
39 71.237924950663 306.3841750088036
40 73.6248593328055 330.083446167866
41 75.5731139354175 331.71313838439016
42 75.83548795199022 333.9199654934176
43 80.3439467670396 360.2306263300218
44 82.64860925870016 368.3096347545002
45 76.65414905993384 368.87184160352797
46 80.60459020920098 370.4973655070732
47 90.6626921184361 413.13862634749216
train accuracy: 0.9966386554621849
validation accuracy: 0.9769503546099291
