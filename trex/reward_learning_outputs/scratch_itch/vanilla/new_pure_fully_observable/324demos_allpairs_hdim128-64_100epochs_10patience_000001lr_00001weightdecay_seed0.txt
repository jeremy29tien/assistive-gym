demos: (480, 200, 20)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=20, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 11008
Number of trainable paramters: 11008
device: cuda:1
end of epoch 0: val_loss 0.07835028679936774, val_acc 0.9698581560283688
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.05369514062890106, val_acc 0.9787234042553191
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.04604038772436147, val_acc 0.9796099290780141
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.04028102916531222, val_acc 0.9831560283687943
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.03594936252709824, val_acc 0.9858156028368794
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.029804190803557572, val_acc 0.9849290780141844
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.030643218521576737, val_acc 0.9867021276595744
trigger times: 1
end of epoch 7: val_loss 0.024472514903902235, val_acc 0.9893617021276596
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.024035826430173015, val_acc 0.9893617021276596
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.026874873069660177, val_acc 0.9884751773049646
trigger times: 1
end of epoch 10: val_loss 0.022202787822009008, val_acc 0.9920212765957447
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.02461838499636046, val_acc 0.9884751773049646
trigger times: 1
end of epoch 12: val_loss 0.018644153750728042, val_acc 0.9937943262411347
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.02399453396783021, val_acc 0.9884751773049646
trigger times: 1
end of epoch 14: val_loss 0.01766410751616742, val_acc 0.9929078014184397
trigger times: 0
saving model weights...
end of epoch 15: val_loss 0.01658766966991054, val_acc 0.9929078014184397
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.0169135679971474, val_acc 0.9929078014184397
trigger times: 1
end of epoch 17: val_loss 0.023569998435820402, val_acc 0.9875886524822695
trigger times: 2
end of epoch 18: val_loss 0.02021943973702495, val_acc 0.9902482269503546
trigger times: 3
end of epoch 19: val_loss 0.023001382993585312, val_acc 0.9929078014184397
trigger times: 4
end of epoch 20: val_loss 0.018470456734834335, val_acc 0.9911347517730497
trigger times: 5
end of epoch 21: val_loss 0.015070859255039978, val_acc 0.9937943262411347
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.016340533864423332, val_acc 0.9937943262411347
trigger times: 1
end of epoch 23: val_loss 0.018287249419828122, val_acc 0.9893617021276596
trigger times: 2
end of epoch 24: val_loss 0.02358482107443974, val_acc 0.9902482269503546
trigger times: 3
end of epoch 25: val_loss 0.014038869313137707, val_acc 0.9937943262411347
trigger times: 0
saving model weights...
end of epoch 26: val_loss 0.01901082935193438, val_acc 0.9937943262411347
trigger times: 1
end of epoch 27: val_loss 0.018436613860517627, val_acc 0.9937943262411347
trigger times: 2
end of epoch 28: val_loss 0.020288208302510553, val_acc 0.9929078014184397
trigger times: 3
end of epoch 29: val_loss 0.014631224338718584, val_acc 0.9946808510638298
trigger times: 4
end of epoch 30: val_loss 0.019369572040596562, val_acc 0.9920212765957447
trigger times: 5
end of epoch 31: val_loss 0.029397085708617036, val_acc 0.9867021276595744
trigger times: 6
end of epoch 32: val_loss 0.01672173611223087, val_acc 0.9946808510638298
trigger times: 7
end of epoch 33: val_loss 0.01468300532023228, val_acc 0.9946808510638298
trigger times: 8
end of epoch 34: val_loss 0.015286299165978512, val_acc 0.9955673758865248
trigger times: 9
end of epoch 35: val_loss 0.01950283594528635, val_acc 0.9929078014184397
trigger times: 10
Early stopping.
0 -75.41122567281127 -168.2213375129506
1 -36.33931183666573 -110.68148617269557
2 -37.293955095577985 -105.98216711742712
3 -35.85847324458882 -95.2328412433046
4 -28.992016645381227 -88.00251735515506
5 -22.32927503575047 -85.65708032322048
6 -21.174667339771986 -71.91238572832597
7 -12.623965123668313 -67.75529879555899
8 -19.358882062137127 -60.99752046863584
9 -11.224321271525696 -48.67811963208713
10 -12.855131554184482 -45.84919271975732
11 -7.974634944694117 -44.91517520770373
12 -5.698312323424034 -39.63316561473836
13 -4.687792121956591 -37.80207708518889
14 -3.7157160942442715 -35.46378324392415
15 -0.2630084245465696 -28.936245477092292
16 9.12264583830256 -3.886671341184772
17 26.435755485785194 22.340428764253097
18 27.36653289757669 23.11531098032165
19 28.86286521004513 27.626233098474128
20 32.63693042262457 34.206289262552914
21 37.02244478346438 40.855643842278326
22 51.02498963102698 66.34646031672955
23 54.28293154633138 71.97667495908537
24 64.7205285297241 87.99707248149413
25 75.30870296666399 107.67434762982126
26 79.72208045708248 116.49385113358302
27 81.74305827880744 121.59296777968088
28 104.52472914208192 153.90033934252173
29 114.70719258021563 178.96626160807773
30 128.7870200658217 199.51892684347123
31 134.84653458558023 210.7972822026549
32 140.7546922340989 223.43616012483506
33 145.09513620194048 232.69970431509427
34 167.61571305571124 267.7439522684791
35 180.9917279498186 284.59693330368526
36 177.77904279694485 287.6950789987769
37 184.11119688278995 289.6006220027607
38 185.84835701808333 302.440738419582
39 193.97012245329097 306.3841750088036
40 203.8092388906516 330.083446167866
41 206.46087769931182 331.71313838439016
42 207.05596667382633 333.9199654934176
43 223.74797843396664 360.2306263300218
44 223.6565676156897 368.3096347545002
45 222.35765039059334 368.87184160352797
46 226.2007951918058 370.4973655070732
47 252.90734911104664 413.13862634749216
train accuracy: 0.9965218056033329
validation accuracy: 0.9929078014184397
