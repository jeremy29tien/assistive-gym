demos: (480, 200, 43)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=43, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13952
Number of trainable paramters: 13952
device: cuda:1
end of epoch 0: val_loss 0.18349428887658809, val_acc 0.9184397163120568
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.13671419952744812, val_acc 0.9335106382978723
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.13235845325067028, val_acc 0.9388297872340425
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.115557357061274, val_acc 0.950354609929078
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.1077164037804286, val_acc 0.949468085106383
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.09657826067637319, val_acc 0.9547872340425532
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.08973243413743126, val_acc 0.9609929078014184
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.10136757033982716, val_acc 0.9556737588652482
trigger times: 1
end of epoch 8: val_loss 0.0892437565196699, val_acc 0.9618794326241135
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.10350332048880523, val_acc 0.9583333333333334
trigger times: 1
end of epoch 10: val_loss 0.08808310256342153, val_acc 0.9663120567375887
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.09534222939360606, val_acc 0.9654255319148937
trigger times: 1
end of epoch 12: val_loss 0.08425446843998897, val_acc 0.9663120567375887
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.10554323414829576, val_acc 0.9583333333333334
trigger times: 1
end of epoch 14: val_loss 0.07531409905316164, val_acc 0.9671985815602837
trigger times: 0
saving model weights...
end of epoch 15: val_loss 0.0983024346557507, val_acc 0.9627659574468085
trigger times: 1
end of epoch 16: val_loss 0.08044652630833683, val_acc 0.9689716312056738
trigger times: 2
end of epoch 17: val_loss 0.08857157923695567, val_acc 0.9689716312056738
trigger times: 3
end of epoch 18: val_loss 0.07641171527082215, val_acc 0.9698581560283688
trigger times: 4
end of epoch 19: val_loss 0.07505911429656878, val_acc 0.9716312056737588
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.0814318012727708, val_acc 0.975177304964539
trigger times: 1
end of epoch 21: val_loss 0.0792658943076005, val_acc 0.9707446808510638
trigger times: 2
end of epoch 22: val_loss 0.07628384922927334, val_acc 0.9716312056737588
trigger times: 3
end of epoch 23: val_loss 0.07878304762956284, val_acc 0.973404255319149
trigger times: 4
end of epoch 24: val_loss 0.07031673120757792, val_acc 0.975177304964539
trigger times: 0
saving model weights...
end of epoch 25: val_loss 0.08358213131705389, val_acc 0.9707446808510638
trigger times: 1
end of epoch 26: val_loss 0.07910857470221369, val_acc 0.974290780141844
trigger times: 2
end of epoch 27: val_loss 0.08300144672477304, val_acc 0.974290780141844
trigger times: 3
end of epoch 28: val_loss 0.08440252628908364, val_acc 0.974290780141844
trigger times: 4
end of epoch 29: val_loss 0.08781187470239499, val_acc 0.9725177304964538
trigger times: 5
end of epoch 30: val_loss 0.08140932926729282, val_acc 0.975177304964539
trigger times: 6
end of epoch 31: val_loss 0.06375046697352912, val_acc 0.974290780141844
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.07028611475802322, val_acc 0.974290780141844
trigger times: 1
end of epoch 33: val_loss 0.0690866592456137, val_acc 0.976063829787234
trigger times: 2
end of epoch 34: val_loss 0.07858169432520408, val_acc 0.976063829787234
trigger times: 3
end of epoch 35: val_loss 0.0687868941730711, val_acc 0.974290780141844
trigger times: 4
end of epoch 36: val_loss 0.06761711245748436, val_acc 0.9769503546099291
trigger times: 5
end of epoch 37: val_loss 0.07389645239975334, val_acc 0.975177304964539
trigger times: 6
end of epoch 38: val_loss 0.06149510054685029, val_acc 0.9769503546099291
trigger times: 0
saving model weights...
end of epoch 39: val_loss 0.07740286533088495, val_acc 0.974290780141844
trigger times: 1
end of epoch 40: val_loss 0.07139936591148068, val_acc 0.976063829787234
trigger times: 2
end of epoch 41: val_loss 0.07832721181885174, val_acc 0.9769503546099291
trigger times: 3
end of epoch 42: val_loss 0.07737097465970387, val_acc 0.974290780141844
trigger times: 4
end of epoch 43: val_loss 0.07091061509810688, val_acc 0.975177304964539
trigger times: 5
end of epoch 44: val_loss 0.0823005761258564, val_acc 0.9778368794326241
trigger times: 6
end of epoch 45: val_loss 0.0633092165897495, val_acc 0.9787234042553191
trigger times: 7
end of epoch 46: val_loss 0.08598270133969299, val_acc 0.974290780141844
trigger times: 8
end of epoch 47: val_loss 0.07499438303462463, val_acc 0.975177304964539
trigger times: 9
end of epoch 48: val_loss 0.07881819710257848, val_acc 0.9769503546099291
trigger times: 10
Early stopping.
0 -27.474140173755586 -145.12331082373387
1 -33.05952475545928 -107.17968594174054
2 -26.87427426688373 -104.67489450468554
3 -20.02558386232704 -99.07031974158546
4 -24.945078767836094 -98.92664249625004
5 -24.54257187154144 -91.24309144941371
6 -18.930393748916686 -86.65072964683067
7 -24.036791504360735 -81.74222104522855
8 -6.054811770678498 -67.82478213164553
9 -12.107972413708922 -59.32004782787961
10 -9.357655127183534 -56.34472211229872
11 -6.6887028847122565 -34.33684892182313
12 0.9501849100925028 -30.673127930938982
13 -0.95042618172738 -15.759229635997984
14 -7.736318371258676 -3.311417265455704
15 4.692126346635632 18.301188796885995
16 5.611946286633611 21.751558053855526
17 3.22445996850729 22.971428827528815
18 6.70291795101366 33.309940907344725
19 8.797861055936664 34.53183125686171
20 10.80868068523705 39.160251163178806
21 8.58369603860774 43.042914098078846
22 11.66254744422622 43.35347295149878
23 19.49265532492427 77.99858790389989
24 22.12803436757531 91.69607932877577
25 25.13637831271626 112.88632714335564
26 27.161806963966228 116.51597366157048
27 25.718495613255072 118.41227664726173
28 34.17463914351538 144.92700850441372
29 42.448097966611385 190.19508575069554
30 44.087413822999224 199.51892684347123
31 47.90777467517182 211.01854341312932
32 50.3297154312022 234.15916506458083
33 55.741496511764126 261.93529616606156
34 58.605473645962775 267.74727867486297
35 61.486576539464295 290.4988541688117
36 61.74036359821912 290.85699373566075
37 67.91737352369819 306.3841750088036
38 67.3478173923213 308.2257215090803
39 71.99721821397543 330.083446167866
40 73.2635525809601 331.71313838439016
41 74.44235420483164 348.2059036139208
42 73.0447850159835 353.73165286153494
43 75.8753849202767 360.2306263300218
44 77.68348709959537 364.1736706930025
45 77.30846103979275 368.3096347545002
46 74.0704346151324 368.87184160352797
47 77.81175411539152 370.4973655070732
train accuracy: 0.996218487394958
validation accuracy: 0.9769503546099291
