demos: (480, 200, 43)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 52326
num train_labels 52326
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=43, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13952
Number of trainable paramters: 13952
device: cuda:1
end of epoch 0: val_loss 0.07934184225041785, val_acc 0.9707446808510638
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.07280506791104417, val_acc 0.9725177304964538
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.08624030983288583, val_acc 0.973404255319149
trigger times: 1
end of epoch 3: val_loss 0.09830818346444269, val_acc 0.974290780141844
trigger times: 2
end of epoch 4: val_loss 0.06742543856810655, val_acc 0.9778368794326241
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.06510134055536862, val_acc 0.974290780141844
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.06864069034073263, val_acc 0.9787234042553191
trigger times: 1
end of epoch 7: val_loss 0.09604345250281775, val_acc 0.976063829787234
trigger times: 2
end of epoch 8: val_loss 0.08159682243596646, val_acc 0.976063829787234
trigger times: 3
end of epoch 9: val_loss 0.09719046579501961, val_acc 0.9787234042553191
trigger times: 4
end of epoch 10: val_loss 0.09159720425456203, val_acc 0.9787234042553191
trigger times: 5
end of epoch 11: val_loss 0.1223788981896697, val_acc 0.975177304964539
trigger times: 6
end of epoch 12: val_loss 0.10280053983659825, val_acc 0.974290780141844
trigger times: 7
end of epoch 13: val_loss 0.06521148269267356, val_acc 0.9778368794326241
trigger times: 8
end of epoch 14: val_loss 0.0775507252083769, val_acc 0.9778368794326241
trigger times: 9
end of epoch 15: val_loss 0.09734397549196, val_acc 0.9769503546099291
trigger times: 10
Early stopping.
0 -49.320707097649574 -145.12331082373387
1 -27.223239275626838 -107.17968594174054
2 -37.869223557412624 -104.67489450468554
3 -37.96706986799836 -99.07031974158546
4 -35.50529474765062 -98.92664249625004
5 -27.810294622555375 -91.24309144941371
6 -30.793749671429396 -86.65072964683067
7 -32.676982045173645 -81.74222104522855
8 -17.151476110331714 -67.82478213164553
9 -27.69542707176879 -59.32004782787961
10 -25.080168830696493 -56.34472211229872
11 -21.199730147607625 -34.33684892182313
12 -17.045071461237967 -30.673127930938982
13 -10.374439025297761 -15.759229635997984
14 -12.855851165950298 -3.311417265455704
15 -4.506724911276251 18.301188796885995
16 -1.7367931045591831 21.751558053855526
17 -4.399262840859592 22.971428827528815
18 -0.08377306594047695 33.309940907344725
19 0.76961350440979 34.53183125686171
20 4.7317130852025 39.160251163178806
21 4.018452766817063 43.042914098078846
22 6.127822095528245 43.35347295149878
23 16.893902943469584 77.99858790389989
24 20.399901904165745 91.69607932877577
25 27.70325299864635 112.88632714335564
26 29.02790892496705 116.51597366157048
27 28.522103854920715 118.41227664726173
28 37.47568346466869 144.92700850441372
29 51.98719846457243 190.19508575069554
30 53.73883312300313 199.51892684347123
31 61.01903008716181 211.01854341312932
32 65.4493992133066 234.15916506458083
33 74.17835584841669 261.93529616606156
34 77.59348409739323 267.74727867486297
35 83.56450821226463 290.4988541688117
36 84.32875273749232 290.85699373566075
37 91.19207178638317 306.3841750088036
38 91.13931450643577 308.2257215090803
39 98.25557311018929 330.083446167866
40 100.2418060800992 331.71313838439016
41 104.56287438096479 348.2059036139208
42 103.29439862817526 353.73165286153494
43 108.16822564043105 360.2306263300218
44 111.23913235589862 364.1736706930025
45 107.68381216283888 368.3096347545002
46 104.67170544527471 368.87184160352797
47 109.39837428182364 370.4973655070732
train accuracy: 0.9941329358254023
validation accuracy: 0.9769503546099291
