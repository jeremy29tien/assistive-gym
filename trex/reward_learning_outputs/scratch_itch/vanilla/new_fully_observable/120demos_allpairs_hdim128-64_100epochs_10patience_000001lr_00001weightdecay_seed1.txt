demos: (480, 200, 43)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=43, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 13952
Number of trainable paramters: 13952
device: cuda:1
end of epoch 0: val_loss 0.15194131191282756, val_acc 0.9379432624113475
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.13249415670702788, val_acc 0.9512411347517731
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.11159359095988126, val_acc 0.9574468085106383
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.0918557780747293, val_acc 0.9636524822695035
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.09130778003349842, val_acc 0.9627659574468085
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.07745614528968237, val_acc 0.9698581560283688
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.08245918430197939, val_acc 0.9680851063829787
trigger times: 1
end of epoch 7: val_loss 0.07522489298295582, val_acc 0.9725177304964538
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.0698123522351577, val_acc 0.9707446808510638
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.06834550074686818, val_acc 0.9725177304964538
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.06708705262908116, val_acc 0.9716312056737588
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.06068597531028924, val_acc 0.9778368794326241
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.06442144211900118, val_acc 0.973404255319149
trigger times: 1
end of epoch 13: val_loss 0.06052869723303164, val_acc 0.9725177304964538
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.059069323049362826, val_acc 0.9778368794326241
trigger times: 0
saving model weights...
end of epoch 15: val_loss 0.06033948027178743, val_acc 0.974290780141844
trigger times: 1
end of epoch 16: val_loss 0.05449624818071964, val_acc 0.9804964539007093
trigger times: 0
saving model weights...
end of epoch 17: val_loss 0.05288029595738503, val_acc 0.9796099290780141
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.056678279290579434, val_acc 0.9787234042553191
trigger times: 1
end of epoch 19: val_loss 0.057609607699623665, val_acc 0.9778368794326241
trigger times: 2
end of epoch 20: val_loss 0.05188985927375093, val_acc 0.975177304964539
trigger times: 0
saving model weights...
end of epoch 21: val_loss 0.050296827392578916, val_acc 0.9813829787234043
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.05690211402465102, val_acc 0.9804964539007093
trigger times: 1
end of epoch 23: val_loss 0.048198294715219, val_acc 0.9804964539007093
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.04671387965196724, val_acc 0.9796099290780141
trigger times: 0
saving model weights...
end of epoch 25: val_loss 0.051848277346892443, val_acc 0.9813829787234043
trigger times: 1
end of epoch 26: val_loss 0.048170866618416096, val_acc 0.9813829787234043
trigger times: 2
end of epoch 27: val_loss 0.04895406550291353, val_acc 0.9813829787234043
trigger times: 3
end of epoch 28: val_loss 0.04811327784930075, val_acc 0.9804964539007093
trigger times: 4
end of epoch 29: val_loss 0.04925781027718209, val_acc 0.9813829787234043
trigger times: 5
end of epoch 30: val_loss 0.04848967510528847, val_acc 0.9813829787234043
trigger times: 6
end of epoch 31: val_loss 0.044648305087685476, val_acc 0.9822695035460993
trigger times: 0
saving model weights...
end of epoch 32: val_loss 0.045394929562461266, val_acc 0.9813829787234043
trigger times: 1
end of epoch 33: val_loss 0.046688868794213634, val_acc 0.9796099290780141
trigger times: 2
end of epoch 34: val_loss 0.04285418592475176, val_acc 0.9796099290780141
trigger times: 0
saving model weights...
end of epoch 35: val_loss 0.049334852831677024, val_acc 0.9822695035460993
trigger times: 1
end of epoch 36: val_loss 0.04812383538572128, val_acc 0.9831560283687943
trigger times: 2
end of epoch 37: val_loss 0.0474263021617915, val_acc 0.9831560283687943
trigger times: 3
end of epoch 38: val_loss 0.06082259793616989, val_acc 0.9840425531914894
trigger times: 4
end of epoch 39: val_loss 0.043906513618104576, val_acc 0.9813829787234043
trigger times: 5
end of epoch 40: val_loss 0.04580593926952294, val_acc 0.9831560283687943
trigger times: 6
end of epoch 41: val_loss 0.04628003417733925, val_acc 0.9840425531914894
trigger times: 7
end of epoch 42: val_loss 0.04285679985825169, val_acc 0.9813829787234043
trigger times: 8
end of epoch 43: val_loss 0.04902086822058113, val_acc 0.9840425531914894
trigger times: 9
end of epoch 44: val_loss 0.0471524376641488, val_acc 0.9849290780141844
trigger times: 10
Early stopping.
0 -45.75605934858322 -114.3478791814026
1 -36.50567773729563 -113.0527592060181
2 -43.841273069381714 -103.2050097258442
3 -32.49209947139025 -101.76388982232147
4 -31.537831887602806 -82.95243894933974
5 -28.509997135028243 -80.41220948132849
6 -35.983042715117335 -62.91270007904664
7 -27.01214474439621 -55.45183907670275
8 -21.509892869740725 -26.226286028753027
9 -17.668324552476406 -25.030892790643723
10 -15.848211221396923 -10.941586969641467
11 -10.227681858465075 -2.6693152362626984
12 -9.659152133506723 -0.6169503682874974
13 -8.402612468227744 7.085127736448486
14 -5.888247283175588 30.085890959085077
15 -3.235623680287972 33.309940907344725
16 -4.878896716050804 34.53183125686171
17 -1.6184575413353741 38.74386523093347
18 -2.077188853174448 39.160251163178806
19 -1.6758770840242505 51.18130221875085
20 3.6556078596040606 59.162585389085
21 2.2632910036481917 69.21057916674489
22 3.233458748087287 80.2319629802139
23 8.757362364791334 91.31020541502424
24 10.539783988613635 91.60291397111007
25 9.850501079577953 95.31009344169037
26 18.96085718460381 128.73676510885525
27 16.018413030542433 131.48967691990322
28 17.383063977584243 133.33968592253453
29 22.714783403091133 153.3157491396614
30 20.755311521701515 154.0081635777143
31 29.603066785261035 177.594294762342
32 30.14844812033698 190.1449010777256
33 39.871492384932935 214.6263040548001
34 34.78621888440102 214.7728217494139
35 39.18963886424899 218.5468376965032
36 38.62795534171164 234.15916506458083
37 42.5848800172098 252.17029359403926
38 47.039454055950046 266.3586992205008
39 52.80768794193864 305.27906870114026
40 58.123141267336905 324.0197976082746
41 58.546269458020106 333.0407029156643
42 62.75398592837155 338.4771724289715
43 63.79050107812509 361.84626739841684
44 70.95816588308662 390.7517184882046
45 73.58233190141618 400.0413426933392
46 73.7959035616368 403.13796958654353
47 78.71013329015113 423.1355871564197
train accuracy: 0.996358543417367
validation accuracy: 0.9849290780141844
