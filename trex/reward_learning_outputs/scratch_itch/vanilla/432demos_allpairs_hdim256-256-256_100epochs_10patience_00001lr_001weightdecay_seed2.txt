demos: (480, 200, 42)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 93096
num train_labels 93096
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=42, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142848
Number of trainable paramters: 142848
device: cuda:0
end of epoch 0: val_loss 0.13950363608918678, val_acc 0.9441489361702128
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.12106168943850786, val_acc 0.9476950354609929
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.12058160369405248, val_acc 0.9459219858156028
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.12601858293418922, val_acc 0.9414893617021277
trigger times: 1
end of epoch 4: val_loss 0.11779093954874487, val_acc 0.950354609929078
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.13228155372038441, val_acc 0.9485815602836879
trigger times: 1
end of epoch 6: val_loss 0.13109877970852446, val_acc 0.9361702127659575
trigger times: 2
end of epoch 7: val_loss 0.1375013111001295, val_acc 0.9406028368794326
trigger times: 3
end of epoch 8: val_loss 0.12930502644067032, val_acc 0.9450354609929078
trigger times: 4
end of epoch 9: val_loss 0.12111621805216569, val_acc 0.9450354609929078
trigger times: 5
end of epoch 10: val_loss 0.13112483324497454, val_acc 0.9432624113475178
trigger times: 6
end of epoch 11: val_loss 0.11844400197495768, val_acc 0.9459219858156028
trigger times: 7
end of epoch 12: val_loss 0.1435471883919603, val_acc 0.9308510638297872
trigger times: 8
end of epoch 13: val_loss 0.13108806575349533, val_acc 0.9370567375886525
trigger times: 9
end of epoch 14: val_loss 0.12539282340041005, val_acc 0.9414893617021277
trigger times: 10
Early stopping.
0 -29.62960072234273 -97.24485765376211
1 -27.92234932258725 -84.84678494475872
2 -29.105355955660343 -78.93406316283158
3 -30.00031661055982 -75.02969540587696
4 -26.90156376734376 -72.61362520197201
5 -24.87245813384652 -67.70332814138712
6 -24.260901702567935 -53.814051919860674
7 -23.487919118255377 -44.49019489529177
8 -21.593560826964676 -41.3106518668958
9 -21.942129095783457 -36.45416009522412
10 -22.873568079434335 -35.636600609806095
11 -22.72591237537563 -35.189650726025825
12 -18.651597231510095 -21.70967376674228
13 -21.80770263366867 -20.14259508741993
14 -19.8048132507829 -14.4711838787269
15 -17.62198678200366 -1.8963398599782397
16 -18.307645836728625 0.971479121789783
17 -15.258454171009362 19.491692038206416
18 -18.593531147111207 20.530588087221908
19 -18.154348674463108 28.52857496230795
20 -18.413728348445147 34.21536023715924
21 -15.891355938045308 64.74582010073695
22 -14.048145404784009 81.89014686183334
23 -12.64970513744629 83.25651005762352
24 -15.592953548650257 99.28484545080774
25 -14.567042757582385 121.32613156169126
26 -9.623118119605351 136.51192077875302
27 -13.621806949842721 137.7582846380186
28 -11.330643685127143 142.42451181000234
29 -9.864384858985431 163.26711477318133
30 -9.721922899945639 164.18896384382953
31 -11.236096089764033 168.3992728696738
32 -8.968241086811759 189.56275247801355
33 -10.022116945096059 189.6723102456173
34 -8.578905184287578 206.11896428081653
35 -9.130886380502488 208.13326989135442
36 -10.388690978870727 221.44584607902624
37 -10.63441862835316 233.63745376530167
38 -7.606264065834694 235.04407254128563
39 -8.182667923509143 237.90451471965713
40 -6.145044571487233 260.765498753965
41 -5.205557562876493 282.2062559864006
42 -1.7950207539252006 290.0025052732212
43 -6.115291316527873 301.6965823877335
44 -4.759274051757529 333.0407029156643
45 -4.139572399144527 343.3928836976398
46 -5.330131099908613 423.1355871564197
47 -0.9743183136451989 434.80815706930116
train accuracy: 0.9630488957635129
validation accuracy: 0.9414893617021277
