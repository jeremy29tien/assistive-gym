demos: (480, 200, 42)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 93096
num train_labels 93096
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=42, out_features=256, bias=True)
  (1): Linear(in_features=256, out_features=256, bias=True)
  (2): Linear(in_features=256, out_features=256, bias=True)
  (3): Linear(in_features=256, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 142848
Number of trainable paramters: 142848
device: cuda:0
end of epoch 0: val_loss 0.165715871731151, val_acc 0.9290780141843972
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.17694297244985316, val_acc 0.9193262411347518
trigger times: 1
end of epoch 2: val_loss 0.15480250504515372, val_acc 0.9317375886524822
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.15312734472269812, val_acc 0.9299645390070922
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.1551448676846624, val_acc 0.9290780141843972
trigger times: 1
end of epoch 5: val_loss 0.1912427014631687, val_acc 0.9281914893617021
trigger times: 2
end of epoch 6: val_loss 0.16764024280178574, val_acc 0.925531914893617
trigger times: 3
end of epoch 7: val_loss 0.15913889589372004, val_acc 0.9335106382978723
trigger times: 4
end of epoch 8: val_loss 0.18324675016515263, val_acc 0.9237588652482269
trigger times: 5
end of epoch 9: val_loss 0.1686894572171604, val_acc 0.9299645390070922
trigger times: 6
end of epoch 10: val_loss 0.16686880203131166, val_acc 0.9308510638297872
trigger times: 7
end of epoch 11: val_loss 0.15801092602861036, val_acc 0.9281914893617021
trigger times: 8
end of epoch 12: val_loss 0.18735734012144173, val_acc 0.9299645390070922
trigger times: 9
end of epoch 13: val_loss 0.15404435517079376, val_acc 0.9308510638297872
trigger times: 10
Early stopping.
0 -43.0575924962759 -133.973547371322
1 -43.194367080926895 -106.29807635708414
2 -35.27456285431981 -106.26498585222654
3 -40.888465240597725 -97.24485765376211
4 -39.25498948991299 -78.93406316283158
5 -36.81878509372473 -65.00607091797544
6 -38.62853190302849 -64.31517892724126
7 -36.741846427321434 -63.24633649146097
8 -31.787724006921053 -63.101430300225374
9 -35.724190928041935 -57.66105272983283
10 -36.05682102590799 -56.30990465208505
11 -29.72586903721094 -36.57284828127824
12 -27.32252919487655 -34.92774699423874
13 -29.839106053113937 -34.487650213835884
14 -24.399791352450848 -29.593314750499733
15 -27.284430656582117 -21.776950647506872
16 -27.422178984619677 -1.8963398599782397
17 -24.754604320507497 0.5321209736499255
18 -26.807020306121558 4.232235681838149
19 -24.127788578160107 6.8392346078963
20 -24.048512598033994 19.622534836185636
21 -24.82616567797959 27.01133577467053
22 -23.481447152793407 55.12239522211616
23 -19.869231996126473 55.92026357237822
24 -23.789286259561777 65.1680640695934
25 -19.92122628632933 87.3487075539824
26 -19.9701494933106 105.72097091834365
27 -17.433706414420158 148.38223634397093
28 -18.170707172714174 158.9381440127044
29 -15.959859388880432 194.9502735624309
30 -16.670349707826972 199.51892684347123
31 -14.092315051704645 209.04733790839154
32 -15.742111396044493 224.80518375748704
33 -11.69959425739944 242.29822403524432
34 -15.939863442443311 247.4247332535776
35 -12.519725335761905 278.9263530439534
36 -8.910502233542502 290.0025052732212
37 -16.094039926305413 291.32952200448
38 -9.455907922703773 306.3841750088036
39 -11.723131466191262 317.59854966426605
40 -8.624684248119593 330.083446167866
41 -8.391351662576199 331.71313838439016
42 -10.025594203267246 336.2643662087101
43 -9.530906002037227 360.2306263300218
44 -8.14706140384078 368.3096347545002
45 -10.851326338481158 368.87184160352797
46 -10.285524872597307 370.4973655070732
47 -10.29571473505348 373.9029898572837
train accuracy: 0.9624581077597318
validation accuracy: 0.9308510638297872
