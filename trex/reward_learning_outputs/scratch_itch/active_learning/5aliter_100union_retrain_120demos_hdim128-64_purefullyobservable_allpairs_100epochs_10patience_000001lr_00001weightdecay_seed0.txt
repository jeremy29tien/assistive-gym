demos: (480, 200, 19)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:3
end of epoch 0: val_loss 0.2027077353658107, val_acc 0.9131205673758865
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.1838339472672568, val_acc 0.9113475177304965
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.19752316388970706, val_acc 0.9104609929078015
trigger times: 1
end of epoch 3: val_loss 0.18858205467267294, val_acc 0.9148936170212766
trigger times: 2
end of epoch 4: val_loss 0.1852635702601362, val_acc 0.9175531914893617
trigger times: 3
end of epoch 5: val_loss 0.19885700250568925, val_acc 0.9148936170212766
trigger times: 4
end of epoch 6: val_loss 0.19429145736634362, val_acc 0.9175531914893617
trigger times: 5
end of epoch 7: val_loss 0.21547338958707501, val_acc 0.9086879432624113
trigger times: 6
end of epoch 8: val_loss 0.20249424636877206, val_acc 0.9166666666666666
trigger times: 7
end of epoch 9: val_loss 0.20053181549989665, val_acc 0.9131205673758865
trigger times: 8
end of epoch 10: val_loss 0.22177284173727932, val_acc 0.9166666666666666
trigger times: 9
end of epoch 11: val_loss 0.20911801945679145, val_acc 0.9184397163120568
trigger times: 10
Early stopping.
0 -34.72144032269716 -133.973547371322
1 -30.899888575077057 -106.29807635708414
2 -27.53911940008402 -106.26498585222654
3 -29.616690084338188 -97.24485765376211
4 -28.77995255589485 -78.93406316283158
5 -25.10343310236931 -65.00607091797544
6 -26.132822720333934 -64.31517892724126
7 -26.742932192981243 -63.24633649146097
8 -22.711986253008945 -63.101430300225374
9 -25.91020866110921 -57.66105272983283
10 -25.10239427536726 -56.30990465208505
11 -20.899484030436724 -36.57284828127824
12 -16.833210790660814 -34.92774699423874
13 -21.196508403401822 -34.487650213835884
14 -15.45166629605228 -29.593314750499733
15 -19.438687820686027 -21.776950647506872
16 -20.508356563339476 -1.8963398599782397
17 -18.741108643822372 0.5321209736499255
18 -20.504434258909896 4.232235681838149
19 -16.93099313424318 6.8392346078963
20 -16.247053437808063 19.622534836185636
21 -18.378615649417043 27.01133577467053
22 -17.66093586414354 55.12239522211616
23 -13.273364780819975 55.92026357237822
24 -15.804570775013417 65.1680640695934
25 -12.633337327453773 87.3487075539824
26 -13.101847865327727 105.72097091834365
27 -14.109012044616975 148.38223634397093
28 -10.855938137392513 158.9381440127044
29 -9.643145221256418 194.9502735624309
30 -7.3209410527779255 199.51892684347123
31 -7.6600896454183385 209.04733790839154
32 -11.234164423658513 224.80518375748704
33 -7.6721926354803145 242.29822403524432
34 -9.264602584837121 247.4247332535776
35 -7.001982075016713 278.9263530439534
36 -2.9331985852622893 290.0025052732212
37 -10.275778761337278 291.32952200448
38 -3.7879044879809953 306.3841750088036
39 -7.764514816459268 317.59854966426605
40 -4.354012453259202 330.083446167866
41 -3.9276865898864344 331.71313838439016
42 -5.7257502459106036 336.2643662087101
43 -2.5617444964736933 360.2306263300218
44 -5.658129761257442 368.3096347545002
45 -7.759546491899528 368.87184160352797
46 -1.874745793378679 370.4973655070732
47 -5.654803240497131 373.9029898572837
train accuracy: 0.9659663865546219
validation accuracy: 0.9184397163120568
demos: (480, 200, 19)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:2
end of epoch 0: val_loss 0.2027077353658107, val_acc 0.9131205673758865
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.1838339472672568, val_acc 0.9113475177304965
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.19752316388970706, val_acc 0.9104609929078015
trigger times: 1
end of epoch 3: val_loss 0.18858205467267294, val_acc 0.9148936170212766
trigger times: 2
end of epoch 4: val_loss 0.1852635702601362, val_acc 0.9175531914893617
trigger times: 3
end of epoch 5: val_loss 0.19885700250568925, val_acc 0.9148936170212766
trigger times: 4
end of epoch 6: val_loss 0.19429145736634362, val_acc 0.9175531914893617
trigger times: 5
end of epoch 7: val_loss 0.21547338958707501, val_acc 0.9086879432624113
trigger times: 6
end of epoch 8: val_loss 0.20249424636877206, val_acc 0.9166666666666666
trigger times: 7
end of epoch 9: val_loss 0.20053181549989665, val_acc 0.9131205673758865
trigger times: 8
end of epoch 10: val_loss 0.22177284173727932, val_acc 0.9166666666666666
trigger times: 9
end of epoch 11: val_loss 0.20911801945679145, val_acc 0.9184397163120568
trigger times: 10
Early stopping.
0 -34.72144032269716 -133.973547371322
1 -30.899888575077057 -106.29807635708414
2 -27.53911940008402 -106.26498585222654
3 -29.616690084338188 -97.24485765376211
4 -28.77995255589485 -78.93406316283158
5 -25.10343310236931 -65.00607091797544
6 -26.132822720333934 -64.31517892724126
7 -26.742932192981243 -63.24633649146097
8 -22.711986253008945 -63.101430300225374
9 -25.91020866110921 -57.66105272983283
10 -25.10239427536726 -56.30990465208505
11 -20.899484030436724 -36.57284828127824
12 -16.833210790660814 -34.92774699423874
13 -21.196508403401822 -34.487650213835884
14 -15.45166629605228 -29.593314750499733
15 -19.438687820686027 -21.776950647506872
16 -20.508356563339476 -1.8963398599782397
17 -18.741108643822372 0.5321209736499255
18 -20.504434258909896 4.232235681838149
19 -16.93099313424318 6.8392346078963
20 -16.247053437808063 19.622534836185636
21 -18.378615649417043 27.01133577467053
22 -17.66093586414354 55.12239522211616
23 -13.273364780819975 55.92026357237822
24 -15.804570775013417 65.1680640695934
25 -12.633337327453773 87.3487075539824
26 -13.101847865327727 105.72097091834365
27 -14.109012044616975 148.38223634397093
28 -10.855938137392513 158.9381440127044
29 -9.643145221256418 194.9502735624309
30 -7.3209410527779255 199.51892684347123
31 -7.6600896454183385 209.04733790839154
32 -11.234164423658513 224.80518375748704
33 -7.6721926354803145 242.29822403524432
34 -9.264602584837121 247.4247332535776
35 -7.001982075016713 278.9263530439534
36 -2.9331985852622893 290.0025052732212
37 -10.275778761337278 291.32952200448
38 -3.7879044879809953 306.3841750088036
39 -7.764514816459268 317.59854966426605
40 -4.354012453259202 330.083446167866
41 -3.9276865898864344 331.71313838439016
42 -5.7257502459106036 336.2643662087101
43 -2.5617444964736933 360.2306263300218
44 -5.658129761257442 368.3096347545002
45 -7.759546491899528 368.87184160352797
46 -1.874745793378679 370.4973655070732
47 -5.654803240497131 373.9029898572837
train accuracy: 0.9659663865546219
validation accuracy: 0.9184397163120568
demos: (580, 200, 19)
demo_rewards: (580,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 1653
num val_labels 1653
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:2
end of epoch 0: val_loss 0.16714567911882905, val_acc 0.9292196007259528
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.14648106438706093, val_acc 0.9340592861464004
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.13846487874520133, val_acc 0.9388989715668482
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.1459674294752596, val_acc 0.9364791288566243
trigger times: 1
end of epoch 4: val_loss 0.14076493717903163, val_acc 0.9346642468239564
trigger times: 2
end of epoch 5: val_loss 0.13389369032828122, val_acc 0.9401088929219601
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.13909297252254682, val_acc 0.9340592861464004
trigger times: 1
end of epoch 7: val_loss 0.1247096672084627, val_acc 0.9401088929219601
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.12521181519169403, val_acc 0.9401088929219601
trigger times: 1
end of epoch 9: val_loss 0.13167133922559396, val_acc 0.9364791288566243
trigger times: 2
end of epoch 10: val_loss 0.1329903059187442, val_acc 0.9376890502117362
trigger times: 3
end of epoch 11: val_loss 0.13351851113379004, val_acc 0.9388989715668482
trigger times: 4
end of epoch 12: val_loss 0.13313724812287442, val_acc 0.9388989715668482
trigger times: 5
end of epoch 13: val_loss 0.13932746125282802, val_acc 0.9376890502117362
trigger times: 6
end of epoch 14: val_loss 0.12932922272384434, val_acc 0.940713853599516
trigger times: 7
end of epoch 15: val_loss 0.13839272901935384, val_acc 0.941318814277072
trigger times: 8
end of epoch 16: val_loss 0.14393312873503042, val_acc 0.940713853599516
trigger times: 9
end of epoch 17: val_loss 0.13910835263428653, val_acc 0.9370840895341803
trigger times: 10
Early stopping.
0 -36.764317115419544 -148.3407094002489
1 -39.73886302183382 -141.2546694417987
2 -35.769013257697225 -140.09268009408567
3 -31.977346654050052 -128.70855472693918
4 -34.39726414531469 -125.90649512105452
5 -31.732196579047013 -119.16778974864786
6 -33.94151412323117 -116.39164759390924
7 -35.768153078854084 -115.4545179094371
8 -30.17201274074614 -115.27865235069261
9 -31.39891162631102 -115.10005467921721
10 -25.47501034475863 -111.1795600852603
11 -32.19453822262585 -109.44587408588339
12 -29.55133874900639 -104.32770507770279
13 -28.478860944509506 -89.88854084032519
14 -26.094882800010964 -78.79738130689596
15 -26.046314407140017 -78.20169549348978
16 -27.676650169305503 -71.83775048714611
17 -24.543347612023354 -71.0097189898932
18 -26.315873329527676 -65.15917851473925
19 -27.29652658069972 -64.31517892724126
20 -24.34886637981981 -48.71971149808798
21 -20.70158325158991 -41.3106518668958
22 -19.21796142833773 -36.57284828127824
23 -16.32151193637401 -29.593314750499733
24 -19.495965983252972 -4.4840564393316305
25 -19.292200514988508 -1.8963398599782397
26 -17.78393789753318 0.5321209736499255
27 -19.401747007039376 0.8487554879327812
28 -15.661580126849003 6.8392346078963
29 -18.28969987144228 18.945378815659346
30 -16.71929442381952 39.92673927109536
31 -18.555641849990934 44.850019698054204
32 -16.810872125948663 50.52841485696464
33 -14.741569727513706 69.90236244353298
34 -16.22088255546987 74.26310022402075
35 -15.76765223883558 103.76850790553507
36 -13.68526561598992 133.04858367101036
37 -9.522242859005928 149.16560553618203
38 -12.98948160908185 157.63388865862527
39 -9.937515694939066 158.9381440127044
40 -11.953827592660673 164.18896384382953
41 -13.107128575909883 164.96088964947137
42 -8.335023209976498 174.7104083975865
43 -10.66205610288307 182.8682701433281
44 -10.292554620071314 204.5598807600962
45 -3.5391707589151338 266.3586992205008
46 -7.258376308949664 286.93129032263715
47 -4.859613198670559 300.668611768182
48 -6.169334556499962 330.083446167866
49 -2.398700622259639 331.71313838439016
50 -2.628508980385959 350.75464928107647
51 -4.309722294157837 360.2306263300218
52 -3.1999081702088006 368.3096347545002
53 -4.912627613637596 389.8471381383487
54 -5.326104835097794 403.13796958654353
55 -2.8807193147949874 414.3856401909296
56 -5.103582072944846 414.43301139651703
57 -2.5877594490302727 467.300177037188
train accuracy: 0.9773109243697479
validation accuracy: 0.9370840895341803
demos: (680, 200, 19)
demo_rewards: (680,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 2278
num val_labels 2278
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:1
end of epoch 0: val_loss 0.18870488511227226, val_acc 0.9183494293239683
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.18923561794223617, val_acc 0.922739244951712
trigger times: 1
end of epoch 2: val_loss 0.2009423723597832, val_acc 0.9152765583845478
trigger times: 2
end of epoch 3: val_loss 0.1936074790853606, val_acc 0.9187884108867428
trigger times: 3
end of epoch 4: val_loss 0.2185343917287671, val_acc 0.9122036874451273
trigger times: 4
end of epoch 5: val_loss 0.21502661482519955, val_acc 0.9192273924495171
trigger times: 5
end of epoch 6: val_loss 0.24065615936445606, val_acc 0.9126426690079017
trigger times: 6
end of epoch 7: val_loss 0.22916534358429463, val_acc 0.9192273924495171
trigger times: 7
end of epoch 8: val_loss 0.2442811347848213, val_acc 0.9152765583845478
trigger times: 8
end of epoch 9: val_loss 0.23338620891809753, val_acc 0.9174714661984197
trigger times: 9
end of epoch 10: val_loss 0.23106501030418986, val_acc 0.9223002633889377
trigger times: 10
Early stopping.
0 -17.79746447038633 -168.28140690606054
1 -34.116544822696596 -147.01341424791994
2 -29.24562355550006 -130.12491644267755
3 -26.361042385920882 -127.631039017647
4 -26.38557471113745 -125.47806732825039
5 -27.766496237833053 -123.69234140007934
6 -27.094459937885404 -117.17797657764834
7 -28.944366271607578 -116.35309021776656
8 -22.777937891427428 -108.8956936643611
9 -15.631086365508963 -106.26498585222654
10 -24.119558637961745 -103.71188217078013
11 -22.471310900058597 -101.55243959156634
12 -22.517093762056902 -101.16927464482806
13 -24.80584066454321 -99.07938895356462
14 -24.484531607478857 -96.05813976775474
15 -23.782723426818848 -94.75279234305356
16 -19.375288625247777 -93.99360086694082
17 -22.309744600206614 -88.85541893746874
18 -21.008866162388586 -86.25406235897236
19 -22.71178170852363 -78.20169549348978
20 -23.868042942136526 -78.05157794611979
21 -21.485782379982993 -75.88537834325405
22 -17.820924192667007 -65.57451367466994
23 -16.363833715673536 -63.034151140059706
24 -17.195294949109666 -61.69729212132328
25 -17.665017323102802 -59.23492089348715
26 -19.07555007666815 -59.03426927429014
27 -18.216872456949204 -57.15894304047781
28 -19.34936453588307 -52.07084375966113
29 -15.599787419720087 -43.10709269696429
30 -16.540603802306578 -41.631582652712986
31 -17.368072811048478 -39.75408227382109
32 -14.858081562444568 -30.59144777988387
33 -15.702914971625432 -29.060880046311485
34 -16.78148059430532 -21.726958928300203
35 -16.730872912798077 -20.14259508741993
36 -15.704233558382839 -14.084087161206785
37 -16.692006316268817 -8.277591352112765
38 -15.571229288354516 0.971479121789783
39 -14.47631712560542 16.592368784775562
40 -11.84694346197648 19.491692038206416
41 -10.150418155244552 55.36888607845635
42 -13.277202734723687 57.76744549995627
43 -10.884477292769589 64.96033396275733
44 -8.47777090856107 88.72799533692061
45 -10.806326337857172 107.71006781863467
46 -9.848444048606325 148.38223634397093
47 -9.344181437802035 158.95650203412697
48 -8.719123237533495 166.29428147232497
49 -8.274264192092232 170.62140513944783
50 -8.839113325346261 171.5093142215239
51 -7.577991655445658 189.5748341940192
52 -7.515418298426084 209.56820737620916
53 -9.25080620427616 233.63745376530167
54 -6.515161324408837 242.29822403524432
55 -6.837146138539538 244.45537108939513
56 -3.2196775805205107 250.19951558675749
57 -3.698383958253544 282.3250275983876
58 -4.487158465868561 300.668611768182
59 -4.987890794465784 307.81271913623794
60 -3.285249598498922 368.3096347545002
61 -2.874307301361114 370.4973655070732
62 -3.555573374731466 370.66500772735526
63 -4.001943028735695 372.3917560212975
64 -4.376769778289599 373.9029898572837
65 -3.6770654300344177 374.0442710820621
66 -2.835972017608583 403.13796958654353
67 -2.748796211963054 467.300177037188
train accuracy: 0.9659663865546219
validation accuracy: 0.9223002633889377
demos: (780, 200, 19)
demo_rewards: (780,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 3003
num val_labels 3003
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:3
end of epoch 0: val_loss 0.3859813486664889, val_acc 0.8834498834498834
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.32433469103028706, val_acc 0.8847818847818848
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.28668636261546393, val_acc 0.8817848817848818
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.24304209943786173, val_acc 0.9174159174159174
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.22211494867009435, val_acc 0.9064269064269064
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.22502882888950634, val_acc 0.906093906093906
trigger times: 1
end of epoch 6: val_loss 0.2257932941786923, val_acc 0.9057609057609057
trigger times: 2
end of epoch 7: val_loss 0.21896685788950648, val_acc 0.9094239094239094
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.20571822964722683, val_acc 0.9180819180819181
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.20554504723887104, val_acc 0.9154179154179154
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.20096713297126814, val_acc 0.922077922077922
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.19577738443845488, val_acc 0.9214119214119214
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.19425762612066194, val_acc 0.9270729270729271
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.20254384828525684, val_acc 0.9230769230769231
trigger times: 1
end of epoch 14: val_loss 0.19315138819028982, val_acc 0.9227439227439227
trigger times: 0
saving model weights...
end of epoch 15: val_loss 0.18500913139852765, val_acc 0.9264069264069265
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.19302865392059004, val_acc 0.9210789210789211
trigger times: 1
end of epoch 17: val_loss 0.1877807320855447, val_acc 0.9310689310689311
trigger times: 2
end of epoch 18: val_loss 0.19698729841163487, val_acc 0.929070929070929
trigger times: 3
end of epoch 19: val_loss 0.18410550162321812, val_acc 0.9274059274059274
trigger times: 0
saving model weights...
end of epoch 20: val_loss 0.1859508243087155, val_acc 0.929070929070929
trigger times: 1
end of epoch 21: val_loss 0.18997673797293516, val_acc 0.9333999333999334
trigger times: 2
end of epoch 22: val_loss 0.20231868230723826, val_acc 0.9224109224109224
trigger times: 3
end of epoch 23: val_loss 0.19124760113781997, val_acc 0.9277389277389277
trigger times: 4
end of epoch 24: val_loss 0.18600710474087453, val_acc 0.9304029304029304
trigger times: 5
end of epoch 25: val_loss 0.18845858985485944, val_acc 0.9284049284049284
trigger times: 6
end of epoch 26: val_loss 0.1859558779961126, val_acc 0.9310689310689311
trigger times: 7
end of epoch 27: val_loss 0.19402189698724862, val_acc 0.9274059274059274
trigger times: 8
end of epoch 28: val_loss 0.21449167507278755, val_acc 0.9197469197469198
trigger times: 9
end of epoch 29: val_loss 0.19058737951868146, val_acc 0.9270729270729271
trigger times: 10
Early stopping.
0 -39.935019046999514 -142.0035905826286
1 -39.903910622000694 -133.973547371322
2 -40.02870148047805 -132.97668139196398
3 -36.70982022723183 -126.20956744433838
4 -31.70977238751948 -125.32139311842607
5 -34.731973776593804 -123.2169151061372
6 -34.493674010038376 -117.17797657764834
7 -37.70242120325565 -116.35309021776656
8 -37.1940910667181 -115.4545179094371
9 -33.28468167409301 -114.76315561697069
10 -35.238254990428686 -109.44587408588339
11 -24.27911629760638 -93.02325299778896
12 -23.69099743058905 -90.84708472720422
13 -21.709909704513848 -89.90589085199838
14 -23.782024778425694 -89.80503494314031
15 -25.18535931361839 -87.46111978699656
16 -27.010904047638178 -85.99096003442662
17 -23.85774127719924 -78.82046848473206
18 -27.594652269035578 -78.41738157747275
19 -28.434699039906263 -78.05157794611979
20 -20.75232206704095 -74.5207303846807
21 -20.106043756008148 -71.83848723408865
22 -26.287945144809783 -69.01056893862284
23 -24.69566192291677 -67.56745019597582
24 -23.732202626764774 -67.07852762813918
25 -21.809201715514064 -63.24633649146097
26 -19.044016929809004 -63.101854707158715
27 -17.677991000469774 -52.39984222917899
28 -19.12981631467119 -44.49019489529177
29 -19.58973736781627 -43.10709269696429
30 -18.70988682936877 -40.222205219523836
31 -18.73223239122308 -37.86455191096491
32 -18.273157682735473 -36.37950117215486
33 -16.390963533893228 -34.487650213835884
34 -15.146896806312725 -33.34809776864483
35 -17.76117521279957 -32.976193655030045
36 -18.69651952572167 -27.547720584776908
37 -14.76791599811986 -26.919011219501066
38 -17.08004397433251 -26.384948563376962
39 -15.675494491122663 -21.776950647506872
40 -18.083771679725032 -21.726958928300203
41 -17.299763433169574 -21.31030639495003
42 -15.041381516028196 -16.114376409083132
43 -16.488321899902076 -14.084087161206785
44 -17.67220398806967 -8.277591352112765
45 -16.459022062830627 0.8487554879327812
46 -16.492207534145564 2.9018419172652745
47 -17.011018027551472 6.2380604935071915
48 -16.422191079938784 8.402906242430207
49 -14.979752651415765 44.850019698054204
50 -14.86279886495322 57.76744549995627
51 -13.57248102244921 58.63093231433321
52 -15.546254683285952 95.48747545957438
53 -11.592785592074506 107.71006781863467
54 -12.516420557629317 116.83743330105035
55 -13.673367607290857 125.53777532052302
56 -8.525263679301133 146.55623577975578
57 -11.617379006083866 157.73723206248386
58 -9.330981894629076 164.18896384382953
59 -10.659111324464902 171.5093142215239
60 -8.26605111558456 189.5748341940192
61 -17.31568595743738 213.39913500740522
62 -6.726676044840133 247.39654282410996
63 -3.798654492245987 250.19951558675749
64 -6.4823986693809275 262.49206401593506
65 -9.702016742157866 266.1054433499994
66 -8.628909945953637 275.0460823215203
67 -4.099429389054421 287.80651503403897
68 -6.675238624389749 291.437334190165
69 -5.987904079869622 300.668611768182
70 -5.410921045753639 310.21066379041264
71 -5.9952173286583275 352.5893348420745
72 -2.0715572361368686 368.3096347545002
73 -1.9399618401075713 370.4973655070732
74 -1.7149437493353616 374.0442710820621
75 -3.9390100794262253 376.1035088128469
76 -4.260639156476827 403.13796958654353
77 -2.842300812364556 467.300177037188
train accuracy: 0.9798319327731092
validation accuracy: 0.9270729270729271
demos: (880, 200, 19)
demo_rewards: (880,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 3828
num val_labels 3828
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:1
end of epoch 0: val_loss 0.267485495234825, val_acc 0.8845350052246604
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.21439209523925395, val_acc 0.910135841170324
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.20382678391102665, val_acc 0.918234064785789
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.1817166065560248, val_acc 0.920846394984326
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.16489364773138776, val_acc 0.93521421107628
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.15270627314705798, val_acc 0.9409613375130617
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.17994606960719273, val_acc 0.9224137931034483
trigger times: 1
end of epoch 7: val_loss 0.17882621977139435, val_acc 0.9239811912225705
trigger times: 2
end of epoch 8: val_loss 0.16044208283934497, val_acc 0.930773249738767
trigger times: 3
end of epoch 9: val_loss 0.14532035691864498, val_acc 0.9399164054336469
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.14522802116934302, val_acc 0.9404388714733543
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.15180500110955644, val_acc 0.9388714733542319
trigger times: 1
end of epoch 12: val_loss 0.15264301190450166, val_acc 0.9373040752351097
trigger times: 2
end of epoch 13: val_loss 0.1472051786996248, val_acc 0.9399164054336469
trigger times: 3
end of epoch 14: val_loss 0.14975035961645186, val_acc 0.9375653082549634
trigger times: 4
end of epoch 15: val_loss 0.14333356593178728, val_acc 0.940700104493208
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.1512970442873958, val_acc 0.9396551724137931
trigger times: 1
end of epoch 17: val_loss 0.1629942485124891, val_acc 0.9284221525600836
trigger times: 2
end of epoch 18: val_loss 0.15980791454233742, val_acc 0.9346917450365726
trigger times: 3
end of epoch 19: val_loss 0.15329106138872278, val_acc 0.9367816091954023
trigger times: 4
end of epoch 20: val_loss 0.15084592893832788, val_acc 0.9388714733542319
trigger times: 5
end of epoch 21: val_loss 0.13833557425003026, val_acc 0.9414838035527691
trigger times: 0
saving model weights...
end of epoch 22: val_loss 0.1430355321470368, val_acc 0.9430512016718914
trigger times: 1
end of epoch 23: val_loss 0.14925666025342155, val_acc 0.9420062695924765
trigger times: 2
end of epoch 24: val_loss 0.13281205464001625, val_acc 0.9461859979101358
trigger times: 0
saving model weights...
end of epoch 25: val_loss 0.1382939594933297, val_acc 0.9440961337513062
trigger times: 1
end of epoch 26: val_loss 0.15422372509972776, val_acc 0.9412225705329154
trigger times: 2
end of epoch 27: val_loss 0.1468397277485255, val_acc 0.9412225705329154
trigger times: 3
end of epoch 28: val_loss 0.14683227732591095, val_acc 0.9420062695924765
trigger times: 4
end of epoch 29: val_loss 0.14539383188333346, val_acc 0.9420062695924765
trigger times: 5
end of epoch 30: val_loss 0.15381236697294254, val_acc 0.9393939393939394
trigger times: 6
end of epoch 31: val_loss 0.13952028807179018, val_acc 0.9446185997910136
trigger times: 7
end of epoch 32: val_loss 0.16062510969122418, val_acc 0.9383490073145245
trigger times: 8
end of epoch 33: val_loss 0.13627080070829575, val_acc 0.945141065830721
trigger times: 9
end of epoch 34: val_loss 0.16166543033770647, val_acc 0.9373040752351097
trigger times: 10
Early stopping.
0 -37.721138179302216 -140.09268009408567
1 -45.208773784921505 -136.49969704181444
2 -33.67484765406698 -130.38227436575804
3 -37.44503668660764 -129.38992725301156
4 -36.48460444062948 -117.17797657764834
5 -37.78099865792319 -115.4545179094371
6 -33.54637810215354 -111.90234771367686
7 -31.98328940011561 -104.55447844836016
8 -31.129495439119637 -95.55000125550123
9 -27.91867184569128 -93.67265104789418
10 -29.213197774253786 -89.80503494314031
11 -30.680616865865886 -87.46111978699656
12 -29.264524032070767 -87.4374576664892
13 -25.127011547796428 -84.28595870565519
14 -24.755410433281213 -78.79738130689596
15 -25.41495260130614 -78.09063002839795
16 -21.99730566609651 -76.36945522986117
17 -27.2341680288082 -75.88537834325405
18 -26.64171329047531 -70.8465383223686
19 -27.655353063717484 -68.87779198746294
20 -21.835549518465996 -68.17092406234961
21 -26.46104788663797 -68.0460992186354
22 -22.7892002138542 -66.14530385595307
23 -23.724906504387036 -65.25218008286699
24 -27.15739804529585 -64.96486486439129
25 -20.538419844815508 -64.36971515307215
26 -23.546932760364143 -63.20833579377938
27 -25.17592126992531 -62.27271141942283
28 -26.00058193784207 -58.14374450325037
29 -21.38232659874484 -57.235086530652374
30 -20.665502648218535 -53.03083396604458
31 -20.992519318067934 -52.07084375966113
32 -19.87720459117554 -51.914857548602754
33 -20.8893580713775 -50.64940171349056
34 -19.435239377897233 -45.78423540739262
35 -19.038399719051085 -43.11665078577158
36 -20.419857163215056 -42.86048762589752
37 -22.56789784639841 -40.18421997436947
38 -20.601811891421676 -36.57284828127824
39 -18.35969580640085 -36.482356301968814
40 -17.310722618800355 -33.34809776864483
41 -18.7491898930748 -32.976193655030045
42 -20.179468572838232 -30.423436495225978
43 -20.206319978635293 -29.72785281263577
44 -19.21731305803405 -24.65303673222151
45 -18.939682549797 -21.65127990722771
46 -20.963832444977015 -20.87167996788709
47 -14.950543634069618 4.232235681838149
48 -19.10827106051147 8.402906242430207
49 -14.131653679301962 15.180025586543433
50 -14.652495894260937 16.592368784775562
51 -14.323713900288567 18.945378815659346
52 -14.484176143771037 19.491692038206416
53 -14.383918448613258 30.589233183073496
54 -15.742943563556764 43.91598246817927
55 -14.463453621603549 48.37766862547772
56 -12.046226206992287 55.36888607845635
57 -14.485840176785132 57.76744549995627
58 -12.506099023012212 65.1680640695934
59 -13.237845267925877 74.26310022402075
60 -13.370768841065 78.02505437250738
61 -14.18105081745307 83.34591849094322
62 -19.957742739352398 95.48747545957438
63 -9.957842011150206 103.45095736623303
64 -11.166100414091488 107.71006781863467
65 -11.966714953625342 116.83743330105035
66 -9.780570431612432 133.57330341432083
67 -9.901848153094761 142.42451181000234
68 -10.686421242775396 157.73723206248386
69 -7.990681160939857 215.42211929419062
70 -8.021442607714562 233.63745376530167
71 -5.0819725982655655 244.45537108939513
72 -6.051724658638705 247.39654282410996
73 -4.031622509850422 256.22410276039744
74 -10.334110269759549 266.1054433499994
75 -3.469792860938469 266.3586992205008
76 -6.3909161342307925 286.93129032263715
77 -2.553871501906542 287.80651503403897
78 -5.052365445677424 291.437334190165
79 -6.589713006309466 306.72710292522027
80 -4.018917981069535 321.3831766312463
81 -1.5119032832153607 323.3596937523082
82 -1.4063046507362742 334.8245118200832
83 -5.022207338392036 352.5893348420745
84 -0.08856914902571589 374.0442710820621
85 -3.122399323619902 403.13796958654353
86 -2.7314488700649235 434.80815706930116
87 0.04958724061725661 467.300177037188
train accuracy: 0.9801120448179271
validation accuracy: 0.9373040752351097
