demos: (480, 200, 19)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:3
end of epoch 0: val_loss 0.2027077353658107, val_acc 0.9131205673758865
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.1838339472672568, val_acc 0.9113475177304965
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.19752316388970706, val_acc 0.9104609929078015
trigger times: 1
end of epoch 3: val_loss 0.18858205467267294, val_acc 0.9148936170212766
trigger times: 2
end of epoch 4: val_loss 0.1852635702601362, val_acc 0.9175531914893617
trigger times: 3
end of epoch 5: val_loss 0.19885700250568925, val_acc 0.9148936170212766
trigger times: 4
end of epoch 6: val_loss 0.19429145736634362, val_acc 0.9175531914893617
trigger times: 5
end of epoch 7: val_loss 0.21547338958707501, val_acc 0.9086879432624113
trigger times: 6
end of epoch 8: val_loss 0.20249424636877206, val_acc 0.9166666666666666
trigger times: 7
end of epoch 9: val_loss 0.20053181549989665, val_acc 0.9131205673758865
trigger times: 8
end of epoch 10: val_loss 0.22177284173727932, val_acc 0.9166666666666666
trigger times: 9
end of epoch 11: val_loss 0.20911801945679145, val_acc 0.9184397163120568
trigger times: 10
Early stopping.
0 -34.72144032269716 -133.973547371322
1 -30.899888575077057 -106.29807635708414
2 -27.53911940008402 -106.26498585222654
3 -29.616690084338188 -97.24485765376211
4 -28.77995255589485 -78.93406316283158
5 -25.10343310236931 -65.00607091797544
6 -26.132822720333934 -64.31517892724126
7 -26.742932192981243 -63.24633649146097
8 -22.711986253008945 -63.101430300225374
9 -25.91020866110921 -57.66105272983283
10 -25.10239427536726 -56.30990465208505
11 -20.899484030436724 -36.57284828127824
12 -16.833210790660814 -34.92774699423874
13 -21.196508403401822 -34.487650213835884
14 -15.45166629605228 -29.593314750499733
15 -19.438687820686027 -21.776950647506872
16 -20.508356563339476 -1.8963398599782397
17 -18.741108643822372 0.5321209736499255
18 -20.504434258909896 4.232235681838149
19 -16.93099313424318 6.8392346078963
20 -16.247053437808063 19.622534836185636
21 -18.378615649417043 27.01133577467053
22 -17.66093586414354 55.12239522211616
23 -13.273364780819975 55.92026357237822
24 -15.804570775013417 65.1680640695934
25 -12.633337327453773 87.3487075539824
26 -13.101847865327727 105.72097091834365
27 -14.109012044616975 148.38223634397093
28 -10.855938137392513 158.9381440127044
29 -9.643145221256418 194.9502735624309
30 -7.3209410527779255 199.51892684347123
31 -7.6600896454183385 209.04733790839154
32 -11.234164423658513 224.80518375748704
33 -7.6721926354803145 242.29822403524432
34 -9.264602584837121 247.4247332535776
35 -7.001982075016713 278.9263530439534
36 -2.9331985852622893 290.0025052732212
37 -10.275778761337278 291.32952200448
38 -3.7879044879809953 306.3841750088036
39 -7.764514816459268 317.59854966426605
40 -4.354012453259202 330.083446167866
41 -3.9276865898864344 331.71313838439016
42 -5.7257502459106036 336.2643662087101
43 -2.5617444964736933 360.2306263300218
44 -5.658129761257442 368.3096347545002
45 -7.759546491899528 368.87184160352797
46 -1.874745793378679 370.4973655070732
47 -5.654803240497131 373.9029898572837
train accuracy: 0.9659663865546219
validation accuracy: 0.9184397163120568
