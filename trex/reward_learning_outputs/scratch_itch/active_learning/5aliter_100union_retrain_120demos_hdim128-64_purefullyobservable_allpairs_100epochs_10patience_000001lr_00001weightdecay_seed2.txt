demos: (480, 200, 19)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:2
end of epoch 0: val_loss 0.1838159086360403, val_acc 0.9335106382978723
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.1694335488456504, val_acc 0.9388297872340425
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.16278455487545415, val_acc 0.9388297872340425
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.15911116059071762, val_acc 0.9397163120567376
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.15996955280019162, val_acc 0.9414893617021277
trigger times: 1
end of epoch 5: val_loss 0.16539781710301368, val_acc 0.9406028368794326
trigger times: 2
end of epoch 6: val_loss 0.15779851839767342, val_acc 0.9423758865248227
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.15821953137678926, val_acc 0.9441489361702128
trigger times: 1
end of epoch 8: val_loss 0.16071230406115578, val_acc 0.9432624113475178
trigger times: 2
end of epoch 9: val_loss 0.16326724106466659, val_acc 0.9423758865248227
trigger times: 3
end of epoch 10: val_loss 0.15458611609748596, val_acc 0.9450354609929078
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.1543100701584052, val_acc 0.9441489361702128
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.15376245459535923, val_acc 0.9441489361702128
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.15695916520859987, val_acc 0.9450354609929078
trigger times: 1
end of epoch 14: val_loss 0.16017920222562104, val_acc 0.9441489361702128
trigger times: 2
end of epoch 15: val_loss 0.15756048700116038, val_acc 0.9414893617021277
trigger times: 3
end of epoch 16: val_loss 0.16299825139194982, val_acc 0.9414893617021277
trigger times: 4
end of epoch 17: val_loss 0.16066429522303444, val_acc 0.9406028368794326
trigger times: 5
end of epoch 18: val_loss 0.1604947077790335, val_acc 0.9450354609929078
trigger times: 6
end of epoch 19: val_loss 0.16026856350492186, val_acc 0.9414893617021277
trigger times: 7
end of epoch 20: val_loss 0.1628804024270922, val_acc 0.9432624113475178
trigger times: 8
end of epoch 21: val_loss 0.16435067680240897, val_acc 0.9423758865248227
trigger times: 9
end of epoch 22: val_loss 0.15943525542896841, val_acc 0.9432624113475178
trigger times: 10
Early stopping.
0 -21.16432719072327 -97.24485765376211
1 -13.391212115064263 -84.84678494475872
2 -17.678871489595622 -78.93406316283158
3 -23.3897185344249 -75.02969540587696
4 -17.08976946095936 -72.61362520197201
5 -16.1577448786702 -67.70332814138712
6 -15.574131913250312 -53.814051919860674
7 -15.112019352964126 -44.49019489529177
8 -11.200067878700793 -41.3106518668958
9 -12.809307403629646 -36.45416009522412
10 -11.014420763822272 -35.636600609806095
11 -10.133123488747515 -35.189650726025825
12 -8.215135229984298 -21.70967376674228
13 -8.495580938295461 -20.14259508741993
14 -12.322966645704582 -14.4711838787269
15 -10.417414378025569 -1.8963398599782397
16 -8.727842595078982 0.971479121789783
17 -4.845542148279492 19.491692038206416
18 -8.066241363354493 20.530588087221908
19 -8.744133271859027 28.52857496230795
20 -6.394244531053118 34.21536023715924
21 -6.561171503388323 64.74582010073695
22 -2.688490283093415 81.89014686183334
23 -1.341874653706327 83.25651005762352
24 -6.165941792540252 99.28484545080774
25 -3.991481817269232 121.32613156169126
26 -0.21530539679224603 136.51192077875302
27 -3.4652981320396066 137.7582846380186
28 -1.739100118051283 142.42451181000234
29 -0.5820399953518063 163.26711477318133
30 -0.577763790381141 164.18896384382953
31 -0.20418431505095214 168.3992728696738
32 0.6546530070481822 189.56275247801355
33 0.7061213522101752 189.6723102456173
34 0.430234533501789 206.11896428081653
35 2.492018638527952 208.13326989135442
36 0.47054546471918 221.44584607902624
37 1.2419826129917055 233.63745376530167
38 2.2208417274523526 235.04407254128563
39 1.4243689300492406 237.90451471965713
40 4.245932168094441 260.765498753965
41 2.8578160699398722 282.2062559864006
42 6.405891477159457 290.0025052732212
43 3.6792607773677446 301.6965823877335
44 5.302508456166834 333.0407029156643
45 4.354225780320121 343.3928836976398
46 2.1030592956813052 423.1355871564197
47 7.572503538103774 434.80815706930116
train accuracy: 0.9704481792717087
validation accuracy: 0.9432624113475178
demos: (580, 200, 19)
demo_rewards: (580,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 1653
num val_labels 1653
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:3
end of epoch 0: val_loss 0.22827126185208077, val_acc 0.9013914095583787
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.2120513147272196, val_acc 0.9134906231094979
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.200702439685089, val_acc 0.9243799153055051
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.1987884946139013, val_acc 0.9249848759830611
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.19559502747940957, val_acc 0.926799758015729
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.19892786704699372, val_acc 0.926194797338173
trigger times: 1
end of epoch 6: val_loss 0.20049412791337748, val_acc 0.9292196007259528
trigger times: 2
end of epoch 7: val_loss 0.1959400700669671, val_acc 0.9286146400483969
trigger times: 3
end of epoch 8: val_loss 0.19773524773061674, val_acc 0.9286146400483969
trigger times: 4
end of epoch 9: val_loss 0.19946622914019674, val_acc 0.9286146400483969
trigger times: 5
end of epoch 10: val_loss 0.1952393135427119, val_acc 0.9292196007259528
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.1981594016283536, val_acc 0.9310344827586207
trigger times: 1
end of epoch 12: val_loss 0.1980470055907377, val_acc 0.9322444041137327
trigger times: 2
end of epoch 13: val_loss 0.19679214428290928, val_acc 0.9304295220810648
trigger times: 3
end of epoch 14: val_loss 0.2044937355719193, val_acc 0.9316394434361767
trigger times: 4
end of epoch 15: val_loss 0.20215731848695265, val_acc 0.9322444041137327
trigger times: 5
end of epoch 16: val_loss 0.20393457660006342, val_acc 0.9334543254688445
trigger times: 6
end of epoch 17: val_loss 0.20983053361903897, val_acc 0.9346642468239564
trigger times: 7
end of epoch 18: val_loss 0.20928916510101933, val_acc 0.9292196007259528
trigger times: 8
end of epoch 19: val_loss 0.2060132109001266, val_acc 0.9334543254688445
trigger times: 9
end of epoch 20: val_loss 0.20933515964887828, val_acc 0.9334543254688445
trigger times: 10
Early stopping.
0 -32.84696966223419 -182.35066025738055
1 -30.600120659917593 -177.43401893695787
2 -31.87203348055482 -176.78076551790582
3 -31.94556638924405 -176.63710879135968
4 -34.7513186018914 -174.8809116186214
5 -29.925667645409703 -174.73064541536968
6 -33.656095653772354 -173.33244105673435
7 -27.396371820708737 -160.2106208148812
8 -34.13844342203811 -159.20016428791192
9 -28.57455036882311 -158.31527342181974
10 -27.35172608681023 -139.42386328096637
11 -18.036615409888327 -125.90649512105452
12 -19.561118498444557 -123.73722586114381
13 -16.484447727445513 -110.57960879670767
14 -17.111558099510148 -100.49594588248743
15 -14.9677276564762 -87.11199891271609
16 -16.188453233698965 -83.35608985615931
17 -13.836414609919302 -81.40890616600856
18 -14.783007551857736 -80.48391381073206
19 -16.613781943917274 -78.63191172502285
20 -28.13959221728146 -75.02969540587696
21 -13.836232990841381 -55.058730007266846
22 -14.088226892519742 -48.71971149808798
23 -8.53561103384709 -21.70967376674228
24 -8.360706266132183 -20.05488751117805
25 -8.366977336045238 0.971479121789783
26 -6.116315618623048 15.180025586543433
27 -6.477706482517533 28.52857496230795
28 -6.780637541727629 55.12239522211616
29 -3.448542196987546 58.63093231433321
30 -6.8727618551347405 68.5395992615062
31 -1.2847872339189053 88.72799533692061
32 -1.1620019180700183 98.71733223851827
33 -0.6318650327593787 103.16215883545624
34 -5.362849500786979 114.76100692572165
35 -1.8345987033681013 133.57330341432083
36 -3.1801894397940487 137.7582846380186
37 -1.9364955333294347 139.13141875003257
38 -2.9136332507259795 143.4966088789707
39 -1.2380675812601112 148.38223634397093
40 -4.327790730108973 151.58462983224345
41 -0.21917765506077558 158.9381440127044
42 -1.5773679496487603 164.18896384382953
43 -0.48509478889172897 174.7104083975865
44 0.3678902421379462 204.14993574950878
45 -0.3779751433758065 206.11896428081653
46 1.9146412992849946 208.13326989135442
47 -0.15464691253146157 221.44584607902624
48 1.9990735573810525 242.29822403524432
49 5.007144398929086 266.1054433499994
50 1.4423981741419993 266.92718944698623
51 1.608489545644261 312.75535109782965
52 4.602895782241831 323.17094557576416
53 2.581921530261752 357.43277401751465
54 4.307180688978406 361.84626739841684
55 5.485377328237519 370.4973655070732
56 3.098328460328048 376.1035088128469
57 3.2630325650970917 407.7184116224722
train accuracy: 0.9750700280112045
validation accuracy: 0.9334543254688445
demos: (680, 200, 19)
demo_rewards: (680,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 2278
num val_labels 2278
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:2
end of epoch 0: val_loss 0.26888071800957536, val_acc 0.8757682177348551
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.2564697948026415, val_acc 0.8880597014925373
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.21970458380887048, val_acc 0.9060579455662863
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.22456908857684105, val_acc 0.9038630377524144
trigger times: 1
end of epoch 4: val_loss 0.22053648248280094, val_acc 0.9073748902546093
trigger times: 2
end of epoch 5: val_loss 0.22996567096841983, val_acc 0.9078138718173837
trigger times: 3
end of epoch 6: val_loss 0.22428547807766344, val_acc 0.9091308165057067
trigger times: 4
end of epoch 7: val_loss 0.23974395609823881, val_acc 0.9091308165057067
trigger times: 5
end of epoch 8: val_loss 0.23995987325281687, val_acc 0.913081650570676
trigger times: 6
end of epoch 9: val_loss 0.23541486250245627, val_acc 0.9126426690079017
trigger times: 7
end of epoch 10: val_loss 0.22813172226358636, val_acc 0.9135206321334504
trigger times: 8
end of epoch 11: val_loss 0.23860525470888821, val_acc 0.913081650570676
trigger times: 9
end of epoch 12: val_loss 0.2388932185219468, val_acc 0.9143985952589991
trigger times: 10
Early stopping.
0 -27.529803164303303 -181.2724878862359
1 -25.567997124046087 -177.43401893695787
2 -27.74957837164402 -173.3433965916979
3 -14.260503168217838 -168.28140690606054
4 -28.009164346847683 -160.47753059461644
5 -24.19409980531782 -144.23826571084155
6 -18.490466102957726 -117.71150096848102
7 -16.10936633311212 -115.27865235069261
8 -12.527111157774925 -114.76315561697069
9 -17.171612292993814 -110.24807278046636
10 -16.32203479669988 -104.75415478343237
11 -15.621692089829594 -96.30769252959296
12 -15.948693567886949 -96.2033057334993
13 -16.750358746387064 -96.1579493613094
14 -23.728497248142958 -95.69265109953105
15 -17.742348509840667 -94.75279234305356
16 -16.285972747951746 -90.74083206813182
17 -14.668357409536839 -88.85541893746874
18 -15.091302273795009 -87.09210261484506
19 -16.37336861435324 -85.11706606236336
20 -12.584352838341147 -84.84678494475872
21 -9.924449327692855 -81.40890616600856
22 -14.51558317989111 -81.00674801179076
23 -14.99731871765107 -74.83072243647564
24 -13.029857422225177 -72.61362520197201
25 -13.426611968548968 -67.75435939358174
26 -6.166868259198964 -21.776950647506872
27 -12.725646131206304 -20.14259508741993
28 -8.412890672916546 -18.79124683288726
29 -12.650492561981082 -16.114376409083132
30 -10.140271622687578 -14.4711838787269
31 -5.8436245654011145 3.131373413745508
32 -8.779226324288175 15.897939912299087
33 -1.4907937668613158 26.072476847691483
34 -6.645101715694182 28.52857496230795
35 -5.053749477723613 32.744257679174034
36 -3.8325676089152694 33.789915940380645
37 -5.844191896670964 44.850019698054204
38 -2.9279379309155047 55.36888607845635
39 -3.89250585087575 68.5552575810774
40 -3.0319259958341718 81.89014686183334
41 -3.428050369839184 90.49006287375651
42 -1.7657185980933718 103.16215883545624
43 -1.9765733839012682 107.0924833923773
44 -3.3430886222049594 121.32613156169126
45 -2.7117088999366388 137.7582846380186
46 -3.77740792860277 141.8448121279314
47 -3.105637359432876 143.4966088789707
48 0.04055894981138408 204.14993574950878
49 0.37134201847948134 209.56820737620916
50 -0.5098240230872761 221.44584607902624
51 -6.860637876205146 228.89303208865374
52 3.3210893845534883 260.765498753965
53 2.2174052391201258 266.1054433499994
54 -1.670634969137609 289.88673786190077
55 3.813562538882252 290.6735314141893
56 4.807522947317921 292.3872649470778
57 1.8315753736969782 294.6666533860516
58 3.669282307193498 301.6965823877335
59 2.5382453803904355 303.57264606950895
60 5.509879387376714 306.3841750088036
61 2.6952209315932123 323.17094557576416
62 4.502905193483457 323.3596937523082
63 4.279468365712091 330.083446167866
64 5.171288484300021 361.84626739841684
65 4.519150908221491 370.4973655070732
66 4.583057224983349 434.80815706930116
67 5.90698075981345 445.9360298763626
train accuracy: 0.9721288515406162
validation accuracy: 0.9143985952589991
demos: (780, 200, 19)
demo_rewards: (780,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 3003
num val_labels 3003
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:2
end of epoch 0: val_loss 0.2090678872280847, val_acc 0.9124209124209124
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.15459657620246528, val_acc 0.9347319347319347
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.13906270324926154, val_acc 0.9383949383949384
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.13225462603667207, val_acc 0.9400599400599401
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.13072342658522706, val_acc 0.9413919413919414
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.12543937133688332, val_acc 0.9440559440559441
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.13233772517412418, val_acc 0.9470529470529471
trigger times: 1
end of epoch 7: val_loss 0.13082429528254835, val_acc 0.9460539460539461
trigger times: 2
end of epoch 8: val_loss 0.12760158581775846, val_acc 0.9437229437229437
trigger times: 3
end of epoch 9: val_loss 0.12559654471893208, val_acc 0.9470529470529471
trigger times: 4
end of epoch 10: val_loss 0.12815069459328254, val_acc 0.9483849483849484
trigger times: 5
end of epoch 11: val_loss 0.12474651092438123, val_acc 0.948051948051948
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.13104239083856464, val_acc 0.948051948051948
trigger times: 1
end of epoch 13: val_loss 0.12770149425817856, val_acc 0.9470529470529471
trigger times: 2
end of epoch 14: val_loss 0.13930843730960032, val_acc 0.948051948051948
trigger times: 3
end of epoch 15: val_loss 0.1336357022239099, val_acc 0.9487179487179487
trigger times: 4
end of epoch 16: val_loss 0.12851959837094276, val_acc 0.949050949050949
trigger times: 5
end of epoch 17: val_loss 0.13880785929990846, val_acc 0.9467199467199467
trigger times: 6
end of epoch 18: val_loss 0.13792758863250462, val_acc 0.9487179487179487
trigger times: 7
end of epoch 19: val_loss 0.13094987434764221, val_acc 0.9503829503829504
trigger times: 8
end of epoch 20: val_loss 0.13237921926702706, val_acc 0.9473859473859474
trigger times: 9
end of epoch 21: val_loss 0.1373421708889226, val_acc 0.949050949050949
trigger times: 10
Early stopping.
0 -40.093376301229 -183.31851142110742
1 -40.67536919564009 -181.2724878862359
2 -38.31800553575158 -177.43401893695787
3 -41.32858744636178 -175.14001776790738
4 -41.502753399312496 -174.2644321699064
5 -40.116787157952785 -171.38029235019366
6 -40.77560870349407 -170.9544441532664
7 -39.588758232304826 -170.66366525184918
8 -39.28923774510622 -169.9513376646074
9 -30.866747592575848 -168.28140690606054
10 -36.35052874847315 -159.88016563444475
11 -38.93167290836573 -156.17123545664032
12 -37.22587572783232 -153.91373103930508
13 -29.959942773915827 -145.7275672542709
14 -27.88185665756464 -139.1230432016261
15 -28.6620427723974 -115.27865235069261
16 -27.839825490489602 -110.57960879670767
17 -25.785532917827368 -96.6600752785595
18 -26.20474434643984 -94.75279234305356
19 -25.08356756158173 -94.29361567054993
20 -22.119588565081358 -93.8350310973348
21 -24.25542020611465 -91.36952919531996
22 -18.182874209654983 -84.28595870565519
23 -25.5957822650671 -83.35608985615931
24 -21.990935171954334 -79.20344890760724
25 -30.360951628535986 -73.52682146291956
26 -22.487210804247297 -73.00553563821455
27 -22.017983122728765 -72.61362520197201
28 -25.956246990012005 -71.46310289096745
29 -19.62111687520519 -65.85899975905869
30 -17.57327331509441 -63.048673175129544
31 -15.464061683043838 -45.80018573473104
32 -16.925469137961045 -40.13066042742665
33 -17.10523821786046 -34.487650213835884
34 -14.180488655809313 -27.194936958847403
35 -14.528432287144824 -27.08260894928669
36 -14.236485976958647 -20.131276401477507
37 -12.856389523949474 -18.79124683288726
38 -13.877146974147763 -1.8963398599782397
39 -12.114654767385218 15.180025586543433
40 -14.30921357718762 15.897939912299087
41 -12.067178412660724 27.01133577467053
42 -12.675762569298968 43.91598246817927
43 -11.9532193869818 55.12239522211616
44 -11.4765703023877 60.290641719835705
45 -8.109017048496753 107.0924833923773
46 -10.231267974944785 139.13141875003257
47 -9.937889083230402 143.51088109267857
48 -9.083727635341347 148.9125362176691
49 -10.417383974796394 151.58462983224345
50 -8.472819493093994 164.18896384382953
51 -6.486782520660199 204.14993574950878
52 -5.6454793585289735 209.04733790839154
53 -7.70824648879352 209.56820737620916
54 -11.306370641948888 228.89303208865374
55 -7.4441025334526785 240.0873832635888
56 -6.760001610731706 240.74621709213264
57 -4.31563859723974 242.29822403524432
58 -5.926977468014229 245.74508912027162
59 -5.898038089391775 258.17926442312745
60 -5.699922049243469 276.7651676683648
61 -6.366911809585872 286.93129032263715
62 -3.9274358125403523 290.6735314141893
63 -3.3215015023597516 292.3872649470778
64 -4.207804667297751 293.6282811719519
65 -6.819622356444597 294.6666533860516
66 -4.573517875862308 298.2124303007052
67 -6.3811018264095765 301.6965823877335
68 -4.6667660446692025 303.57264606950895
69 -4.920472372541553 306.3841750088036
70 -3.7579077737173066 306.72710292522027
71 -2.059725737781264 323.3596937523082
72 -3.2116064160945825 330.083446167866
73 -3.3942691296397243 343.3928836976398
74 -2.9016150091192685 361.84626739841684
75 0.16527483475510962 400.0413426933392
76 -3.288002315443009 434.80815706930116
77 -0.6424233582365559 445.9360298763626
train accuracy: 0.976610644257703
validation accuracy: 0.949050949050949
demos: (880, 200, 19)
demo_rewards: (880,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 3828
num val_labels 3828
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:3
end of epoch 0: val_loss 0.28435313602192286, val_acc 0.8680773249738767
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.2164185981334608, val_acc 0.8991640543364682
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.1888967381509648, val_acc 0.918234064785789
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.18676373374302663, val_acc 0.9119644723092999
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.16637193475352605, val_acc 0.9260710553814002
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.16690394861525346, val_acc 0.9221525600835946
trigger times: 1
end of epoch 6: val_loss 0.19136061462447182, val_acc 0.9122257053291536
trigger times: 2
end of epoch 7: val_loss 0.16229756583722604, val_acc 0.927115987460815
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.15363158400085172, val_acc 0.9333855799373041
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.15152965370173774, val_acc 0.9328631138975967
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.14618828061340158, val_acc 0.9354754440961337
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.15562916158623186, val_acc 0.9305120167189133
trigger times: 1
end of epoch 12: val_loss 0.15112189356011885, val_acc 0.930773249738767
trigger times: 2
end of epoch 13: val_loss 0.1550147618589608, val_acc 0.9318181818181818
trigger times: 3
end of epoch 14: val_loss 0.16242788163934602, val_acc 0.9302507836990596
trigger times: 4
end of epoch 15: val_loss 0.156101595701609, val_acc 0.9349529780564263
trigger times: 5
end of epoch 16: val_loss 0.14820232096713193, val_acc 0.9373040752351097
trigger times: 6
end of epoch 17: val_loss 0.15926916239432848, val_acc 0.93521421107628
trigger times: 7
end of epoch 18: val_loss 0.15025360049987457, val_acc 0.9391327063740856
trigger times: 8
end of epoch 19: val_loss 0.15855053844770747, val_acc 0.9383490073145245
trigger times: 9
end of epoch 20: val_loss 0.1591090949518082, val_acc 0.9359979101358412
trigger times: 10
Early stopping.
0 -34.20872999727726 -181.2724878862359
1 -34.96070698276162 -179.25102027435773
2 -33.720574267208576 -174.2644321699064
3 -32.39264108147472 -174.1371367923306
4 -34.32419681549072 -173.6634095247099
5 -33.51765273051569 -173.60899340916492
6 -33.89405617117882 -171.675904102178
7 -33.00122362188995 -171.03024663651152
8 -33.66032408922911 -169.6856952193804
9 -26.67501936922781 -168.28140690606054
10 -32.27849049773067 -167.85699735382585
11 -33.038488410413265 -160.47753059461644
12 -25.05445623025298 -106.96994903066323
13 -23.422725949436426 -106.29807635708414
14 -25.28386210370809 -103.11083598155567
15 -25.04910036828369 -102.15807317428477
16 -23.68467135587707 -99.19418278585783
17 -22.76314371591434 -97.22576794372458
18 -20.827338641509414 -93.29377703786322
19 -23.193567493464798 -92.6146763741173
20 -18.196779442951083 -90.84708472720422
21 -23.223539172671735 -90.35535191760539
22 -21.40822631958872 -88.85370845265362
23 -22.081387396436185 -88.7422128019347
24 -22.809000886045396 -86.13109104215737
25 -22.842298541218042 -83.92618692152796
26 -22.34572177566588 -83.1863422598594
27 -19.42369171231985 -81.84756319767409
28 -22.578735296614468 -81.75554873249479
29 -20.83857825398445 -81.44663773131116
30 -21.908030923455954 -80.64582368966838
31 -20.492685912176967 -80.48391381073206
32 -23.089696522802114 -79.37341094797618
33 -21.509409595746547 -78.18290749960688
34 -21.19795429520309 -78.01522564853663
35 -20.308474641875364 -77.86939786098138
36 -15.032761581940576 -77.58277354246064
37 -19.69601519871503 -74.83072243647564
38 -18.409133008361096 -72.56115827647086
39 -20.800968590425327 -67.87948087083302
40 -22.467100650072098 -67.1735577895282
41 -20.665906133130193 -66.97253936750184
42 -16.921425262858975 -66.32315248879468
43 -18.5738217625767 -63.788594242997156
44 -14.231606055880548 -63.048673175129544
45 -16.395327702979557 -61.77865311608099
46 -17.753256819676608 -61.18974803141127
47 -15.6220115092583 -59.23492089348715
48 -17.37051663757302 -54.88149440041123
49 -13.313057366875 -54.57378866047313
50 -13.537922276067548 -54.495852149447366
51 -19.76536557963118 -50.11773601698566
52 -15.883080400526524 -49.448255632219045
53 -15.340917176014045 -40.55045024852991
54 -13.590483142063022 -38.71920473617941
55 -12.342860296717845 -21.611873952715207
56 -13.843207674566656 -21.16158033494747
57 -12.000258685089648 8.109549320576296
58 -10.343678165692836 15.180025586543433
59 -6.448001247277716 26.072476847691483
60 -9.989014894352295 27.01133577467053
61 -8.145918182097375 55.36888607845635
62 -9.713917664194014 65.1680640695934
63 -8.119994304142892 101.50661112827956
64 -5.6580488322651945 116.17998438016197
65 -6.544483619858511 133.04858367101036
66 -6.523363084619632 133.57330341432083
67 -5.652989443668048 164.18896384382953
68 -5.853207380336244 179.01942269845122
69 -6.390078092052136 189.5748341940192
70 -3.6994144918135135 205.26719391532652
71 -3.9419313026592135 209.04733790839154
72 -4.45886476529995 225.14588563919364
73 -5.767561096319696 226.62816707088624
74 -4.972903831774602 233.63745376530167
75 -3.4314384997123852 242.29822403524432
76 -3.9676658659009263 245.74508912027162
77 -4.7740287199558225 258.17926442312745
78 -4.6263748763885815 289.88673786190077
79 -3.3076233153697103 291.32952200448
80 -3.559630264993757 306.3841750088036
81 -1.5836790154426126 330.083446167866
82 -2.6506447199062677 334.9429820211255
83 -2.5367308552376926 343.3928836976398
84 -1.7635916633589659 351.0987697960898
85 -1.699443533201702 361.84626739841684
86 -4.567127383663319 420.94996211459903
87 -5.436686179833487 423.1355871564197
train accuracy: 0.9753501400560224
validation accuracy: 0.9359979101358412
