demos: (480, 200, 19)
demo_rewards: (480,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 1128
num val_labels 1128
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:1
end of epoch 0: val_loss 0.22041019215444116, val_acc 0.9078014184397163
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.19811488039688874, val_acc 0.9148936170212766
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.19191186840039387, val_acc 0.9228723404255319
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.18110321673731333, val_acc 0.925531914893617
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.194742299840467, val_acc 0.9175531914893617
trigger times: 1
end of epoch 5: val_loss 0.19890437829123173, val_acc 0.9184397163120568
trigger times: 2
end of epoch 6: val_loss 0.19694690444026683, val_acc 0.9157801418439716
trigger times: 3
end of epoch 7: val_loss 0.1919001132552254, val_acc 0.9219858156028369
trigger times: 4
end of epoch 8: val_loss 0.19534797362718243, val_acc 0.9131205673758865
trigger times: 5
end of epoch 9: val_loss 0.20144723646460921, val_acc 0.9166666666666666
trigger times: 6
end of epoch 10: val_loss 0.19517263466931264, val_acc 0.9210992907801419
trigger times: 7
end of epoch 11: val_loss 0.20482450908271682, val_acc 0.9113475177304965
trigger times: 8
end of epoch 12: val_loss 0.19228205978323246, val_acc 0.9166666666666666
trigger times: 9
end of epoch 13: val_loss 0.21083902962991022, val_acc 0.9193262411347518
trigger times: 10
Early stopping.
0 -18.004411665839143 -103.1521388660611
1 -19.083411435829476 -87.66500551980913
2 -15.465350663755089 -84.84678494475872
3 -16.01868487536558 -81.84756319767409
4 -13.431115190964192 -81.40890616600856
5 -18.321424681693316 -80.85477622402837
6 -17.771857930812985 -80.48391381073206
7 -11.6964770257473 -80.25156008912637
8 -10.378413174767047 -33.34809776864483
9 -12.078606336377561 -20.418009315237114
10 -11.216257102380041 -20.131276401477507
11 -8.59592589829117 -15.362938437720853
12 -11.049866575980559 -1.8963398599782397
13 -11.015636441588867 -0.5728421252635456
14 -8.25655517843552 4.383180415256293
15 -6.501125277020037 6.8392346078963
16 -11.028234740253538 6.841662837067183
17 -8.530169938574545 27.01133577467053
18 -4.080184718128294 33.68434584390655
19 -6.896728907246143 35.05308871945419
20 -7.270528380002361 50.52841485696464
21 -2.9374121548607945 56.635725849291575
22 -7.088530259206891 57.76744549995627
23 -6.356554535217583 69.90236244353298
24 -6.70872945379233 81.30023532085879
25 -0.43574248417280614 92.20740196031777
26 -3.9780374999390915 92.26562045392546
27 -3.8644629425834864 101.50661112827956
28 -5.360764235374518 125.53777532052302
29 -3.308430311270058 157.73723206248386
30 -0.8993297659326345 158.4543688649787
31 -4.2214656204450876 169.19923674906562
32 0.46904945676214993 185.35424212127486
33 -1.686273108702153 204.5598807600962
34 1.9373370986431837 205.26719391532652
35 6.202840181067586 214.6263040548001
36 -0.8668814778793603 233.63745376530167
37 6.746973644476384 266.3586992205008
38 4.067282045958564 284.9323058364518
39 0.16788467625156045 291.32952200448
40 6.914136794861406 320.0946312511725
41 5.531006282195449 324.0197976082746
42 4.650242235511541 333.0407029156643
43 7.897209285292774 361.84626739841684
44 7.442427721340209 390.7517184882046
45 9.301316555589437 400.0413426933392
46 6.795252521056682 403.13796958654353
47 4.917593996040523 423.1355871564197
train accuracy: 0.9677871148459384
validation accuracy: 0.9193262411347518
demos: (580, 200, 19)
demo_rewards: (580,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 1653
num val_labels 1653
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:2
end of epoch 0: val_loss 0.3911245831390283, val_acc 0.8548094373865699
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.267881353308904, val_acc 0.8765880217785844
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.21800716479458812, val_acc 0.895946763460375
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.22030917881961157, val_acc 0.8995765275257108
trigger times: 1
end of epoch 4: val_loss 0.2080632994994932, val_acc 0.9086509376890503
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.2158455232386552, val_acc 0.9019963702359347
trigger times: 1
end of epoch 6: val_loss 0.23170537090192952, val_acc 0.9032062915910466
trigger times: 2
end of epoch 7: val_loss 0.2119300361973406, val_acc 0.9098608590441621
trigger times: 3
end of epoch 8: val_loss 0.19980188748244893, val_acc 0.9159104658197217
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.21104205813547328, val_acc 0.9159104658197217
trigger times: 1
end of epoch 10: val_loss 0.21348536333048862, val_acc 0.9147005444646098
trigger times: 2
end of epoch 11: val_loss 0.21756881281091026, val_acc 0.91167574107683
trigger times: 3
end of epoch 12: val_loss 0.21627236623616505, val_acc 0.9134906231094979
trigger times: 4
end of epoch 13: val_loss 0.2325016723714503, val_acc 0.9074410163339383
trigger times: 5
end of epoch 14: val_loss 0.2409700478330628, val_acc 0.9062310949788264
trigger times: 6
end of epoch 15: val_loss 0.2376173723410759, val_acc 0.9092558983666061
trigger times: 7
end of epoch 16: val_loss 0.22253800782265865, val_acc 0.9165154264972777
trigger times: 8
end of epoch 17: val_loss 0.23245149113036165, val_acc 0.911070780399274
trigger times: 9
end of epoch 18: val_loss 0.26329242395500846, val_acc 0.9032062915910466
trigger times: 10
Early stopping.
0 -25.65043909661472 -110.57960879670767
1 -18.837904408574104 -100.66675527026814
2 -22.816938491538167 -97.22576794372458
3 -17.38137622177601 -86.60279916124063
4 -17.91462374245748 -86.40637800193252
5 -19.243081980384886 -84.09148558861091
6 -16.714138782117516 -82.55824152292685
7 -17.970420595258474 -81.95553196168702
8 -17.66767886048183 -81.64289387881709
9 -16.39403651934117 -81.59001396328485
10 -19.61033874936402 -80.48391381073206
11 -15.264765912666917 -79.78060319968824
12 -16.704826548695564 -79.77349111172407
13 -20.313140373677015 -76.23442589037947
14 -16.659214342944324 -75.0210639505327
15 -22.147779861465096 -74.17956457851813
16 -15.19888296769932 -72.06078791443208
17 -13.878375595086254 -68.25744829885676
18 -15.948729475756409 -65.7774597497154
19 -12.475577299832366 -53.814051919860674
20 -14.676785266026855 -52.39984222917899
21 -13.996967937098816 -49.448255632219045
22 -17.537888632621616 -48.71971149808798
23 -9.977757372893393 -38.434791244243044
24 -11.892720845993608 -33.695690388277306
25 -8.190933015430346 -21.70967376674228
26 -10.6187999304384 -21.16158033494747
27 -11.767529875040054 -20.05488751117805
28 -10.288793541723862 -13.926776816108745
29 -5.225691834464669 55.36888607845635
30 -5.352307307097362 58.63093231433321
31 0.805383586557582 66.874978985273
32 -2.463564430596307 87.3487075539824
33 -4.402601569076069 91.33884405649833
34 -1.9689318450400606 92.20740196031777
35 -2.6027216978836805 111.62538713568061
36 -5.090339852089528 133.04858367101036
37 -4.003359252004884 158.95650203412697
38 -2.2588872712804005 164.18896384382953
39 -2.306766883470118 168.20510565770127
40 -1.7627980990801007 174.64466434998226
41 -0.46773978788405657 182.96560001968115
42 -1.804636181332171 185.35424212127486
43 -2.395670104888268 204.5598807600962
44 -0.27938547101803124 210.60150900542712
45 3.7558118131710216 214.6263040548001
46 -0.37792457279283553 235.04407254128563
47 -1.3098441999172792 237.90451471965713
48 2.529181567253545 278.9263530439534
49 -0.7784124056342989 291.32952200448
50 4.451576697872952 296.5022879483941
51 0.318838051520288 304.44976263688386
52 -0.5604158905334771 308.09696350962963
53 3.572326181223616 319.02000876252197
54 2.1973894818220288 323.07390219598693
55 0.9714350136637222 333.0407029156643
56 5.749209622852504 413.88199240361837
57 -0.8066707149846479 423.1355871564197
train accuracy: 0.9568627450980393
validation accuracy: 0.9032062915910466
demos: (680, 200, 19)
demo_rewards: (680,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 2278
num val_labels 2278
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:3
end of epoch 0: val_loss 0.26907144944341993, val_acc 0.897278314310799
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.2444101712773681, val_acc 0.897278314310799
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.21555418121152156, val_acc 0.9104477611940298
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.21682865544330682, val_acc 0.9108867427568043
trigger times: 1
end of epoch 4: val_loss 0.20486469691069384, val_acc 0.9126426690079017
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.22891701637419193, val_acc 0.9043020193151887
trigger times: 1
end of epoch 6: val_loss 0.21665648979454577, val_acc 0.9122036874451273
trigger times: 2
end of epoch 7: val_loss 0.18796552092037824, val_acc 0.9165935030728709
trigger times: 0
saving model weights...
end of epoch 8: val_loss 0.2110006169622373, val_acc 0.9100087796312555
trigger times: 1
end of epoch 9: val_loss 0.2042736028247926, val_acc 0.9152765583845478
trigger times: 2
end of epoch 10: val_loss 0.18739581607713066, val_acc 0.9139596136962248
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.18886250282150974, val_acc 0.9148375768217735
trigger times: 1
end of epoch 12: val_loss 0.20606243200957708, val_acc 0.9152765583845478
trigger times: 2
end of epoch 13: val_loss 0.18440071421285223, val_acc 0.9143985952589991
trigger times: 0
saving model weights...
end of epoch 14: val_loss 0.18767734686998475, val_acc 0.9165935030728709
trigger times: 1
end of epoch 15: val_loss 0.17959269848952383, val_acc 0.917910447761194
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.1783203369317582, val_acc 0.9187884108867428
trigger times: 0
saving model weights...
end of epoch 17: val_loss 0.1795072153262961, val_acc 0.9205443371378402
trigger times: 1
end of epoch 18: val_loss 0.19535820910982654, val_acc 0.9152765583845478
trigger times: 2
end of epoch 19: val_loss 0.18606008778899072, val_acc 0.9196663740122915
trigger times: 3
end of epoch 20: val_loss 0.18841025417429033, val_acc 0.9201053555750659
trigger times: 4
end of epoch 21: val_loss 0.19227817414654444, val_acc 0.9161545215100966
trigger times: 5
end of epoch 22: val_loss 0.1895397372227369, val_acc 0.9161545215100966
trigger times: 6
end of epoch 23: val_loss 0.1855523012856872, val_acc 0.9205443371378402
trigger times: 7
end of epoch 24: val_loss 0.21061289243333026, val_acc 0.9174714661984197
trigger times: 8
end of epoch 25: val_loss 0.19406402509557225, val_acc 0.9187884108867428
trigger times: 9
end of epoch 26: val_loss 0.2152910790622097, val_acc 0.9174714661984197
trigger times: 10
Early stopping.
0 -26.090932906139642 -110.57960879670767
1 -21.703091762727126 -106.96994903066323
2 -19.014612210798077 -103.1521388660611
3 -15.032135913264938 -101.56590801090363
4 -24.57573871780187 -100.66675527026814
5 -13.899146910582203 -97.95729555753762
6 -21.46327041229233 -88.85541893746874
7 -18.335210727527738 -86.40637800193252
8 -15.92390484502539 -83.04498690176096
9 -15.725677276030183 -80.96555936343618
10 -13.991438952391036 -80.85477622402837
11 -15.309864426031709 -75.08824294464698
12 -15.626381497830153 -71.22531133858006
13 -8.859777175588533 -67.85891443557135
14 -13.289988413453102 -61.05586462768669
15 -13.135942827444524 -58.84101958867363
16 -17.71421893965453 -57.249802418294564
17 -12.604966573417187 -56.97345028192729
18 -12.664922603406012 -56.88135323658909
19 -12.499421263113618 -56.84362143123344
20 -12.868473631795496 -56.27935231460382
21 -12.79783022031188 -55.974804798928766
22 -10.917930889874697 -55.83498617386148
23 -10.005039767129347 -55.305479009750066
24 -11.631278072833084 -53.626705466814165
25 -13.363174496218562 -52.708706070044116
26 -12.171999843791127 -52.40737534648188
27 -7.555513582658023 -52.39984222917899
28 -10.13817615271546 -51.48075468789002
29 -11.335171684622765 -51.07320004017745
30 -10.99895000772085 -49.448255632219045
31 -6.26533637335524 -6.037980325145148
32 -3.8201451111235656 0.971479121789783
33 -4.9387273687752895 3.131373413745508
34 -5.5600118033908075 25.866033245873773
35 -3.0139947230927646 33.789915940380645
36 -4.357898899004795 39.960284600421595
37 -3.010081533808261 57.76744549995627
38 -2.885193058522418 58.63093231433321
39 -2.349937746534124 71.05660111371789
40 -0.5214432431384921 107.71006781863467
41 -1.4798068965319544 113.54769019058138
42 -4.277245977311395 114.76100692572165
43 1.110937706544064 142.42451181000234
44 -0.5069855441106483 158.95650203412697
45 -0.14754172088578343 164.18896384382953
46 1.6538135041482747 168.20510565770127
47 0.787853320594877 185.35424212127486
48 -0.07466690707951784 204.5598807600962
49 7.250151256565005 214.6263040548001
50 1.7260282492497936 221.44584607902624
51 5.617684800177813 291.7555455443344
52 3.644742696080357 306.72710292522027
53 4.777894692495465 317.59854966426605
54 6.95033565768972 320.0946312511725
55 5.573451274540275 320.89937398550296
56 3.2979544845875353 323.07390219598693
57 3.604673153720796 327.0892269073537
58 4.356351915514097 333.0407029156643
59 6.069303125841543 334.8245118200832
60 6.78716347226873 336.2643662087101
61 6.776325918268412 338.06425040083286
62 5.131824481766671 341.5861141114972
63 6.685143860522658 345.467793071763
64 3.3251722776331007 381.301733003876
65 4.487623214256018 384.24601373121897
66 5.86916369618848 396.0376311459591
67 2.474239200062584 423.1355871564197
train accuracy: 0.9721288515406162
validation accuracy: 0.9174714661984197
demos: (780, 200, 19)
demo_rewards: (780,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 3003
num val_labels 3003
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:1
end of epoch 0: val_loss 0.29029730992281527, val_acc 0.8734598734598734
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.25367803099843794, val_acc 0.8941058941058941
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.27214081408388896, val_acc 0.8811188811188811
trigger times: 1
end of epoch 3: val_loss 0.28803271202901065, val_acc 0.8841158841158842
trigger times: 2
end of epoch 4: val_loss 0.28107257363072535, val_acc 0.8864468864468864
trigger times: 3
end of epoch 5: val_loss 0.28948746658130964, val_acc 0.8801198801198801
trigger times: 4
end of epoch 6: val_loss 0.30399333562232167, val_acc 0.8804528804528805
trigger times: 5
end of epoch 7: val_loss 0.2898588009054729, val_acc 0.8867798867798867
trigger times: 6
end of epoch 8: val_loss 0.2961762469667735, val_acc 0.8837828837828838
trigger times: 7
end of epoch 9: val_loss 0.2670986058115168, val_acc 0.8954378954378954
trigger times: 8
end of epoch 10: val_loss 0.2704132411066322, val_acc 0.8934398934398934
trigger times: 9
end of epoch 11: val_loss 0.24933780291422017, val_acc 0.8997668997668997
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.28560595818922213, val_acc 0.8981018981018981
trigger times: 1
end of epoch 13: val_loss 0.26665129887756817, val_acc 0.8881118881118881
trigger times: 2
end of epoch 14: val_loss 0.2906609276273428, val_acc 0.8954378954378954
trigger times: 3
end of epoch 15: val_loss 0.2588312995441381, val_acc 0.9010989010989011
trigger times: 4
end of epoch 16: val_loss 0.3339559903139646, val_acc 0.8861138861138861
trigger times: 5
end of epoch 17: val_loss 0.29099588517930713, val_acc 0.8941058941058941
trigger times: 6
end of epoch 18: val_loss 0.25532372315901164, val_acc 0.9020979020979021
trigger times: 7
end of epoch 19: val_loss 0.27402843648801994, val_acc 0.8947718947718948
trigger times: 8
end of epoch 20: val_loss 0.25840208104244766, val_acc 0.9014319014319014
trigger times: 9
end of epoch 21: val_loss 0.30109184336903577, val_acc 0.8914418914418915
trigger times: 10
Early stopping.
0 -19.411229957826436 -111.04132768200442
1 -18.35237500630319 -105.88061755154125
2 -19.493654819205403 -100.34835270036645
3 -25.160178849007934 -94.88284189624625
4 -24.374490751186386 -94.32862595525613
5 -22.563896554289386 -89.28976343040237
6 -24.474572761915624 -86.60279916124063
7 -21.62267568707466 -84.98332803623823
8 -17.639391450909898 -84.84678494475872
9 -21.23872831184417 -84.61662760311957
10 -21.703888952266425 -81.73541981419673
11 -22.284438529983163 -81.64289387881709
12 -21.54352613678202 -81.59001396328485
13 -18.769930109381676 -78.49950928065765
14 -25.322750927414745 -78.12285027441519
15 -19.88795034121722 -74.67838389230367
16 -20.03535247500986 -74.12887018406042
17 -20.570598849095404 -72.73052374293789
18 -20.969459731597453 -71.22531133858006
19 -17.570076113101095 -71.0097189898932
20 -13.004026423674077 -67.85891443557135
21 -16.338932803831995 -67.75435939358174
22 -19.00574897462502 -67.13206402828338
23 -20.03013023780659 -65.182790905733
24 -20.07499466324225 -63.914777062561654
25 -21.213853338733315 -61.18412656844619
26 -22.755751724354923 -58.21110681570058
27 -20.4119824974332 -57.22348494163697
28 -23.116058458108455 -57.15998239442794
29 -14.569444918539375 -56.971402586042636
30 -19.441648712847382 -56.23567502531983
31 -19.350411523599178 -56.003714705544816
32 -18.56035235989839 -55.7154797164491
33 -19.023799922317266 -55.29324970038961
34 -20.412016339600086 -54.413355374563544
35 -15.276263865176588 -53.30346624552765
36 -15.298161507584155 -52.39984222917899
37 -13.844739119987935 -50.319068336803426
38 -17.91511330101639 -46.902580140149695
39 -15.25895277922973 -36.95797988815738
40 -14.903046539984643 -35.2532611648008
41 -13.170272610615939 -33.34809776864483
42 -11.836724324617535 -21.776950647506872
43 -15.247593604493886 -16.890197526450656
44 -12.942081166896969 -6.037980325145148
45 -11.571594990324229 -1.8963398599782397
46 -11.114417807664722 4.093881034642614
47 -10.535457098041661 13.433614752242036
48 -11.941295042168349 20.530588087221908
49 -12.938527749385685 30.589233183073496
50 -11.193984744139016 35.05308871945419
51 -9.469373440602794 44.23272913794082
52 -10.090583159588277 55.66923778730837
53 -8.971679129695985 71.05660111371789
54 -9.990569139365107 74.26310022402075
55 -8.193581635132432 92.20740196031777
56 -8.992692844942212 103.76850790553507
57 -8.767677130410448 111.92489522247226
58 -8.95557751506567 141.63811961722755
59 -7.408295955043286 189.56275247801355
60 -2.6048866843339056 214.6263040548001
61 -5.093716784613207 215.42211929419062
62 -3.7763140443712473 262.49206401593506
63 -5.322625393047929 301.8768216958117
64 -5.044015804771334 304.44976263688386
65 -4.076055956538767 306.72710292522027
66 -3.8787291082553566 312.75535109782965
67 -2.2875879302155226 319.02000876252197
68 -2.5211943476460874 320.89937398550296
69 -2.581356862327084 334.8245118200832
70 -1.3833299111574888 338.06425040083286
71 -3.2041855873540044 345.467793071763
72 -0.9521352226147428 374.0442710820621
73 -4.643542822217569 381.301733003876
74 -4.176270462805405 384.24601373121897
75 -1.3458393984474242 396.0376311459591
76 -0.4119290984235704 414.43301139651703
77 -0.6543630873784423 444.55401748791013
train accuracy: 0.969187675070028
validation accuracy: 0.8914418914418915
demos: (880, 200, 19)
demo_rewards: (880,)
maximum traj length 200
maximum traj length 200
num train_obs 7140
num train_labels 7140
num val_obs 3828
num val_labels 3828
ModuleList(
  (0): Linear(in_features=19, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Training reward model from scratch...
Total number of parameters: 10880
Number of trainable paramters: 10880
device: cuda:2
end of epoch 0: val_loss 0.35402871849503276, val_acc 0.8429989550679206
trigger times: 0
saving model weights...
end of epoch 1: val_loss 0.35346975110132395, val_acc 0.8463949843260188
trigger times: 0
saving model weights...
end of epoch 2: val_loss 0.2917054385093406, val_acc 0.8670323928944619
trigger times: 0
saving model weights...
end of epoch 3: val_loss 0.27761413182486716, val_acc 0.8743469174503657
trigger times: 0
saving model weights...
end of epoch 4: val_loss 0.24251167705768628, val_acc 0.8868861024033438
trigger times: 0
saving model weights...
end of epoch 5: val_loss 0.2348388114451375, val_acc 0.8918495297805643
trigger times: 0
saving model weights...
end of epoch 6: val_loss 0.21439116504211944, val_acc 0.8999477533960293
trigger times: 0
saving model weights...
end of epoch 7: val_loss 0.22854150616233912, val_acc 0.8926332288401254
trigger times: 1
end of epoch 8: val_loss 0.19703068168705257, val_acc 0.9085684430512017
trigger times: 0
saving model weights...
end of epoch 9: val_loss 0.1957017487109842, val_acc 0.9114420062695925
trigger times: 0
saving model weights...
end of epoch 10: val_loss 0.19338788957841457, val_acc 0.9111807732497388
trigger times: 0
saving model weights...
end of epoch 11: val_loss 0.19184132406138343, val_acc 0.912748171368861
trigger times: 0
saving model weights...
end of epoch 12: val_loss 0.17726023670083357, val_acc 0.9174503657262278
trigger times: 0
saving model weights...
end of epoch 13: val_loss 0.19137955151339678, val_acc 0.9135318704284221
trigger times: 1
end of epoch 14: val_loss 0.1879446691245715, val_acc 0.9119644723092999
trigger times: 2
end of epoch 15: val_loss 0.17386765550077452, val_acc 0.9205851619644723
trigger times: 0
saving model weights...
end of epoch 16: val_loss 0.16495823578030702, val_acc 0.9260710553814002
trigger times: 0
saving model weights...
end of epoch 17: val_loss 0.1608906365730116, val_acc 0.9260710553814002
trigger times: 0
saving model weights...
end of epoch 18: val_loss 0.19085980490776427, val_acc 0.9148380355276907
trigger times: 1
end of epoch 19: val_loss 0.18065463401017995, val_acc 0.9156217345872518
trigger times: 2
end of epoch 20: val_loss 0.1730105338950757, val_acc 0.920846394984326
trigger times: 3
end of epoch 21: val_loss 0.16627064670315764, val_acc 0.9239811912225705
trigger times: 4
end of epoch 22: val_loss 0.20229684056690264, val_acc 0.9132706374085684
trigger times: 5
end of epoch 23: val_loss 0.1561290341825274, val_acc 0.9258098223615465
trigger times: 0
saving model weights...
end of epoch 24: val_loss 0.17609156542061344, val_acc 0.9211076280041798
trigger times: 1
end of epoch 25: val_loss 0.15929984017380672, val_acc 0.9273772204806687
trigger times: 2
end of epoch 26: val_loss 0.15453772327383944, val_acc 0.9284221525600836
trigger times: 0
saving model weights...
end of epoch 27: val_loss 0.16395790723310993, val_acc 0.9281609195402298
trigger times: 1
end of epoch 28: val_loss 0.181631694879199, val_acc 0.9192789968652038
trigger times: 2
end of epoch 29: val_loss 0.16045048602730674, val_acc 0.9284221525600836
trigger times: 3
end of epoch 30: val_loss 0.15770305682817115, val_acc 0.9292058516196448
trigger times: 4
end of epoch 31: val_loss 0.17165819439527127, val_acc 0.9273772204806687
trigger times: 5
end of epoch 32: val_loss 0.1658408136139509, val_acc 0.9292058516196448
trigger times: 6
end of epoch 33: val_loss 0.16322338153447374, val_acc 0.9320794148380356
trigger times: 7
end of epoch 34: val_loss 0.16320383775068514, val_acc 0.9265935214211076
trigger times: 8
end of epoch 35: val_loss 0.1591302338644572, val_acc 0.9341692789968652
trigger times: 9
end of epoch 36: val_loss 0.1690598478796668, val_acc 0.9281609195402298
trigger times: 10
Early stopping.
0 -33.003669161349535 -121.00676721393931
1 -24.888113065622747 -110.24807278046636
2 -25.492336686700583 -106.3952241264332
3 -25.49790952913463 -104.46720335574521
4 -21.786564108915627 -104.32182825064179
5 -23.624206978827715 -102.28883030087748
6 -23.69488330092281 -100.73794776548738
7 -25.728930613957345 -100.3317082402663
8 -23.505146025680006 -100.24539753679875
9 -20.94828615244478 -95.36551244241093
10 -22.106419895775616 -94.75279234305356
11 -16.594897890696302 -84.84678494475872
12 -18.896758154965937 -82.77260854194975
13 -19.392088127322495 -81.95553196168702
14 -17.882996107451618 -81.9134488838375
15 -21.220367622561753 -81.90234450328067
16 -18.408750203438103 -81.73541981419673
17 -18.676217306405306 -81.57670400275852
18 -15.884336080402136 -81.44857474627001
19 -18.73903994075954 -81.4417666295641
20 -17.892926977481693 -81.17041527269753
21 -18.143998836399987 -80.96555936343618
22 -20.664037873968482 -80.85477622402837
23 -13.072039144812152 -78.96515452668575
24 -18.144302221597172 -78.41738157747275
25 -19.762287240941077 -78.25248960973302
26 -17.95683535328135 -78.11720276327773
27 -19.857444036751986 -77.531820485362
28 -19.0217399392277 -76.48145549257502
29 -16.261221202090383 -76.1552673387719
30 -18.17079868260771 -75.59392400684068
31 -16.86774332250934 -74.12887018406042
32 -16.495164711494 -73.82321968568533
33 -12.706453965976834 -73.5287660536066
34 -15.821654562838376 -72.28179976294354
35 -16.426686827559024 -69.01056893862284
36 -15.52737889625132 -68.03052882302825
37 -16.14200246054679 -66.12758372945994
38 -13.844550367677584 -65.44841493774571
39 -12.985931638628244 -65.182790905733
40 -16.97218013741076 -64.68943751357592
41 -14.494983488693833 -57.22348494163697
42 -13.441492419224232 -56.27935231460382
43 -12.886885863728821 -56.23567502531983
44 -14.028763535898179 -55.99342239542531
45 -9.92215226800181 -55.83498617386148
46 -10.604403143748641 -52.674911235288164
47 -12.360684923361987 -52.39984222917899
48 -9.939136686269194 -51.48075468789002
49 -10.831154226325452 -42.7774040227911
50 -12.647509328089654 -36.37950117215486
51 -12.761683438671753 -33.695690388277306
52 -10.125744001445128 -27.194936958847403
53 -12.644959224504419 -20.05488751117805
54 -8.252513917163014 -17.565172259114018
55 -12.75377622967062 -16.890197526450656
56 -8.666530529910233 -0.5728421252635456
57 -10.60436276247492 8.109549320576296
58 -6.428533350001089 24.11700288682878
59 -6.923472583293915 35.05308871945419
60 -2.475677710957825 37.24561873455588
61 -6.505270939145703 44.23272913794082
62 -2.97374889429193 55.92026357237822
63 -0.5532755861058831 66.874978985273
64 -7.598978412337601 81.30023532085879
65 -4.194257429233403 92.26562045392546
66 -2.190510912798345 111.62538713568061
67 -5.885074132820591 139.13141875003257
68 -3.1924810535274446 164.96088964947137
69 -2.4872878966853023 174.64466434998226
70 2.777115396456793 214.6263040548001
71 -0.0467516053467989 215.42211929419062
72 -1.0200678030960262 221.44584607902624
73 1.2192345303483307 240.0873832635888
74 -1.9694240633398294 251.00572441045193
75 1.1478044502437115 262.49206401593506
76 2.1972840186208487 266.3586992205008
77 2.025601861998439 277.0432845755202
78 5.145359405782074 294.9326012268983
79 1.7251063191797584 301.8768216958117
80 1.835593415191397 302.1125613831873
81 2.5754463961347938 312.75535109782965
82 1.258552290033549 314.5744211003952
83 4.1329542263410985 338.06425040083286
84 1.3739799056202173 345.467793071763
85 5.6390934092924 374.0442710820621
86 3.7890083172824234 381.301733003876
87 6.101933414116502 384.24601373121897
train accuracy: 0.9780112044817927
validation accuracy: 0.9281609195402298
