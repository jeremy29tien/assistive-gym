demo lengths 200
demos: (120, 200, 12)
demo_rewards: (120,)
[-147.46364928 -142.23388835 -122.62366971 -118.44560088 -116.1083748
 -115.4367786  -111.12884204 -110.83967461  -99.79565201  -99.64931501
  -96.59530824  -95.93196513  -95.2610607   -79.4622591   -76.17790383
  -73.19589637  -65.39617499  -62.455946    -61.13021271  -60.1343363
  -50.14643167  -48.67846358  -47.40424202  -43.39777325  -41.86614966
  -36.32048246  -32.74837227  -32.32929369  -25.77093927  -25.59499594
  -25.57213436  -22.39844637  -20.39120992  -14.48148891  -12.30909187
  -11.39102442  -10.90201702   -9.33568969    0.74191804    3.83189024
    3.98800904   10.76441523   12.93427412   14.20520687   24.91083686
   33.81267702   40.19882336   43.5062037    54.79421159   62.69478416
   74.22571892   84.10165836   88.64858938   92.13810233   95.59709195
   98.11620352   98.43438298  106.6069539   113.06528437  117.98751873
  131.64414318  132.2879436   133.11348989  145.24215615  158.38078213
  158.50808437  167.96954732  168.21394205  170.47202355  173.61957394
  174.15080882  181.61209907  183.63062646  195.24714457  197.54995255
  198.07853498  208.43810158  221.98821142  225.96493031  234.91253006
  236.4104291   245.5887449   245.81351896  246.75481392  249.51084172
  251.35714482  253.85724382  257.91690315  263.15565254  273.4391394
  276.57432091  281.20676917  283.08945277  285.72871086  290.13944852
  291.92437615  302.36442469  306.17125326  312.37244815  321.23740112
  321.29051072  321.65930958  325.94333209  327.94107559  337.7280743
  339.661882    342.79982242  347.93295476  353.38121002  354.05687467
  355.77598362  361.50704767  374.01284717  374.86035192  390.91142776
  409.5886739   424.42641682  425.34255271  430.80730528  445.22536513]
maximum traj length 200
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
ModuleList(
  (0): Linear(in_features=12, out_features=1, bias=False)
)
Total number of parameters: 12
Number of trainable paramters: 12
device: cuda:0
end of epoch 0: val_loss 0.0006519062203727799, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0019, -0.0006,  0.0003,  0.0002,  0.0022,  0.0004,  0.0010, -0.0013,
          0.0028,  0.0005, -0.0018,  0.0230]], device='cuda:0'))])
end of epoch 1: val_loss 4.559293563772826e-06, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0013,  0.0129,  0.0124,  0.0070,  0.0091,  0.0001, -0.0012, -0.0026,
          0.0067,  0.0069, -0.0138,  0.0418]], device='cuda:0'))])
end of epoch 2: val_loss 0.00030988478308852763, val_acc 1.0
trigger times: 1
end of epoch 3: val_loss 0.00037166300167086776, val_acc 1.0
trigger times: 2
end of epoch 4: val_loss 0.00038635777420793717, val_acc 1.0
trigger times: 3
end of epoch 5: val_loss 0.0005223549874261835, val_acc 1.0
trigger times: 4
end of epoch 6: val_loss 0.00022133873095551594, val_acc 1.0
trigger times: 5
end of epoch 7: val_loss 0.0003319796952396814, val_acc 1.0
trigger times: 6
end of epoch 8: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-7.5223e-03, -6.9130e-03, -2.5987e-04, -5.9000e-05,  7.1791e-02,
         -2.0126e-02, -4.5617e-03,  3.9941e-04,  2.6892e-03, -9.5172e-04,
         -7.2358e-03,  1.9750e-01]], device='cuda:0'))])
end of epoch 9: val_loss 0.0005454516488219951, val_acc 1.0
trigger times: 1
end of epoch 10: val_loss 8.344646289515368e-09, val_acc 1.0
trigger times: 2
end of epoch 11: val_loss 1.3797284125480757e-05, val_acc 1.0
trigger times: 3
end of epoch 12: val_loss 2.72204963902567e-06, val_acc 1.0
trigger times: 4
end of epoch 13: val_loss 5.388178423615386e-07, val_acc 1.0
trigger times: 5
end of epoch 14: val_loss 0.0004346813442223407, val_acc 1.0
trigger times: 6
end of epoch 15: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0254,  0.0453, -0.0106, -0.0163,  0.0262, -0.0140,  0.0238,  0.0217,
          0.0315, -0.0237, -0.0263,  0.0693]], device='cuda:0'))])
end of epoch 16: val_loss 4.827965934595113e-08, val_acc 1.0
trigger times: 1
end of epoch 17: val_loss 0.00014299970439228814, val_acc 1.0
trigger times: 2
end of epoch 18: val_loss 0.00012181952819116048, val_acc 1.0
trigger times: 3
end of epoch 19: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0424,  0.0556,  0.0442, -0.0229,  0.0589, -0.0346,  0.0460,  0.0466,
          0.0516,  0.0351, -0.0510,  0.0731]], device='cuda:0'))])
end of epoch 20: val_loss 2.799265799612982e-05, val_acc 1.0
trigger times: 1
end of epoch 21: val_loss 5.5602026882866085e-05, val_acc 1.0
trigger times: 2
end of epoch 22: val_loss 4.3152385607925227e-07, val_acc 1.0
trigger times: 3
end of epoch 23: val_loss 0.0006912456069920836, val_acc 1.0
trigger times: 4
end of epoch 24: val_loss 4.467058968366189e-05, val_acc 1.0
trigger times: 5
end of epoch 25: val_loss 0.0002800255428545384, val_acc 1.0
trigger times: 6
end of epoch 26: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.0014,  0.0152, -0.0086,  0.0033,  0.0373, -0.0481,  0.0370, -0.0024,
          0.0360,  0.0287, -0.0548,  0.0722]], device='cuda:0'))])
end of epoch 27: val_loss 0.00027004778144331, val_acc 1.0
trigger times: 1
end of epoch 28: val_loss 2.8797624917338284e-05, val_acc 1.0
trigger times: 2
end of epoch 29: val_loss 0.00029462765431457425, val_acc 1.0
trigger times: 3
end of epoch 30: val_loss 0.0006590579447316714, val_acc 1.0
trigger times: 4
end of epoch 31: val_loss 0.0, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-0.0036,  0.0946, -0.0655, -0.0965,  0.0586, -0.0468,  0.0903,  0.0230,
          0.1001, -0.0577, -0.1059,  0.1819]], device='cuda:0'))])
end of epoch 32: val_loss 4.5544996103821236e-05, val_acc 1.0
trigger times: 1
end of epoch 33: val_loss 0.0016357453875799522, val_acc 1.0
trigger times: 2
end of epoch 34: val_loss 0.000133696277149582, val_acc 1.0
trigger times: 3
end of epoch 35: val_loss 0.0019111491359853972, val_acc 1.0
trigger times: 4
end of epoch 36: val_loss 6.264376204256905e-07, val_acc 1.0
trigger times: 5
end of epoch 37: val_loss 0.0003076173349198896, val_acc 1.0
trigger times: 6
end of epoch 38: val_loss 0.00010684269125832202, val_acc 1.0
trigger times: 7
end of epoch 39: val_loss 3.201560429282324e-06, val_acc 1.0
trigger times: 8
end of epoch 40: val_loss 4.005117335239561e-06, val_acc 1.0
trigger times: 9
end of epoch 41: val_loss 0.0004315319099018211, val_acc 1.0
trigger times: 10
Early stopping.
0 -0.5565789612010121 -147.4636492765731
1 -0.4120032872306183 -142.23388834828145
2 -0.5578094375086948 -122.62366970851606
3 -0.3386640071403235 -118.44560087924461
4 -0.4662975430255756 -116.108374803282
5 -0.39989396202145144 -115.43677859764247
6 -0.3843406988598872 -111.12884203537087
7 -0.4352823120134417 -110.83967460658118
8 -0.3205148821580224 -99.7956520082127
9 0.022887841216288507 -99.64931500624076
10 -0.4296659213723615 -96.59530823562386
11 -0.31796751213551033 -95.93196512830177
12 -0.4507880584860686 -95.26106070296488
13 -0.10069883188407402 -79.46225909654602
14 0.10937355759961065 -76.17790382876726
15 -0.047273428659536876 -73.19589636713303
16 0.006634520832449198 -65.39617499434047
17 -0.12379138598043937 -62.45594599657271
18 0.37181453002267517 -61.13021271290543
19 0.1585815723228734 -60.13433630255377
20 -0.1580668756578234 -50.14643167239018
21 0.01768839493888663 -48.67846358399454
22 0.002225522417575121 -47.40424201767414
23 0.17954750882927328 -43.397773251146276
24 0.01797177169646602 -41.86614966014691
25 0.1565557652793359 -36.32048246348241
26 0.1936610258417204 -32.74837227240138
27 0.5517284070083406 -32.32929369302946
28 0.4114085443143267 -25.770939267539205
29 0.75216836387699 -25.594995938321564
30 0.20701299854772515 -25.57213435865645
31 0.28229499288863735 -22.39844636520923
32 0.4662206076900475 -20.391209924013385
33 0.22929826042673085 -14.481488913331033
34 0.32471361051284475 -12.309091865555642
35 0.1857904897624394 -11.391024421481049
36 0.7955155247327639 -10.902017021977287
37 0.5426661298406543 -9.335689694740658
38 1.6569648450240493 0.7419180430902008
39 0.7380495468823938 3.8318902357964784
40 0.6615064362704288 3.9880090400704225
41 2.29597497083887 10.764415234755278
42 1.007277752723894 12.934274123226212
43 1.504160691605648 14.205206874791333
44 1.3446975468978053 24.910836857676962
45 1.6399979326015455 33.81267702300805
46 2.179741242012824 40.198823361309046
47 1.9488187013339484 43.50620370379428
48 2.0393962912930874 54.79421159291256
49 2.643494336385629 62.69478416198752
50 3.0043467344294186 74.22571892062575
51 2.680946507185581 84.10165836377381
52 3.9357099260523682 88.64858937911765
53 6.441440866503399 92.13810232868671
54 2.6758080188010354 95.5970919455078
55 3.5801969888270833 98.11620351598519
56 6.189525378518738 98.43438297600927
57 3.285698891922948 106.6069538993196
58 4.87614177208161 113.06528436824188
59 4.541491766110994 117.98751873386719
60 3.702082945339498 131.64414317765036
61 4.423027455457486 132.28794359627105
62 3.6495559045579284 133.11348989265034
63 3.4445024245942477 145.24215614623898
64 4.7023961379309185 158.38078213380754
65 4.156635436753277 158.50808436766704
66 6.028547587062349 167.96954732210003
67 8.27363335582777 168.2139420510252
68 5.466699547076132 170.47202355350058
69 5.718542230213643 173.61957394095663
70 6.050586362791364 174.15080882295382
71 6.104240660046344 181.61209907304905
72 7.038743135024561 183.630626461537
73 6.8782655490067555 195.24714457233625
74 9.223333562869811 197.54995255175572
75 8.559518456531805 198.07853498456498
76 7.916306146951683 208.43810157983071
77 6.842744527006289 221.98821142020677
78 5.992254775803303 225.96493030587686
79 4.969076729394146 234.912530058772
80 6.080753785572597 236.41042909711607
81 8.71777663184912 245.5887448959621
82 7.885862714829273 245.81351895912266
83 8.20943566369533 246.75481391697613
84 9.22609285460203 249.51084171854058
85 7.517737976450007 251.3571448151926
86 10.371864843058574 253.85724381515183
87 6.217163305816939 257.91690314903366
88 6.252613484219182 263.15565253602887
89 9.522642812822596 273.43913939999
90 7.251701079352642 276.57432091057194
91 8.603347135576769 281.20676916848015
92 12.181919188136817 283.0894527688991
93 10.293900305128773 285.72871085599655
94 10.384137260683929 290.13944852415955
95 9.499990314565366 291.9243761531606
96 8.884439765970455 302.3644246881209
97 11.043376080997405 306.1712532575959
98 8.311909260140965 312.3724481497734
99 7.491012569356826 321.2374011234209
100 8.948100111359963 321.2905107198387
101 9.154197537965956 321.65930957943124
102 9.171376171914744 325.94333208988763
103 8.790465107624186 327.9410755934301
104 10.08410075136635 337.728074301983
105 11.156976430764189 339.66188199747455
106 13.184453932728502 342.7998224222616
107 9.012265305485926 347.9329547559379
108 7.517732188629452 353.3812100160134
109 11.04981505194155 354.05687466597453
110 9.283694941113936 355.7759836246162
111 12.415722647085204 361.50704766788135
112 13.660097710977425 374.0128471692673
113 12.185451980491052 374.8603519157193
114 11.015127268357901 390.9114277638717
115 11.906173791736364 409.58867389683314
116 13.82963743401342 424.42641682066693
117 13.290788215148496 425.3425527059334
118 12.433000926612294 430.80730528172944
119 13.976070608812734 445.2253651296969
train accuracy: 1.0
validation accuracy: 1.0
