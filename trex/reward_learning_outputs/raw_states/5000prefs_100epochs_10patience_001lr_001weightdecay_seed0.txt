demo lengths 200
demos: (120, 200, 25)
demo_rewards: (120,)
[-238.5653939  -226.82452825 -226.22417315 -208.89755486 -196.98407553
 -194.40174577 -193.75394353 -173.38768743 -172.73844576 -172.58495982
 -148.80630048 -146.54022995 -144.91636018 -144.50987996 -129.52710351
 -119.67891388 -119.53338183 -115.00841352 -113.49466461 -107.45837168
 -105.499371   -101.91049433 -101.6629413   -98.89606861  -95.19099379
  -93.31854914  -85.41926577  -83.18012153  -79.47665069  -77.34308622
  -60.56041622  -58.68833689  -22.79021188  -19.65788927  -16.74518768
  -13.7194023     4.23518987    5.09528062   10.17879679   13.54105579
   14.98971093   21.80202968   31.12027906   40.64550157   40.69898986
   50.09032867   57.29053552   60.51441689   62.58785811   63.47489587
   65.39553199   68.71330703   69.48459209   72.16624272   74.22332021
   79.16074532   80.25553157   87.50520028   90.28646232   90.72533901
   91.52552636   91.95773979   92.9922131    94.14052713   94.28590507
   95.24073533   96.09581961   96.50341248   97.4582029    97.90553231
   98.27346636   98.35872828   98.92550904   99.09003072   99.89253842
  100.07765056  101.35869828  103.56921378  103.74194541  104.00039884
  104.36031384  104.79584302  105.40349711  105.60678193  106.26867976
  106.37916921  106.67969424  106.81132709  107.05410486  107.44441498
  107.68792314  107.72369032  108.58631071  108.79454915  109.15712102
  109.71287331  110.48639228  111.46302588  112.59201368  113.45309909
  114.85996347  115.04838323  115.56466246  115.7702745   116.29805012
  116.38499267  116.41989071  117.08008836  117.29357602  118.29160886
  119.01062501  120.85290512  120.89021502  121.40892075  122.30522808
  129.13346806  129.67718524  129.79664443  130.61780867  131.40806577]
maximum traj length 200
maximum traj length 200
num training_obs 4500
num training_labels 4500
num val_obs 500
num val_labels 500
num test_obs 7140
num test_labels 7140
ModuleList(
  (0): Linear(in_features=25, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Total number of parameters: 11648
Number of trainable paramters: 11648
device: cuda:0
end of epoch 0: val_loss 0.34533496867358915, val_acc 0.84
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[-1.6589e-03,  7.5785e-03, -1.8560e-05,  ..., -6.7316e-03,
         -5.9347e-04, -6.4044e-04],
        [-4.4537e-04,  5.5121e-03, -2.2731e-05,  ...,  5.7754e-04,
         -2.6671e-04, -9.7872e-04],
        [ 3.7673e-03,  2.4853e-03,  9.2000e-07,  ..., -9.0717e-05,
          5.8205e-03, -4.7009e-04],
        ...,
        [ 2.9863e-01, -1.7818e-02,  6.1594e-02,  ...,  4.7442e-01,
          6.9164e-02,  1.8585e-01],
        [-5.4637e-02,  3.1015e-03, -4.6376e-05,  ..., -4.5631e-03,
         -5.4016e-02,  1.3801e-02],
        [ 5.4354e-08,  3.2946e-07,  7.2586e-09,  ...,  3.6870e-08,
         -4.8526e-08, -1.6758e-08]], device='cuda:0')), ('fcs.0.bias', tensor([-2.7080e-03, -2.8907e-04, -1.3468e-02, -9.5943e-03, -3.3201e-04,
         2.9359e-06, -1.0770e-01, -2.2692e-04, -2.7980e-01,  2.4665e-01,
        -2.3904e-03, -4.5486e-02, -7.7998e-04, -1.2326e-01,  3.3962e-03,
        -1.2529e-06, -9.5211e-05,  6.8652e-01,  8.9824e-02,  8.2853e-04,
         1.6619e-06, -2.2614e-02,  2.0658e-04, -3.2182e-02, -8.8478e-03,
         1.2054e-01, -5.1884e-04, -9.6953e-05,  2.7637e-01,  3.0404e-05,
        -2.1175e-01,  2.1263e-01,  1.2319e-06, -9.4707e-02, -2.2500e-01,
        -1.7962e-01, -1.8054e-02, -4.6676e-05, -1.9066e-02, -5.5927e-06,
        -1.6875e-06, -1.7726e-07,  3.5526e-01, -1.6183e-01, -3.0539e-03,
        -8.3694e-05, -6.7550e-03, -8.7381e-03, -7.2581e-05,  1.0676e-01,
        -7.4773e-05, -1.0053e-03, -8.8160e-03, -2.8394e-01, -5.7001e-02,
        -4.0486e-02,  1.8522e-01, -2.3777e-06, -9.6442e-06, -9.2807e-06,
        -6.7629e-03,  2.5126e-02,  2.5388e-02, -2.1163e-03, -6.6606e-03,
        -2.2972e-03, -1.1341e-03, -2.5388e-05, -2.2716e-03, -4.4821e-07,
        -8.8499e-02, -1.9161e-03, -8.6292e-04,  2.7515e-06, -7.3992e-02,
        -4.2691e-03, -2.2478e-01, -1.3754e-03, -1.3855e-02, -6.8727e-04,
        -1.9433e-01, -3.6000e-02, -1.9332e-02, -3.1202e-04, -2.0852e-01,
        -2.5597e-01, -1.9145e-03,  2.6301e-01,  2.9694e-01, -1.0198e-01,
        -3.4619e-05,  7.9140e-05, -1.7243e-07, -2.5644e-03, -1.2020e-02,
        -3.5235e-03, -9.4108e-02, -5.3707e-05, -4.3724e-03,  3.2774e-01,
         4.3833e-02, -9.0752e-05, -4.3806e-02, -4.9871e-04,  7.2645e-01,
        -7.3852e-06, -9.6928e-04, -1.3128e-02, -2.5837e-03, -2.7685e-03,
        -2.9439e-03, -7.2266e-06, -4.5617e-05,  8.1721e-01, -7.3249e-09,
        -3.3287e-03, -4.0463e-03, -4.1789e-04, -4.2125e-04, -3.4907e-03,
         7.7543e-04, -6.1725e-04, -1.8519e-03, -1.8783e-01, -4.6148e-07,
         3.5823e-01, -6.1647e-02, -7.2171e-09], device='cuda:0')), ('fcs.1.weight', tensor([[ 6.9262e-06, -9.8646e-06,  3.8636e-05,  ..., -1.8789e-01,
         -1.1597e-04,  1.6845e-06],
        [ 4.9012e-05,  2.8510e-05,  7.4522e-06,  ..., -9.5079e-04,
         -3.1873e-04,  2.0512e-08],
        [-5.3118e-05, -3.9269e-06,  1.0086e-06,  ...,  1.9943e-03,
          1.4525e-05, -4.1845e-09],
        ...,
        [ 6.2185e-04,  4.5430e-05, -4.1018e-05,  ..., -3.8916e-01,
         -1.2665e-04, -6.4461e-08],
        [-4.2007e-04,  1.4372e-06,  6.4736e-05,  ..., -1.0421e-01,
          6.1227e-05,  5.8700e-06],
        [-2.0095e-04, -6.3821e-05, -2.6957e-05,  ...,  2.2064e-01,
         -1.6390e-03, -5.4451e-07]], device='cuda:0')), ('fcs.1.bias', tensor([-5.2865e-02,  8.8255e-03, -1.1931e-04,  2.8264e-02, -5.7837e-02,
        -7.8363e-02, -8.6646e-04, -1.3840e-02, -1.0330e-02,  7.6764e-02,
         1.0227e-03, -8.0050e-05, -1.5652e-02, -8.7621e-02, -1.3367e-01,
         9.5167e-07,  1.0237e-01,  5.1518e-02, -3.0941e-06, -6.2058e-03,
        -3.2209e-02, -1.4104e-01, -1.9118e-02, -6.2970e-03,  3.2632e-09,
         1.4067e-06, -5.3704e-02, -4.3109e-03, -1.2328e-02, -1.3463e-03,
         3.1297e-02, -1.8913e-03, -2.8326e-03, -1.6476e-02,  7.8592e-03,
        -1.5116e-02, -5.7140e-03, -5.8256e-03, -5.2716e-02, -4.3561e-02,
        -1.1521e-01, -9.0614e-02, -2.1123e-02,  2.4538e-03, -2.0208e-03,
         2.8601e-03,  6.0997e-02,  3.8122e-07, -2.4615e-07, -1.4792e-05,
         1.5504e-04, -1.0599e-01,  7.8603e-03, -1.2058e-01, -1.6777e-01,
        -1.5504e-01,  6.1226e-04, -1.4063e-01, -2.3579e-02,  1.3429e-04,
        -8.2661e-02, -2.1767e-01, -1.2457e-01,  8.8317e-02], device='cuda:0')), ('fcs.2.weight', tensor([[-8.9822e-02,  5.1690e-02,  5.8374e-03,  6.6347e-03, -1.9036e-01,
          4.1472e-02,  4.0215e-02,  2.7208e-02,  9.1734e-03, -2.3461e-03,
         -4.0219e-02, -4.7859e-02, -2.5466e-02,  4.8659e-02, -3.9756e-02,
         -7.0153e-02,  7.1371e-03, -2.9994e-02,  1.9773e-06,  4.7506e-03,
         -3.0072e-02, -1.2394e-01,  2.2779e-02,  2.5745e-04, -1.2505e-03,
          8.9480e-05, -1.2996e-01, -1.2263e-01,  4.7844e-02, -3.9896e-03,
          1.3789e-02,  3.6175e-02,  5.1774e-04, -1.2276e-01, -6.7919e-03,
         -2.0893e-02,  3.4236e-04, -1.5247e-01,  5.3851e-02,  9.9209e-02,
          2.2386e-02,  2.0351e-02, -3.3647e-04,  5.7668e-04, -3.6846e-03,
          2.7645e-02, -1.3002e-02,  1.1177e-02, -5.6123e-05,  2.4160e-04,
          3.3947e-02, -1.5900e-01,  5.0645e-03,  1.1968e-01,  6.3463e-02,
         -8.2151e-03, -2.2913e-02, -8.5133e-02, -5.1426e-02, -2.2164e-03,
         -2.9346e-02, -2.0387e-01,  1.8420e-01,  1.8848e-02]], device='cuda:0'))])
end of epoch 1: val_loss 1.5022454896518624, val_acc 0.822
trigger times: 1
end of epoch 2: val_loss 0.41572781582436996, val_acc 0.852
trigger times: 2
end of epoch 3: val_loss 0.4466879742444958, val_acc 0.844
trigger times: 3
end of epoch 4: val_loss 0.5409067774482981, val_acc 0.838
trigger times: 4
end of epoch 5: val_loss 0.3794880389036312, val_acc 0.84
trigger times: 5
end of epoch 6: val_loss 1.7355051981855383, val_acc 0.786
trigger times: 6
end of epoch 7: val_loss 1.725421681853007, val_acc 0.704
trigger times: 7
end of epoch 8: val_loss 17.87872134659481, val_acc 0.798
trigger times: 8
end of epoch 9: val_loss 0.3671303037284838, val_acc 0.812
trigger times: 9
end of epoch 10: val_loss 0.4456058293363202, val_acc 0.826
trigger times: 10
Early stopping.
0 13.155333336442709 -238.56539389647227
1 15.389655355364084 -226.82452825484978
2 14.988522604107857 -226.2241731451399
3 16.90416006371379 -208.89755486387574
4 24.209598641842604 -196.9840755292238
5 20.248129665851593 -194.4017457710484
6 19.316696712747216 -193.75394353311003
7 31.071726620197296 -173.38768743356906
8 27.179768301546574 -172.73844575529262
9 26.030331514775753 -172.5849598191294
10 29.03678499907255 -148.80630048130243
11 32.94415684044361 -146.54022994848935
12 27.680332377552986 -144.9163601843784
13 27.908192560076714 -144.5098799638122
14 37.851507589221 -129.52710350510273
15 41.63103029131889 -119.6789138826375
16 31.140171758830547 -119.53338183357056
17 27.93628690391779 -115.00841352193844
18 39.09825912117958 -113.49466461352053
19 46.34212763607502 -107.45837168117659
20 37.01674537360668 -105.49937100040788
21 36.46801932156086 -101.9104943327425
22 43.93657398223877 -101.66294130080523
23 38.68983805179596 -98.89606860541423
24 38.88293209671974 -95.1909937931514
25 36.70739895105362 -93.31854913617367
26 45.27094918489456 -85.41926577166943
27 44.576839312911034 -83.18012153468804
28 42.71478147804737 -79.47665068598401
29 37.78277599811554 -77.34308621853297
30 46.880399748682976 -60.56041622227008
31 50.677193373441696 -58.68833689293549
32 41.63033847510815 -22.79021187897909
33 48.48381492495537 -19.65788926803626
34 43.57959906756878 -16.74518768263467
35 46.151886478066444 -13.719402302108403
36 45.55454207956791 4.235189873860978
37 43.81980812549591 5.0952806210809465
38 45.94113779067993 10.178796790507759
39 41.18612124025822 13.541055788508864
40 41.76383325457573 14.989710929193077
41 44.82902793586254 21.802029683432238
42 47.12855103611946 31.12027905920427
43 49.93488746881485 40.64550156746333
44 45.58818203210831 40.69898986216935
45 49.05417577922344 50.09032867204584
46 44.99672009050846 57.290535515424786
47 45.79958790540695 60.51441689036072
48 48.045668214559555 62.587858111632386
49 39.454566061496735 63.47489586756552
50 45.94373416900635 65.395531990224
51 48.05377969145775 68.71330703108626
52 45.805630564689636 69.48459209048262
53 53.82848110795021 72.16624272108386
54 45.73679941892624 74.2233202149037
55 44.07370863854885 79.16074531670823
56 47.10851192474365 80.25553156558418
57 43.69290193915367 87.50520028403979
58 44.36346788704395 90.28646231697512
59 46.45748029649258 90.72533900600311
60 43.75041498243809 91.52552635781885
61 47.18964210152626 91.95773979158345
62 43.38988643884659 92.99221310223136
63 42.111285865306854 94.14052712880752
64 44.624930426478386 94.28590507273401
65 45.65620233118534 95.24073532592021
66 48.010156124830246 96.09581961281374
67 49.02670840919018 96.50341248035636
68 45.11041861772537 97.45820290396632
69 47.641759514808655 97.90553231413178
70 43.671972155570984 98.27346635731779
71 46.381895914673805 98.35872828331676
72 46.43015897274017 98.9255090371917
73 44.78781644999981 99.09003071828721
74 48.54077823460102 99.89253842063019
75 46.649853840470314 100.07765055593329
76 46.25254845619202 101.35869827851857
77 47.44298195838928 103.56921378052652
78 46.37823890149593 103.74194540937737
79 47.47100858390331 104.00039883786899
80 47.728951916098595 104.36031383612816
81 44.33295604586601 104.79584301893571
82 48.33581651747227 105.40349710769756
83 50.5537012219429 105.60678192821229
84 44.21424728631973 106.26867976298877
85 46.28529807925224 106.37916921043517
86 49.413202345371246 106.67969423997131
87 47.40905211865902 106.81132709017433
88 49.48283164203167 107.05410485829283
89 49.700537502765656 107.44441498172971
90 47.737571597099304 107.68792314282867
91 48.969944685697556 107.72369032284489
92 47.564665004611015 108.58631070626225
93 48.46091093122959 108.79454915391223
94 46.86096695065498 109.15712102315346
95 48.679597705602646 109.71287331230647
96 49.822815373539925 110.48639228144049
97 48.461566388607025 111.4630258761226
98 46.111655697226524 112.59201367677376
99 48.416953444480896 113.4530990931055
100 50.45136807858944 114.85996346749971
101 49.23646426200867 115.04838323377467
102 50.60307818651199 115.5646624637931
103 50.34032255411148 115.77027450439896
104 49.96769092977047 116.29805012422126
105 47.26174718141556 116.38499266629886
106 49.57631689310074 116.4198907102986
107 49.0372548699379 117.08008835928221
108 52.295173451304436 117.29357601720739
109 50.59798854589462 118.29160885622866
110 52.84428222477436 119.01062500757773
111 47.16112758219242 120.85290512447664
112 50.24552798271179 120.8902150231427
113 50.085797518491745 121.40892075278066
114 49.98710283637047 122.30522808413322
115 53.88712649047375 129.13346806288703
116 52.884040012955666 129.67718524437547
117 54.54791107773781 129.79664443488912
118 50.00976987183094 130.61780866873775
119 56.50489726662636 131.40806576766226
train accuracy: 0.8322222222222222
validation accuracy: 0.826
test accuracy: 0.8208683473389355
