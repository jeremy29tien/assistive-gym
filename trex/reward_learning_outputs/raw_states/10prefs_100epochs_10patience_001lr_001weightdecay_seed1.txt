demo lengths 200
demos: (120, 200, 25)
demo_rewards: (120,)
[-238.5653939  -226.82452825 -226.22417315 -208.89755486 -196.98407553
 -194.40174577 -193.75394353 -173.38768743 -172.73844576 -172.58495982
 -148.80630048 -146.54022995 -144.91636018 -144.50987996 -129.52710351
 -119.67891388 -119.53338183 -115.00841352 -113.49466461 -107.45837168
 -105.499371   -101.91049433 -101.6629413   -98.89606861  -95.19099379
  -93.31854914  -85.41926577  -83.18012153  -79.47665069  -77.34308622
  -60.56041622  -58.68833689  -22.79021188  -19.65788927  -16.74518768
  -13.7194023     4.23518987    5.09528062   10.17879679   13.54105579
   14.98971093   21.80202968   31.12027906   40.64550157   40.69898986
   50.09032867   57.29053552   60.51441689   62.58785811   63.47489587
   65.39553199   68.71330703   69.48459209   72.16624272   74.22332021
   79.16074532   80.25553157   87.50520028   90.28646232   90.72533901
   91.52552636   91.95773979   92.9922131    94.14052713   94.28590507
   95.24073533   96.09581961   96.50341248   97.4582029    97.90553231
   98.27346636   98.35872828   98.92550904   99.09003072   99.89253842
  100.07765056  101.35869828  103.56921378  103.74194541  104.00039884
  104.36031384  104.79584302  105.40349711  105.60678193  106.26867976
  106.37916921  106.67969424  106.81132709  107.05410486  107.44441498
  107.68792314  107.72369032  108.58631071  108.79454915  109.15712102
  109.71287331  110.48639228  111.46302588  112.59201368  113.45309909
  114.85996347  115.04838323  115.56466246  115.7702745   116.29805012
  116.38499267  116.41989071  117.08008836  117.29357602  118.29160886
  119.01062501  120.85290512  120.89021502  121.40892075  122.30522808
  129.13346806  129.67718524  129.79664443  130.61780867  131.40806577]
maximum traj length 200
maximum traj length 200
num training_obs 9
num training_labels 9
num val_obs 1
num val_labels 1
num test_obs 7140
num test_labels 7140
ModuleList(
  (0): Linear(in_features=25, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Total number of parameters: 11648
Number of trainable paramters: 11648
device: cuda:0
end of epoch 0: val_loss 0.00723764393478632, val_acc 1.0
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 0.1302, -0.0697, -0.0600,  ..., -0.0140, -0.1161, -0.0098],
        [-0.0052,  0.0612,  0.1498,  ..., -0.0003, -0.0447,  0.0244],
        [-0.2135, -0.0581,  0.0744,  ...,  0.1493, -0.0188,  0.1114],
        ...,
        [-0.1928, -0.1590, -0.1187,  ..., -0.1727,  0.0033, -0.1496],
        [ 0.2061,  0.0658,  0.0216,  ..., -0.1130,  0.1000, -0.0921],
        [ 0.0560, -0.1659,  0.1473,  ...,  0.1660,  0.1308,  0.0607]],
       device='cuda:0')), ('fcs.0.bias', tensor([-1.3509e-01,  1.5851e-01,  1.2120e-01, -6.9521e-02, -1.6899e-01,
        -1.2374e-02,  1.2306e-01,  1.2679e-01, -1.8643e-01,  2.1663e-01,
        -1.6664e-01,  7.7250e-02, -6.4406e-02, -1.5262e-01, -6.4476e-02,
        -3.6341e-02, -1.7216e-01, -1.0505e-01,  1.7879e-01,  2.1497e-02,
        -9.1728e-02,  1.6090e-01, -2.2694e-01, -8.2569e-02,  1.2242e-01,
        -1.8839e-02, -1.0039e-01, -1.6082e-01, -5.1962e-02, -1.3523e-01,
         1.1753e-02,  2.2987e-01,  6.4558e-02, -3.1417e-02,  1.7355e-01,
        -2.2110e-01,  3.6972e-02,  2.0560e-01,  3.4034e-02, -1.8205e-01,
         3.3637e-02,  8.3821e-02, -1.5785e-02,  6.0633e-02, -1.1983e-01,
         1.6621e-01, -1.5775e-01,  9.8978e-02, -1.7476e-01, -6.5313e-03,
         2.8120e-02, -6.4136e-02, -6.5059e-02, -1.1211e-01, -9.6030e-02,
         1.6213e-01,  9.7586e-02, -1.9925e-02, -3.9432e-02,  1.3966e-02,
         8.4924e-02,  8.1154e-03, -2.6075e-02, -2.9358e-02,  1.2491e-01,
        -1.8221e-01, -1.4127e-01, -1.3515e-01,  4.6086e-02, -1.3410e-01,
        -1.2861e-01, -4.9433e-02, -4.4781e-02, -1.3592e-01,  6.9138e-02,
         1.1162e-01,  1.0694e-01, -1.9599e-01, -1.0883e-01,  1.5647e-01,
         3.8364e-02,  1.3382e-01,  1.0978e-01, -8.4589e-02,  1.6557e-01,
         2.3600e-01,  7.3415e-02, -7.6549e-02,  8.0497e-02,  1.7302e-01,
        -1.1557e-01,  9.5475e-02, -1.3674e-01, -9.3919e-05, -1.0062e-01,
        -7.6901e-02, -2.3646e-02, -6.9270e-02, -9.3272e-02,  6.8222e-02,
        -9.5026e-02,  8.9295e-02, -4.7425e-02,  2.0850e-01, -3.7383e-02,
        -1.4130e-01, -5.2767e-02, -1.3776e-02,  3.5509e-03,  4.5026e-02,
        -1.9685e-01,  8.6733e-02,  1.4840e-01, -1.1464e-01, -3.3466e-02,
         1.5383e-01, -2.0477e-01,  9.7582e-02, -2.0741e-01, -2.6304e-02,
         9.9968e-02, -4.3896e-02, -1.4016e-01, -1.8742e-01, -3.4457e-02,
        -5.5055e-02, -1.1284e-02,  1.0359e-01], device='cuda:0')), ('fcs.1.weight', tensor([[ 0.0039, -0.0316, -0.1161,  ...,  0.1128, -0.0296, -0.0407],
        [ 0.0166, -0.0248, -0.0858,  ...,  0.1021, -0.0271, -0.0170],
        [-0.0755, -0.0343, -0.0291,  ...,  0.0223,  0.0093,  0.0410],
        ...,
        [ 0.0173, -0.0302, -0.0014,  ..., -0.0754,  0.0591,  0.0370],
        [-0.1131,  0.0601, -0.0260,  ...,  0.0898, -0.0373, -0.0356],
        [-0.0017, -0.0365,  0.0019,  ..., -0.0335,  0.0269, -0.0078]],
       device='cuda:0')), ('fcs.1.bias', tensor([-1.2054e-01,  9.0398e-02,  3.3578e-02,  2.7437e-02,  1.1433e-01,
        -3.8589e-02, -1.2435e-02, -2.5499e-02, -8.4660e-02, -7.6288e-05,
         2.9343e-03, -4.5768e-03,  1.7978e-02, -6.7629e-02,  9.3332e-02,
        -4.5097e-02,  5.4259e-03, -3.1333e-02,  2.2538e-02, -7.3830e-02,
        -1.4113e-02,  1.2339e-01,  2.1693e-03, -1.4081e-02,  1.3823e-02,
        -3.2677e-03,  1.1420e-01,  1.2129e-01, -9.4300e-02, -2.0271e-02,
        -1.1608e-02, -9.1060e-02, -8.8491e-02, -3.5047e-03,  4.7193e-02,
         4.3883e-03,  2.0493e-02,  5.2137e-02,  5.4900e-02, -1.5931e-02,
         3.2405e-02,  1.0809e-02, -3.6137e-02,  3.4967e-02,  6.9644e-02,
        -9.0875e-02, -1.0576e-01,  1.0285e-01, -7.5590e-02, -1.0144e-02,
         3.7535e-02,  3.3901e-02, -2.5426e-02, -1.2607e-01, -1.2581e-02,
        -1.1572e-02, -2.7984e-02,  1.5145e-02, -2.1782e-02,  1.2545e-02,
         8.3246e-02, -1.5199e-02,  7.3153e-02, -8.3700e-02], device='cuda:0')), ('fcs.2.weight', tensor([[-7.0485e-03, -3.2966e-02,  3.9392e-03,  5.2136e-02, -1.5166e-01,
          8.8391e-02, -8.1398e-02, -2.6250e-02,  9.6317e-02, -6.8085e-02,
         -1.1956e-01,  4.9804e-02,  5.6698e-02, -6.5439e-02,  1.7106e-02,
          1.1967e-02, -7.1931e-02,  3.3977e-02,  2.2629e-02,  7.4595e-03,
          3.5525e-02,  3.3421e-02,  3.4202e-02,  9.5434e-02, -8.3530e-02,
         -1.4069e-01, -6.4036e-02, -1.0387e-01, -1.8230e-02,  4.1616e-02,
          1.9891e-02,  5.3777e-02,  2.3100e-02,  6.6805e-02,  7.2666e-02,
         -1.5591e-01,  1.3188e-02,  1.0826e-01, -4.6927e-02, -2.2566e-02,
          1.4000e-05, -2.4072e-02, -1.6446e-02,  1.1724e-01,  1.2749e-01,
          1.2807e-02,  5.1650e-02, -5.7891e-02,  1.9838e-02,  7.7281e-02,
         -2.6955e-02,  2.8108e-02, -3.7742e-02, -7.8877e-02,  1.2766e-01,
          2.5383e-02, -2.7451e-02,  7.0179e-02,  9.4779e-02,  1.1991e-02,
          1.5185e-03,  4.4207e-02, -1.6428e-01, -7.1030e-02]], device='cuda:0'))])
end of epoch 1: val_loss 22.323867797851562, val_acc 0.0
trigger times: 1
end of epoch 2: val_loss 0.02051854319870472, val_acc 1.0
trigger times: 2
end of epoch 3: val_loss 5.1074113845825195, val_acc 0.0
trigger times: 3
end of epoch 4: val_loss 0.9935376048088074, val_acc 0.0
trigger times: 4
end of epoch 5: val_loss 0.2748912572860718, val_acc 1.0
trigger times: 5
end of epoch 6: val_loss 3.7819228172302246, val_acc 0.0
trigger times: 6
end of epoch 7: val_loss 5.661412239074707, val_acc 0.0
trigger times: 7
end of epoch 8: val_loss 6.368994235992432, val_acc 0.0
trigger times: 8
end of epoch 9: val_loss 6.594630241394043, val_acc 0.0
trigger times: 9
end of epoch 10: val_loss 6.658160209655762, val_acc 0.0
trigger times: 10
Early stopping.
0 147.45061337947845 -238.56539389647227
1 144.14608013629913 -226.82452825484978
2 117.79217138886452 -226.2241731451399
3 148.02122968435287 -208.89755486387574
4 128.2625703215599 -196.9840755292238
5 133.46746492385864 -194.4017457710484
6 130.18083888292313 -193.75394353311003
7 146.50278770923615 -173.38768743356906
8 124.73935002088547 -172.73844575529262
9 144.67872387170792 -172.5849598191294
10 163.51219606399536 -148.80630048130243
11 149.12838757038116 -146.54022994848935
12 147.64912819862366 -144.9163601843784
13 135.17532312870026 -144.5098799638122
14 148.96631067991257 -129.52710350510273
15 155.64994686841965 -119.6789138826375
16 160.96215164661407 -119.53338183357056
17 134.30868327617645 -115.00841352193844
18 142.2116323709488 -113.49466461352053
19 157.77669137716293 -107.45837168117659
20 155.4372239112854 -105.49937100040788
21 148.58842861652374 -101.9104943327425
22 157.63511806726456 -101.66294130080523
23 156.92165023088455 -98.89606860541423
24 158.43192595243454 -95.1909937931514
25 157.7865995168686 -93.31854913617367
26 169.44141626358032 -85.41926577166943
27 157.68793135881424 -83.18012153468804
28 151.70535427331924 -79.47665068598401
29 144.42759108543396 -77.34308621853297
30 150.00732272863388 -60.56041622227008
31 157.4563176035881 -58.68833689293549
32 168.5305718779564 -22.79021187897909
33 144.66646736860275 -19.65788926803626
34 145.7590390443802 -16.74518768263467
35 160.3237606883049 -13.719402302108403
36 153.34581691026688 4.235189873860978
37 161.5187616944313 5.0952806210809465
38 159.91014170646667 10.178796790507759
39 151.62715429067612 13.541055788508864
40 152.7766157388687 14.989710929193077
41 152.1854366660118 21.802029683432238
42 166.31978529691696 31.12027905920427
43 160.12255781888962 40.64550156746333
44 157.32245111465454 40.69898986216935
45 165.61089605093002 50.09032867204584
46 166.88933283090591 57.290535515424786
47 158.9232565164566 60.51441689036072
48 152.10036903619766 62.587858111632386
49 170.34177631139755 63.47489586756552
50 171.59155863523483 65.395531990224
51 164.91655844449997 68.71330703108626
52 166.24664217233658 69.48459209048262
53 171.90817081928253 72.16624272108386
54 163.3739289045334 74.2233202149037
55 164.47800225019455 79.16074531670823
56 154.6363689303398 80.25553156558418
57 166.13016825914383 87.50520028403979
58 164.33270692825317 90.28646231697512
59 165.7858134508133 90.72533900600311
60 165.04602545499802 91.52552635781885
61 160.45414340496063 91.95773979158345
62 146.48603039979935 92.99221310223136
63 154.32619792222977 94.14052712880752
64 158.4843322634697 94.28590507273401
65 157.4506487250328 95.24073532592021
66 158.7132472395897 96.09581961281374
67 164.15345960855484 96.50341248035636
68 159.67297154664993 97.45820290396632
69 156.85389876365662 97.90553231413178
70 169.07131415605545 98.27346635731779
71 167.3237111568451 98.35872828331676
72 168.59856128692627 98.9255090371917
73 166.19715350866318 99.09003071828721
74 169.33642143011093 99.89253842063019
75 168.79833817481995 100.07765055593329
76 160.26251256465912 101.35869827851857
77 168.88916915655136 103.56921378052652
78 159.3283234834671 103.74194540937737
79 163.7685605287552 104.00039883786899
80 166.4897040128708 104.36031383612816
81 157.60651344060898 104.79584301893571
82 165.548470556736 105.40349710769756
83 167.22406381368637 105.60678192821229
84 153.76139491796494 106.26867976298877
85 156.9439941048622 106.37916921043517
86 146.96838730573654 106.67969423997131
87 158.7097269296646 106.81132709017433
88 162.66629821062088 107.05410485829283
89 161.21310490369797 107.44441498172971
90 165.05384474992752 107.68792314282867
91 155.70057326555252 107.72369032284489
92 158.65262573957443 108.58631070626225
93 151.136698782444 108.79454915391223
94 163.05987054109573 109.15712102315346
95 161.78745883703232 109.71287331230647
96 153.778375685215 110.48639228144049
97 155.4541643857956 111.4630258761226
98 167.81774932146072 112.59201367677376
99 159.97269988059998 113.4530990931055
100 163.94302409887314 114.85996346749971
101 160.72067266702652 115.04838323377467
102 173.8637044429779 115.5646624637931
103 157.17627483606339 115.77027450439896
104 163.22416150569916 116.29805012422126
105 143.99285107851028 116.38499266629886
106 171.13475847244263 116.4198907102986
107 165.47262293100357 117.08008835928221
108 165.97196406126022 117.29357601720739
109 172.70539838075638 118.29160885622866
110 162.16821604967117 119.01062500757773
111 150.21399372816086 120.85290512447664
112 167.29036784172058 120.8902150231427
113 161.0647007226944 121.40892075278066
114 162.1414640545845 122.30522808413322
115 162.61918479204178 129.13346806288703
116 156.11257433891296 129.67718524437547
117 170.85617405176163 129.79664443488912
118 166.9825164079666 130.61780866873775
119 161.70998269319534 131.40806576766226
train accuracy: 1.0
validation accuracy: 0.0
test accuracy: 0.6645658263305322
