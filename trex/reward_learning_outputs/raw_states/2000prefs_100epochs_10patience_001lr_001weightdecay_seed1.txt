demo lengths 200
demos: (120, 200, 25)
demo_rewards: (120,)
[-238.5653939  -226.82452825 -226.22417315 -208.89755486 -196.98407553
 -194.40174577 -193.75394353 -173.38768743 -172.73844576 -172.58495982
 -148.80630048 -146.54022995 -144.91636018 -144.50987996 -129.52710351
 -119.67891388 -119.53338183 -115.00841352 -113.49466461 -107.45837168
 -105.499371   -101.91049433 -101.6629413   -98.89606861  -95.19099379
  -93.31854914  -85.41926577  -83.18012153  -79.47665069  -77.34308622
  -60.56041622  -58.68833689  -22.79021188  -19.65788927  -16.74518768
  -13.7194023     4.23518987    5.09528062   10.17879679   13.54105579
   14.98971093   21.80202968   31.12027906   40.64550157   40.69898986
   50.09032867   57.29053552   60.51441689   62.58785811   63.47489587
   65.39553199   68.71330703   69.48459209   72.16624272   74.22332021
   79.16074532   80.25553157   87.50520028   90.28646232   90.72533901
   91.52552636   91.95773979   92.9922131    94.14052713   94.28590507
   95.24073533   96.09581961   96.50341248   97.4582029    97.90553231
   98.27346636   98.35872828   98.92550904   99.09003072   99.89253842
  100.07765056  101.35869828  103.56921378  103.74194541  104.00039884
  104.36031384  104.79584302  105.40349711  105.60678193  106.26867976
  106.37916921  106.67969424  106.81132709  107.05410486  107.44441498
  107.68792314  107.72369032  108.58631071  108.79454915  109.15712102
  109.71287331  110.48639228  111.46302588  112.59201368  113.45309909
  114.85996347  115.04838323  115.56466246  115.7702745   116.29805012
  116.38499267  116.41989071  117.08008836  117.29357602  118.29160886
  119.01062501  120.85290512  120.89021502  121.40892075  122.30522808
  129.13346806  129.67718524  129.79664443  130.61780867  131.40806577]
maximum traj length 200
maximum traj length 200
num training_obs 1800
num training_labels 1800
num val_obs 200
num val_labels 200
num test_obs 7140
num test_labels 7140
ModuleList(
  (0): Linear(in_features=25, out_features=128, bias=True)
  (1): Linear(in_features=128, out_features=64, bias=True)
  (2): Linear(in_features=64, out_features=1, bias=False)
)
Total number of parameters: 11648
Number of trainable paramters: 11648
device: cuda:0
end of epoch 0: val_loss 0.3778411116923962, val_acc 0.81
trigger times: 0
saving model weights...
Weights: OrderedDict([('fcs.0.weight', tensor([[ 4.6107e-02,  8.1632e-04,  1.0925e-01,  ...,  2.0072e-01,
         -1.0057e-01, -1.8761e-01],
        [-6.2144e-03,  9.3665e-02, -1.6179e-03,  ...,  9.6649e-02,
         -4.1597e-02, -5.8333e-03],
        [-3.9470e-02, -1.5887e-02, -1.3214e-04,  ...,  8.2716e-04,
         -2.6930e-02, -1.3492e-02],
        ...,
        [-1.8650e-01, -2.2395e-01, -4.9774e-02,  ...,  6.0997e-02,
         -1.3138e-01, -2.0025e-01],
        [ 9.9527e-04,  1.5058e-03,  2.5846e-04,  ...,  1.4968e-03,
         -1.7858e-03, -1.7470e-04],
        [ 1.4666e-02,  1.6254e-02, -9.5340e-05,  ...,  1.5526e-01,
         -1.8977e-02, -2.9479e-03]], device='cuda:0')), ('fcs.0.bias', tensor([-9.3265e-02, -2.9293e-03,  2.5314e-02, -3.3049e-02, -1.2305e-01,
        -5.5879e-02, -1.5881e-01,  1.4778e-02, -1.3146e-01,  2.2190e-01,
        -1.9110e-01, -1.1340e-02, -7.2252e-02, -5.4790e-02, -1.3879e-01,
        -6.4787e-05, -8.7682e-02, -5.7027e-02, -3.0717e-02, -1.4931e-02,
        -1.2665e-01,  6.0661e-02, -1.1133e-01, -6.7169e-03, -1.1336e-02,
        -3.5497e-02, -3.9552e-02, -1.2216e-01, -7.3565e-02,  2.3370e-01,
        -7.2304e-02,  1.4661e-02, -2.0612e-02, -1.7798e-02, -2.9356e-02,
         1.3916e-02, -2.2231e-04, -1.2875e-04, -1.1665e-05, -2.3447e-02,
        -2.9142e-04, -1.9393e-02, -2.1814e-02, -1.3989e-05, -3.9144e-02,
        -5.1386e-02,  4.8929e-05,  1.9753e-02, -3.3899e-03, -2.8761e-02,
        -6.4953e-02, -4.1416e-04, -1.3899e-02, -7.8950e-03, -3.3120e-03,
        -7.8559e-03, -8.3728e-03, -1.1640e-02,  1.4614e-02,  6.8341e-06,
         2.2670e-03, -4.4224e-02, -1.1536e-02, -3.6252e-02,  2.1802e-03,
         1.6517e-01, -2.5056e-01, -4.4034e-03, -2.5072e-02, -5.6648e-02,
        -2.0487e-02, -2.0520e-02, -1.8200e-02, -1.8397e-01, -5.1511e-04,
        -1.9517e-01, -5.7997e-04, -2.7013e-01, -4.4781e-02, -5.7427e-05,
         8.2812e-03,  8.2613e-02, -4.2688e-03, -8.5917e-02,  1.0418e-05,
        -2.4049e-02, -7.1171e-03, -1.4939e-01,  7.2119e-02, -7.9531e-03,
        -2.3612e-01, -9.9344e-04, -5.3682e-02, -5.4728e-03, -7.8679e-02,
        -3.9068e-02, -3.4025e-03,  7.6684e-06, -3.8238e-02,  5.1483e-06,
         3.1808e-01, -1.0866e-02, -9.1363e-02,  2.1780e-03,  2.8003e-05,
        -1.3961e-02, -3.9639e-02, -1.1939e-01, -1.4111e-01,  5.0086e-03,
        -5.7912e-02,  2.5909e-01, -3.4410e-02, -1.2321e-03, -2.8132e-02,
         3.6724e-01, -7.9365e-02, -1.7483e-03, -3.3343e-07, -6.1821e-02,
        -6.0253e-04, -3.1836e-03, -9.3938e-03, -8.4857e-03,  3.9302e-03,
        -5.0267e-02, -5.4148e-03, -1.0977e-02], device='cuda:0')), ('fcs.1.weight', tensor([[-1.3951e-01,  4.6067e-03,  1.7443e-03,  ..., -5.4329e-02,
          2.4021e-04,  2.0961e-02],
        [ 3.3561e-02, -3.9444e-02,  1.9161e-02,  ...,  5.1886e-02,
         -1.0121e-02, -3.1547e-02],
        [-3.8321e-04, -7.9481e-03,  1.0335e-03,  ..., -1.6562e-02,
          7.8696e-06,  9.8496e-03],
        ...,
        [-3.4127e-05,  4.6540e-02,  1.6831e-04,  ..., -3.5193e-02,
          3.0857e-05,  9.2610e-04],
        [-1.3900e-04, -2.1229e-02, -1.2819e-03,  ..., -9.5675e-03,
         -1.0374e-05, -2.4442e-03],
        [-6.7024e-04,  6.3572e-05,  1.7746e-04,  ..., -1.3095e-01,
          2.1070e-05, -1.7843e-05]], device='cuda:0')), ('fcs.1.bias', tensor([-8.6858e-02, -9.7151e-02, -3.5944e-02, -1.1042e-02,  4.9805e-02,
        -1.1462e-01,  1.9892e-01,  3.6749e-02, -2.4047e-02, -1.2721e-02,
        -4.4563e-02, -7.3075e-04,  6.7409e-03,  6.4869e-02,  2.7795e-02,
        -3.8645e-02,  2.2457e-02, -1.2253e-01, -1.5483e-03, -5.3171e-03,
        -5.7832e-02,  4.6841e-01, -1.7242e-02, -5.8718e-03, -7.2142e-04,
         1.2468e-01,  1.6938e-02,  1.5516e-04, -1.7164e-02, -1.4880e-02,
        -1.4997e-02,  2.0434e-01,  3.9181e-03, -1.0891e-02, -3.3635e-02,
        -4.0196e-02,  3.0689e-02, -1.1723e-02,  2.2104e-02,  1.0725e-03,
        -4.4531e-02,  1.1161e-03, -2.7836e-02, -6.6230e-02,  7.6545e-05,
        -5.2411e-02, -4.5459e-02, -1.0239e-02, -4.7768e-02, -5.0582e-02,
        -9.8346e-03,  2.2976e-01, -4.0605e-02, -3.9871e-02, -4.4129e-03,
        -2.7186e-05, -2.3827e-03,  1.1570e-02,  1.2273e-02, -2.5987e-03,
        -2.2628e-02,  5.8791e-05,  1.3665e-02, -1.3073e-01], device='cuda:0')), ('fcs.2.weight', tensor([[-0.0525,  0.0454, -0.0556, -0.1019, -0.0442, -0.1595, -0.0134, -0.0276,
         -0.0024,  0.0749, -0.0041, -0.0098,  0.0063,  0.0177,  0.0310,  0.0242,
          0.0122, -0.0464,  0.0032, -0.0141, -0.0377, -0.0462,  0.0558, -0.0221,
         -0.0579, -0.0193, -0.0174, -0.0313,  0.0009, -0.0937, -0.0218, -0.0075,
          0.0140,  0.0181,  0.1495,  0.0163, -0.0138,  0.0120,  0.0913,  0.0559,
          0.1252, -0.0052, -0.0252, -0.0527,  0.0039, -0.0039, -0.1208, -0.0457,
         -0.0055, -0.1416, -0.0641,  0.0229, -0.0088, -0.0154,  0.0101, -0.0081,
         -0.0020,  0.0326,  0.0046,  0.0024,  0.0078, -0.0449,  0.0245, -0.1050]],
       device='cuda:0'))])
end of epoch 1: val_loss 0.42038297650270806, val_acc 0.775
trigger times: 1
end of epoch 2: val_loss 0.4200612787451769, val_acc 0.8
trigger times: 2
end of epoch 3: val_loss 0.7092021345649431, val_acc 0.805
trigger times: 3
end of epoch 4: val_loss 1.1675296472054717, val_acc 0.71
trigger times: 4
end of epoch 5: val_loss 0.39734930346232167, val_acc 0.815
trigger times: 5
end of epoch 6: val_loss 1.2134730026338374, val_acc 0.75
trigger times: 6
end of epoch 7: val_loss 0.5267220881492811, val_acc 0.78
trigger times: 7
end of epoch 8: val_loss 0.43680139910645494, val_acc 0.805
trigger times: 8
end of epoch 9: val_loss 0.387608685973355, val_acc 0.79
trigger times: 9
end of epoch 10: val_loss 0.516545455043818, val_acc 0.815
trigger times: 10
Early stopping.
0 41.50234127044678 -238.56539389647227
1 40.58248749375343 -226.82452825484978
2 33.991141490638256 -226.2241731451399
3 44.424459263682365 -208.89755486387574
4 39.34969937801361 -196.9840755292238
5 44.5937494635582 -194.4017457710484
6 37.89078002423048 -193.75394353311003
7 46.05004635453224 -173.38768743356906
8 40.27081407606602 -172.73844575529262
9 42.09286314249039 -172.5849598191294
10 51.97930261492729 -148.80630048130243
11 50.62874738872051 -146.54022994848935
12 48.85461950302124 -144.9163601843784
13 48.67770765721798 -144.5098799638122
14 51.33813579380512 -129.52710350510273
15 49.62671339511871 -119.6789138826375
16 50.55852061510086 -119.53338183357056
17 47.29861031472683 -115.00841352193844
18 48.38130795955658 -113.49466461352053
19 58.29128363728523 -107.45837168117659
20 53.78049947321415 -105.49937100040788
21 53.25260320305824 -101.9104943327425
22 51.068268433213234 -101.66294130080523
23 53.46040715277195 -98.89606860541423
24 54.353044122457504 -95.1909937931514
25 52.62819363176823 -93.31854913617367
26 56.18679244816303 -85.41926577166943
27 53.025497287511826 -83.18012153468804
28 51.38444082438946 -79.47665068598401
29 49.73598666489124 -77.34308621853297
30 55.86548487842083 -60.56041622227008
31 56.138240963220596 -58.68833689293549
32 57.07694032788277 -22.79021187897909
33 56.6939859688282 -19.65788926803626
34 58.35257202386856 -16.74518768263467
35 55.86187471449375 -13.719402302108403
36 56.171720281243324 4.235189873860978
37 53.429820477962494 5.0952806210809465
38 58.1765802949667 10.178796790507759
39 55.451281026005745 13.541055788508864
40 55.22835147380829 14.989710929193077
41 56.17281919717789 21.802029683432238
42 52.75163619220257 31.12027905920427
43 56.862185791134834 40.64550156746333
44 59.084526896476746 40.69898986216935
45 57.369869127869606 50.09032867204584
46 57.57374703884125 57.290535515424786
47 56.66870802640915 60.51441689036072
48 55.06697626411915 62.587858111632386
49 55.927825182676315 63.47489586756552
50 58.866135612130165 65.395531990224
51 59.86063292622566 68.71330703108626
52 58.54546794295311 69.48459209048262
53 60.912894293665886 72.16624272108386
54 55.12536495923996 74.2233202149037
55 53.10853496193886 79.16074531670823
56 55.32237648963928 80.25553156558418
57 55.07617139816284 87.50520028403979
58 57.53099490702152 90.28646231697512
59 57.39202681183815 90.72533900600311
60 56.940122202038765 91.52552635781885
61 56.918220192193985 91.95773979158345
62 50.64989712834358 92.99221310223136
63 52.90953969955444 94.14052712880752
64 55.17453616857529 94.28590507273401
65 55.89229096472263 95.24073532592021
66 55.55455857515335 96.09581961281374
67 60.06724715232849 96.50341248035636
68 56.225126534700394 97.45820290396632
69 59.04946357011795 97.90553231413178
70 55.666147619485855 98.27346635731779
71 56.83143311738968 98.35872828331676
72 57.089277908205986 98.9255090371917
73 57.63216054439545 99.09003071828721
74 58.10035693645477 99.89253842063019
75 58.761936634778976 100.07765055593329
76 60.14124634861946 101.35869827851857
77 53.694774106144905 103.56921378052652
78 57.71140471100807 103.74194540937737
79 59.33921977877617 104.00039883786899
80 56.015426605939865 104.36031383612816
81 56.52231639623642 104.79584301893571
82 57.72358477115631 105.40349710769756
83 59.58451060950756 105.60678192821229
84 57.35892504453659 106.26867976298877
85 55.20543359220028 106.37916921043517
86 55.96264788508415 106.67969423997131
87 58.1503938883543 106.81132709017433
88 59.39366263151169 107.05410485829283
89 58.50883400440216 107.44441498172971
90 58.9652618765831 107.68792314282867
91 58.73642358183861 107.72369032284489
92 56.14645004272461 108.58631070626225
93 58.3047513961792 108.79454915391223
94 60.27877348661423 109.15712102315346
95 60.45899677276611 109.71287331230647
96 58.60477416217327 110.48639228144049
97 59.44174103438854 111.4630258761226
98 59.87746161222458 112.59201367677376
99 56.575381964445114 113.4530990931055
100 59.797801330685616 114.85996346749971
101 57.90400642156601 115.04838323377467
102 63.41530957818031 115.5646624637931
103 53.39816179871559 115.77027450439896
104 57.10967209935188 116.29805012422126
105 59.39447641372681 116.38499266629886
106 59.9954439252615 116.4198907102986
107 59.29686293005943 117.08008835928221
108 58.690408289432526 117.29357601720739
109 60.99998667836189 118.29160885622866
110 63.33075159788132 119.01062500757773
111 58.43771666288376 120.85290512447664
112 63.05945307016373 120.8902150231427
113 61.30985899269581 121.40892075278066
114 60.4343833476305 122.30522808413322
115 64.92180442810059 129.13346806288703
116 64.87121617794037 129.67718524437547
117 62.89532193541527 129.79664443488912
118 61.849363058805466 130.61780866873775
119 67.87621587514877 131.40806576766226
train accuracy: 0.8116666666666666
validation accuracy: 0.815
test accuracy: 0.7522408963585434
